---
title: "Sample Clustering"
author: "Sarah Killcoyne"
date: "23/02/2016"
output: 
  html_document: 
    toc: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
library(ggplot2)
library(GGally)
library(plyr)
library(dplyr)
library(pander)
library(Hmisc)
library(ggdendro)
library(reshape2)
library(GenomicRanges)
library(gridExtra)
library(biomaRt)
library(multtest)
library(tibble)
library(mclust)
library(ggfortify)

source('lib/load_patient_metadata.R')

data = '~/Data/Ellie'

data.files = list.files(paste(data, 'QDNAseq',sep='/'), full.names=T)
analysis.files = list.files(paste(data, 'Analysis', sep='/'), full.names=T)
plot.dir = paste(data, 'Analysis/multipcf_plots_fitted_perPatient', sep='/')

if (length(list.files(plot.dir)) <= 0)
  stop(paste("No analysis files found in", plot.dir ))

## Patient info file
patient.file = grep('All_patient_info.txt', data.files, value=T)
if (length(patient.file) != 1)
  stop(paste("Missing/too many patient info file(s) in", data))
demo.file = grep('Demographics_full.txt', data.files, value=T)


load(grep('Training_patients.Rdata', analysis.files, value=T), verbose=T)

patient.info = read.patient.info(patient.file, demo.file)
#head(patient.info)

patient.info = arrange(patient.info, Status, Patient, Endoscopy.Year, Pathology)

sum.patient.data = summarise.patient.info(patient.info)
sum.patient.data = as.data.frame(subset(sum.patient.data, Patient %in% names(patient.data))) ## For now
pander(sum.patient.data)

# Missing some of the samples as they aren't all sequenced yet
for (pt in names(patient.data)) {
  patient.data[[pt]]$info= patient.data[[pt]]$info[patient.data[[pt]]$info$Samplename %in% colnames(patient.data[[pt]]$seg.vals)[-(1:5)],]
}

```

# Global View and Questions

We have a few questions we needed to answer with this data.  The pairwise correlation plots appeared to show a difference between progressors and non-progressors as well as some differences between samples within a single patient.  So...

1. Within a single patient, can we quantify in some way the diversity that appears to be present in the pairwise correlation plots? 
2. Given a diversity measure within a single patient, can we differentiate the progressors from the non-progressors?  Can we do it prior to the HGD/cancer sample?
3. Is there then a way to select patient-specific thresholds for calling variable regions?  Alternately, given what we know about the cancer can we presume diploidy and treat these samples as if we had absolute copy number?

The first two questions were addressed with the clustering below.  The third is in progress currently.


```{r global_pt, echo=F, message=F, warning=F, error=F, eval=F}
be = subset(patient.info, Pathology == 'BE')
lgd = subset(patient.info, Pathology <= 'LGD')

p53Status = data.frame(rbind('Non-Progressors'=table(subset(patient.info, Status == 'NP')$p53.Status), 
           'Progressors'=table(subset(patient.info, Status == 'P')$p53.Status)))
colnames(p53Status) = c('None', 'Positive')

set.seed(99)
mtp1 = MTP(X=t(patient.info[,c('Barretts.Cellularity','Total.Reads')]), Y=as.integer((as.integer(patient.info$Pathology) > 3)), 
           get.adjp=T, na.rm=T, test='t.twosamp.equalvar')
mtp2 = MTP(X=t(patient.info[,c('Barretts.Cellularity','Total.Reads')]), Y=patient.info$Status, 
            get.adjp=T, na.rm=T, test='t.twosamp.equalvar')

# Currently with multi test correction the samples for which cellularity and #of reads are available show no difference for either status (P vs NP)
# `r pander(mtp2@adjp, justify='left', caption="Adjusted p-value")`
# 
# or for samples that are LGD/IMC/HGD vs BE
# `r pander(mtp1@adjp, justify='left', caption="Adjusted p-value")`
# 
# However, it should be noted that pathologists do not like calling cellularity on Barrett's generally so it's a fairly inaccurate measure.
# `r pander(table( patient.info[,c('Status','Barretts.Cellularity')]), caption="Numbers of cellularity calls by status")`
# 
# With regards to p53 staining, non-progressors have no p53 positive tests, while progressors had `r p53Status[2,2]` overall.
# `r pander(p53Status, justify='left', caption="P53 staining status for progressors vs non")`
# Of these `r table(be$p53.Status)[['1']]` were positive in a BE sample, and `r table(lgd$p53.Status)[['1']]` were positive when LGD samples were included.

```


## How do the sample timepoints separate?

It appears that in the progressors that generally the HGD or IMC pathology may suddenly appear and be very different (PR1-HIN-044) from other samples, or those where it may have been a progressive sampling that can be fairly related to previous samples (AHM0952).  In most cases the HGD/IMC samples drive the variance, but it is often there in the LGD samples as well.  In contrast the non-progressor patients, even where they have LGD, don't generally show a sudden jump in the sample (AD0591).  

This would suggest we might have a more difficult time separating out the progressors where the early samples were all highly related and a sudden shift occurred to generate the HGD sample. This could be either sampling bias or a sudden model of progression, but I can't think of a method right now that might separate the two.

```{r pca, echo=F, warning=F, message=F, fig.width=8, fig.height=4, eval=F}
sum.patient.data = arrange(sum.patient.data, Status, total.samples)

plot.pca<-function(pt) {
  print(pt)
  info = arrange(patient.data[[pt]]$info, Endoscopy.Year)
  if (length(info$Samplename) <= 1) return(NA)
  
  info = info[which(info$Samplename %in% colnames(patient.data[[pt]][['seg.vals']])[-(1:5)]),]
  plots = list()
  for (sv in c('seg.vals')) {
    
    x1 = patient.data[[pt]][[sv]][, info$Samplename]
    
    if (length(info$Samplename) <= 1) { 
      x1 = as.data.frame(x1)
      colnames(x1) = info$Samplename
    } else {
      x1 = x1[, info$Samplename]
    }

    colnames(x1) = paste(info$Endoscopy.Year, '(', info$Pathology, ')', sep='' )

    #pca = prcomp(t(x1), scale=length(info$Samplename) > 2)
    pca = prcomp(t(x1), scale=length(info$Samplename) > 2)
  
    eigenvalues = pca$sdev^2
    #screeplot(pca, main=paste(pt, sv, 'scree'))
  
    #plot(pca$sd^2)

    df = as.data.frame(pca$x)
    df$year = as.factor(info$Endoscopy.Year)
    df$label = paste(info$Endoscopy.Year, '(', info$Pathology, ')', sep='' )
    df$Pathology = info$Pathology

    gg = ggplot(df, aes(PC1, PC2)) + geom_line() +
        geom_label(aes(label=year),nudge_y=0.04) + 
        geom_point( aes(color=Pathology, size=Pathology)) +
        labs(title=paste(pt, sv))
  plots[[sv]] = gg
  }  
  ga = do.call(grid.arrange, c(plots, list(ncol=2)))
  return(ga)
}

pplots = lapply(subset(sum.patient.data, Status == 'P')$Patient, plot.pca)
#npplots = lapply(subset(sum.patient.data, Status == 'NP')$Patient, plot.pca)
```

```{r echo=F, message=F, warning=F, pcaplots, fig.align='left', fig.height=30, fig.width=20, eval=F}
grid.arrange(grobs=pplots, nrow=length(subset(sum.patient.data, Status == 'P')$Patient), top="Progressors")
#grid.arrange(grobs=npplots, nrow=length(subset(sum.patient.data, Status == 'NP')$Patient), top="Non-Progressors")
```

```{r persample, eval=F} 

lapply(patient.data, function(df) {
  
  lapply(df$seg.vals[,-(1:5)], quantile)
  
})

par(mfrow=c(2,2))
  lapply(patient.data$AHM0896$seg.vals[,-(1:5)], hist)

summary(sapply(patient.data$AHM0896$seg.vals[,-(1:5)], mean))
lapply(lapply(patient.data$AHM0896$seg.vals[,-(1:5)], kmeans, 4), function(fit) fit$centers)
  
  
km.fits = lapply(patient.data, function(df) kmeans(df$seg.vals[,-(1:5)], 4))
autoplot(km.fits$AHM0896, patient.data$AHM0896$seg.vals[,-(1:5)])

plot(patient.data$AHM0896$seg.vals[,-(1:5)], col=km.fits$AHM0896$cluster, pch=19)




  ## TODO find an OCCAMS patient with genome doubling, downsample and compare
  
```

# Clustering

One of the questions we'd like to answer is how to separate the progressors from the non-progressors before we see the pathological progression (HGD or even LGD).  If there is increasing instability in the genome how do we characterize it?

## Mixture Models

I tried were several mixture models (e.g. EM, kmeans) but wasn't able to find a pattern that could reliably separate the distributions.

### Raw segmented values 

First using EM (mclust) looking at three different values for G (2,3,4).

#### EM
```{r clust, echo=F, message=F, warning=F, fig.height=4, fig.width=4, eval=F}
em.dr<-function(patient, status, x1, var.regions) {
  cat(paste('**', patient, '**'))
  clusters=c(2:4)
  
  llk = list()
  
  for (g in clusters) {
    print(g)
    model = densityMclust(x1, G=g)
    highCluster = which.max(colSums(model$parameters$mean))
    plot(model, what='density')
    # tryCatch({
    #   mod1dr = MclustDR(model, lambda = 0.5)
    #   
    #   #     par(mfrow=c(2,1)) 
    #   plot(mod1dr, what="evalues")
    #   title(paste(pt, '(',status,') G=', g, sep=''))
    #   plot(mod1dr, what='classification') # while this looks like PCA, it's not quite
    #   title(paste(pt, '(',status,') G=', g, sep=''))
    #   #     dev.off()
    #   plot(mod1dr, what='density', dimens=1)
    #   title(paste(pt, '  G=', g, sep=''))
    # }, error = function(e) {
    #   cat(paste(pt, g, "failed DR"))
    # })
    # 
    # a1 = rownames(x1[which(model$classification == highCluster),] )
    # if (nrow(var.regions) > 0) {
    #   am = merge(a1, var.regions, all=T)
    #   pander( as.data.frame(rownames(x1[which(model$classification == highCluster),])), caption=paste("From the cluster with the highest means the regions above are selected, of these ",length(setdiff(rownames(var.regions), a1))," regions are not found in the variable regions selected using cutoffs.", length(intersect(rownames(var.regions), a1)), " are in both."))
    # } else {
    #   pander(as.data.frame(a1),caption=paste("From the cluster with the highest means the regions above are selected. This patient had no variable regions selected based on cutoffs"))
    # 
    # }
    llk[[as.character(g)]] = model
  }
  return(llk)
}

llk = as.data.frame(matrix(nrow=nrow(sum.patient.data),ncol=3,dimnames=list(c(sum.patient.data$Patient), c(2:4))))
for (pt in sum.patient.data$Patient) {
  print(pt)
  
    status = subset(sum.patient.data, Patient == pt)$Status

    x1 = patient.data[[pt]]$seg.vals
    rownames(x1) = apply(x1, 1, function(x) strip.whitespace(paste(x[['chrom']], ':', x[['start.pos']], '-', x[['end.pos']], sep=''))) 
    x1 = x1[,patient.data[[pt]]$info$Samplename]

    var = patient.data[[pt]]$variable.regions
    if (nrow(var) > 0)
      rownames(var) = apply(var, 1, function(x) strip.whitespace(paste(x[['chrom']], ':', x[['start.pos']], '-', x[['end.pos']], sep=''))) 

    models = em.dr(pt,status,as.matrix(x1), var[,-(1:5)])
    llk[pt,] = sapply(models, function(x) x$loglik)
  }

ggplot(melt(as.matrix(llk)), aes(x=factor(Var1), y=value, color=factor(Var2))) + 
  geom_point() +
  theme(axis.text.x=element_text(angle = 45)) + 
  labs(title="Raw segment value log liklihoods", x='', y='loglik')


# Log liklihood doesn't separate them individually or the range
llk.diff = apply(llk, 1, function(x) diff(range(x)) )
#t.test(llk.diff[subset(sum.patient.data, Status == 'NP')$Patient], llk.diff[subset(sum.patient.data, Status == 'P')$Patient])

#t.test(llk[subset(sum.patient.data, Status == 'NP')$Patient,'2'], llk[subset(sum.patient.data, Status == 'P')$Patient,'2'])
#t.test(llk[subset(sum.patient.data, Status == 'NP')$Patient,'3'], llk[subset(sum.patient.data, Status == 'P')$Patient,'3'])
#t.test(llk[subset(sum.patient.data, Status == 'NP')$Patient,'4'], llk[subset(sum.patient.data, Status == 'P')$Patient,'4'])
```

#### Kmeans

```{r kmeansraw, echo=F, message=F, warning=F, eval=F, fig.height=6, fig.width=6, eval=F} 
for (pt in sum.patient.data$Patient) {
    status = subset(sum.patient.data, Patient == pt)$Status
    x1 = patient.data[[pt]]$seg.vals
    rownames(x1) = apply(patient.data[[pt]]$seg.vals[,c(1:4)], 1, function(x) strip.whitespace(paste(x[['chrom']], ':', x[['start.pos']], '-', x[['end.pos']], sep=''))) 
    x1 = x1[,patient.data[[pt]]$info$Samplename]
    
    fit = kmeans(x1, 3)
    autoplot(fit, data=x1, label=F) + labs(title=paste(pt, '(', status, ') raw seg', sep=''))
    pander(patient.data[[pt]]$variable.regions[,(1:4)], caption='Variable regions selected by cutoffs')
}
```

### Normalized segmented values

#### EM

Row-normalized (per region)

```{r clust2, echo=F, message=F, warning=F, fig.height=4, fig.width=4, eval=F}
llk = as.data.frame(matrix(nrow=nrow(sum.patient.data),ncol=3,dimnames=list(c(sum.patient.data$Patient), c(2:4))))
for (pt in sum.patient.data$Patient) {
    status = subset(sum.patient.data, Patient == pt)$Status
    x1 = patient.data[[pt]]$norm.seg.vals
    rownames(x1) = apply(patient.data[[pt]]$seg.vals[,c(1:4)], 1, function(x) strip.whitespace(paste(x[['chrom']], ':', x[['start.pos']], '-', x[['end.pos']], sep=''))) 
    x1 = x1[,patient.data[[pt]]$info$Samplename]

    var = patient.data[[pt]]$variable.regions
    if (nrow(var) > 0)
      rownames(var) = apply(var, 1, function(x) strip.whitespace(paste(x[['chrom']], ':', x[['start.pos']], '-', x[['end.pos']], sep=''))) 
  
        models = em.dr(pt,status,as.matrix(x1), var[,-(1:5)])
    llk[pt,] = sapply(models, function(x) x$loglik)
  }

ggplot(melt(as.matrix(llk)), aes(x=Var1, y=value, color=factor(Var2))) + geom_point() + 
  theme(axis.text.x=element_text(angle = 45)) + 
  labs(title="Normalized segment value log liklihoods", x='', y='loglik')

# Log liklihood doesn't separate them individually or the range
llk.diff = apply(llk, 1, function(x) diff(range(x)) )
wilcox.test(llk.diff[subset(sum.patient.data, Status == 'NP')$Patient], llk.diff[subset(sum.patient.data, Status == 'P')$Patient])

#t.test(llk[subset(sum.patient.data, Status == 'NP')$Patient,'2'], llk[subset(sum.patient.data, Status == 'P')$Patient,'2'])
#t.test(llk[subset(sum.patient.data, Status == 'NP')$Patient,'3'], llk[subset(sum.patient.data, Status == 'P')$Patient,'3'])
#t.test(llk[subset(sum.patient.data, Status == 'NP')$Patient,'4'], llk[subset(sum.patient.data, Status == 'P')$Patient,'4'])
```

#### Kmeans
```{r kmeansnorm, echo=F, message=F, warning=F, fig.height=6, fig.width=6, eval=F} 
for (pt in sum.patient.data$Patient) {
    status = subset(sum.patient.data, Patient == pt)$Status
    x1 = patient.data[[pt]]$norm.seg.vals
    rownames(x1) = apply(patient.data[[pt]]$seg.vals[,c(1:4)], 1, function(x) strip.whitespace(paste(x[['chrom']], ':', x[['start.pos']], '-', x[['end.pos']], sep=''))) 
    x1 = x1[,patient.data[[pt]]$info$Samplename]
    
    fit = kmeans(x1, 3)
    print(autoplot(fit, data=x1, label=F) + labs(title=paste(pt, '(', status, ') normalized seg', sep='')))
    pander(patient.data[[pt]]$variable.regions[,(1:4)], caption='Variable regions selected by cutoffs')
}
```

In neither the raw or normalized matrices could I reliably distinguish clusters from each other within a single patient. Some patients separate well (AH0254) others don't separate at all.  I looked as well for a high level measuremeant that might separate the patients by progressor/non. Log liklihoods did not show any difference between patients in the mixture model, and with poor clusters separation I was a bit stumped as to what else might work.


## AP Clustering

```{r apclust-func, echo=F, message=F, warning=F}
suppressPackageStartupMessages( library(apcluster) )

apclust.data<-function(segdata, samples) {
  x1 = as.matrix(segdata[,intersect(colnames(segdata), samples)])
  #x1 = x1[, samples]
  rownames(x1) = (segdata[,c(1:4)] %>%
    rowwise() %>%
    mutate(location=paste(paste(chrom, arm, sep=''), '.', start.pos, '-', end.pos, sep='')))$location
  if (length(samples) == 1) colnames(x1) = samples
  return(x1)  
}

apclust<-function(segdata) {
  x1 = apclust.data(segdata)
  # q=0 minimize off-diagonal similarity
  ac = apcluster(negDistMat(r=2), x1, details=T, convits=25, q=0)
  return(list('apres'=ac, 'data'=x1))
}

net.similarity<-function(ac) {
  if (is.list(ac))
    ac = ac$apres
  ac@netsim
}

sum.similarity<-function(ac) {
  if (is.list(ac))
    ac = ac$apres
  ac@dpsim
}

clusters<-function(ac) {
  if (is.list(ac))
    ac = ac$apres
  ac@exemplars
}

plotAC<-function(aclist) {
  plot(aclist$apres, aclist$data)
} 

heatmapAC<-function(aclist) {
  heatmap(aclist$apres)
}

# First vs last samples
#x1 = apclust.data(patient.data[[pt]]$seg.vals[,c(1:4, 6, ncol(patient.data[[pt]]$seg.vals))], patient.data[[pt]]$info$Samplename)
```

I then tried Affinity Propagation (described by Frey & Dueck, 2007) as that doesn't require the pre-definition for number of clusters. My hypothesis was that the progressors would show a higher number of clusters, and that the variable regions might tend to show up in the same clusters.  

### Raw segmentation values

```{r apclust, echo=F, message=F, warning=F, fig.height=10, fig.width=10, fig.align='left'}
sample.vals = list()
q=0.2

sum.patient.data = arrange(sum.patient.data, Status)

## Using the segmentation values
sum.patient.data$years = with(sum.patient.data, paste(start.year, end.year, sep='-'))
sim = as.data.frame(sum.patient.data[,c('Status','years')])
rownames(sim) = sum.patient.data$Patient
for (pt in sum.patient.data$Patient) {
  print(pt)
  patient.data[[pt]]$info = arrange(patient.data[[pt]]$info,  Pathology, Endoscopy.Year)
  x1 = apclust.data(patient.data[[pt]]$seg.vals, patient.data[[pt]]$info$Samplename)

  gg = ggplot(melt(x1), aes(x=Var2, y=value, group=Var2, fill=Var2)) + 
    geom_boxplot(notch=T, show.legend=F) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position='none') +
    labs(title=paste(pt, patient.data[[pt]]$info$Status[1], sep=' - '), x='') 

  sample.vals[[pt]] = gg

  info = patient.data[[pt]]$info[which(colnames(x1) %in% patient.data[[pt]]$info$Samplename),]
  colnames(x1) = paste(info$Endoscopy.Year, '(', info$Pathology, ')', sep='' )

  ac = apcluster(negDistMat(x1, r=2), q=q, details=T, includeSim=T)
  
  last = ncol(x1)
  if (patient.data[[pt]]$info$Status[1] == 'P' & length(which(grepl('HGD|IMC', info$Pathology)) > 0)  )
    last = sort(grep('HGD|IMC', info$Pathology), decreasing=T)[1]
  if (ncol(x1) <= 15) {
    #plot(ac, x1[,c(c(1:ncol(x1))[-last], last)], main=pt)
  } else {
    #plot(ac, x1[,c(1, sort(sample(c(1:(ncol(x1)-1))[c(-1,-last)], 8)) ,last) ], main=pt)
  }

  sim[pt, 'ns'] = net.similarity(ac)
  sim[pt, 'ss'] = sum.similarity(ac)
  sim[pt, 'clusters'] = length(clusters(ac))
  sim[pt, 'max.sd'] = max(apply(x1, 2, sd))
  
  tmp = patient.data[[pt]]$variable.regions[,-(1:5)]
  sim[pt, 'n.var'] = ifelse(!is.null(tmp) && nrow(tmp)>0, nrow(patient.data[[pt]]$variable.regions[,-(1:5)]), 0)
  
  if (is.null(tmp) || nrow(tmp) <= 0) {
    sim[pt,c('gains', 'loss')] = c(0,0)
  } else {
    sim[pt,'gains'] = length(tmp[tmp>=1.1])
    sim[pt,'loss'] =  length(tmp[tmp<=0.9])
  }
}
sim$id = rownames(sim)
rc = cor.test(sim$max.sd, sim$clusters)

#do.call(grid.arrange, c(sample.vals))

# May be a difference
ttps = wilcox.test( subset(sim, Status == 'NP')$ss, subset(sim, Status == 'P')$ss )$p.value
ttpc = wilcox.test( subset(sim, Status == 'NP')$clusters, subset(sim, Status == 'P')$clusters )$p.value

## Number of variable regions isn't so good, but maybe...
#sim %>%
#  group_by(Status) %>%
#  summarise( mean = mean(n.var), sd = sd(n.var), med = median(n.var))
#t.test( subset(sim, Status == 'NP')$n.var, subset(sim, Status == 'P')$n.var )$p.value

g1 = ggplot(sim[,c('Status','ss')], aes(y=ss, x=Status, fill=Status)) + geom_boxplot() + 
  geom_jitter(width=0.2) + stat_boxplot(geom ='errorbar') +
  labs(title=paste("p-value", signif(ttps, 3)), y='sum similarity')

g2 = ggplot(sim[,c('Status','clusters')], aes(y=clusters, x=Status, fill=Status)) + geom_boxplot() + 
  geom_jitter(width=0.2) + stat_boxplot(geom ='errorbar') +
  labs(title=paste("p-value", signif(ttpc, 3)), y='num. clusters')

sim$Status.Int = as.integer(sim$Status)-1
chisq.test(sim[,c('Status.Int','clusters')])
```

This plot compares the two possible measures of overall complexity from AP. The summed similarity value (top) and the number of clusters (bottom). In this case, the number of clusters appears to be a more robust measure.

```{r echo=F, message=F, warning=F, fig.width=5, fig.height=5}
grid.arrange(g1, g2, top='Raw seg values')
```

Based on the number of clusters in each patient, it appears that there are a higher number of clusters being selected by AP in the progressors vs the non, suggesting it may be able to relate it to diversity in the progressors CN. That metric appears to provide a good separation between the patients as well, though there's a suggestion of two different types of progressors.


```{r apglmclus, echo=F, message=F, warning=F}
precisionRecall<-function(actual, predicted) {
    retrieved <- sum(predicted==1)
    recall <- sum(predicted==1 & actual==1) / sum(actual==1)
    precision <- sum(predicted==1 & actual==1) / retrieved
    #f1 = signif(2*(1/(1/precision+1/recall)), 2)
    f1 =signif(2*precision*recall/(precision+recall), 2)
    return(data.frame('precision'=precision, 'recall'=recall, 'F1'=signif(f1, 2)))
  }

sim = sim[, c('Status', 'clusters', 'ns','ss')]
actualMatrix = as.data.frame(matrix(0,0,4,dimnames=list(c(), c('status','precision','recall', 'F1'))))
for (i in 1:100) {
  trainingRows = sample(1:nrow(sim), nrow(sim)-10)
  train = sim[trainingRows,]
  test = sim[-trainingRows,]

  fit = glm( (as.integer(Status)-1)~clusters, train, family=binomial(link='logit'))
  pred = predict(fit, test, se.fit=F) # probabilities -- of which outcome though?

  #glm_link_scores <- predict(fit, test, type="link")
  #glm_response_scores <- predict(fit, test, type="response")

  actualMatrix = rbind(actualMatrix, cbind('status'='P', precisionRecall(test$Status == 'P', pred>0)))
  actualMatrix = rbind(actualMatrix, cbind('status'='NP', precisionRecall(test$Status == 'NP', pred<0)))
}

g1 = ggplot(actualMatrix, aes(x=recall, y=precision, color=status, label=F1)) + 
  geom_point() + geom_text(aes(label=ifelse(precision>0.5 & recall>0.5, as.character(F1), ''), hjust=0, vjust=0)) +
   geom_hline(yintercept = 0.5, col='grey') + geom_vline(xintercept = 0.5, col='grey') +
   labs(title='Precision/Recall for Progressor/NP prediction')

g2 = ggplot(actualMatrix, aes(x=status, y=F1, group=status, fill=status)) + 
  geom_violin() + geom_jitter(alpha=0.5) + ylim(0,1) + 
  geom_violin(data=subset(actualMatrix,precision>0.5 & recall>0.5), aes(x=status, y=F1, group=status, fill='P&R>0.5') , alpha=0.6) +
  labs(title="F1 statistic")

grid.arrange(g1, g2, top='100 iterations 10f xval')
```

Randomize the data and see if the F1 statistic still holds

```{r gpglmclus_rand, echo=F, message=F, warning=F}
sim = sim[, c('Status', 'clusters')]

rand = as.data.frame(matrix(0,0,4,dimnames=list(c(), c('status','precision','recall', 'F1'))))
for (i in 1:100) {
  # rand.sim = as.data.frame(cbind(  'Status'=sample(c('P','NP'), nrow(sim), replace=T, prob=round(table(sim$Status)/nrow(sim), 2)),
  #                    'clusters'=sample((3:30), nrow(sim), replace=T) ))
  # rand.sim = as.data.frame(cbind(  'Status'=sample(c('P','NP'), nrow(sim), replace=T, prob=round(table(sim$Status)/nrow(sim), 2)),
  #                    'clusters'=sample((3:30), nrow(sim), replace=T) ))

  rand.sim = as.data.frame(cbind(  'Status'=sample(c('P','NP'), nrow(sim), replace=T),
                     'clusters'=sim$clusters ))

  
  rand.sim$clusters = as.integer(as.character(rand.sim$clusters))
  
  trainingRows = sample(1:nrow(rand.sim), nrow(rand.sim)-10)
  train = rand.sim[trainingRows,]
  test = rand.sim[-trainingRows,]

  fit = glm( (as.integer(Status)-1)~clusters, train, family=binomial(link='logit'))
  pred = predict(fit, test, se.fit=F) # probabilities -- of which outcome though?


  rand = rbind(rand, cbind('status'='P', precisionRecall(test$Status == 'P', pred>0)))
  rand = rbind(rand, cbind('status'='NP', precisionRecall(test$Status == 'NP', pred<0)))
}

g1 = ggplot(rand, aes(x=recall, y=precision, color=status, label=F1)) + 
  geom_point() + geom_text(aes(label=ifelse(precision>0.5 & recall>0.5, as.character(F1), ''), hjust=0, vjust=0)) +
   geom_hline(yintercept = 0.5, col='grey') + geom_vline(xintercept = 0.5, col='grey') +
   labs(title='Precision/Recall for Progressor/NP prediction')

g2 = ggplot(rand, aes(x=status, y=F1, group=status, fill=status)) + 
  geom_violin() + geom_jitter(alpha=0.5) + ylim(0,1) + 
  #geom_violin(data=subset(actualMatrix,precision>0.5 & recall>0.5), aes(x=status, y=F1, group=status, fill='P&R>0.5') , alpha=0.6) +
  labs(title="F1 statistic")

grid.arrange(g1, g2, top='100 iterations 10f xval - randomized labels')

```


#### Cellularity?

There is a question about whether the cellularity of the sample drives the differences in the clusters. Overall that does not appear to be the case if we use the maximum standard deviation across all samples in a patient as a proxy for cellularity measure: r=`r rc$estimate`, p-value=`r signif(rc$p.value, 2)`

#### Are the clusters random?

Especially with copy-number the clusters could simply be groups of neighboring regions from the same chromosomes as other analyses have shown high correlations between neighboring CN regions. So are the clusters that AP finds randomly distributed?

```{r example_pt, echo=F, warning=F, message=F, fig.width=10, fig.height=10}
pt = 'PR1_HIN_042'
patient.data[[pt]]$info = arrange(patient.data[[pt]]$info, Endoscopy.Year, Pathology)

x1 = apclust.data(patient.data[[pt]]$seg.vals, patient.data[[pt]]$info$Samplename)
colnames(x1) = paste(patient.data[[pt]]$info$Endoscopy.Year, '(', patient.data[[pt]]$info$Pathology, ')', sep='' )

ac = apcluster(negDistMat(x1, r=2), q=q, details=T, includeSim=T)
plot(ac, x1, main=pt)

cols = c('chr','cluster', 'P.x','x')
hgmx = matrix(nrow=0,ncol=length(cols),dimnames=list(c(), cols))
for (chr in c(1:22)) {
  for (i in 1:length(ac@clusters)) {
    cl = x1[ac@clusters[[i]],]  
    
    x = length(grep(paste('^',chr,'(p|q)',sep=''),  rownames(cl), value=T)) # n in cluster
    m = length(grep(paste('^',chr,'(p|q)',sep=''),  rownames(x1), value=T)) # total n
    n = nrow(x1)-m
    k = length(ac@clusters[[i]])

    hgmx = rbind(hgmx, cbind(chr, i, signif(dhyper(x,m,n,k),3), x))
  }
}

hgmx = as.data.frame(hgmx)
```

```{r hgmx, echo=F, message=F, warning=F, fig.height=6, fig.width=6}

g1 = ggplot(subset(hgmx, x > 0), aes(x=factor(chr),y=P.x,group=chr,fill=factor(chr))) + ylim(0,0.5)+ 
  geom_boxplot() +
  labs(title='Probability of cluster assignment', x='Chromosome', y='P(x)')


g2 = ggplot(subset(hgmx, x > 0), aes(x=factor(cluster),y=P.x,group=cluster,fill=factor(cluster))) + ylim(0,0.5)+ 
  geom_boxplot() +
  labs(title='Probability of cluster assignment', x='Cluster', y='P(x)')

grid.arrange(g1, g2, nrow=2)
```

No cluster in any chromosome reached even a 50% probability, so it appears that the regions are reasonably randomly distributed across clusters rather than being made up of all of chromosome 1 or 17 etc.

```{r echo=F, warning=F, message=F}
ggplot(subset(hgmx, x > 0 & cluster == 9), aes(x=factor(chr),y=P.x,fill=factor(cluster),label=x)) + ylim(0,0.5) + 
  geom_bar(colour='grey', fill='purple', stat='identity', position=position_dodge()) + 
  geom_text(aes(y=P.x+0.005), position=position_dodge()) +
  labs(title='Cluster 9', x='P(x)', y='chromosome')
```

In the purple cluster are those unique regions to the HGD sample. So just as an example, the chromosomes that appear in that cluster are `r paste(subset(hgmx, cluster==9 & x>0)$chr, collapse=', ')` with a total of `r sum(subset(hgmx, cluster==9 & x>0)$x)` regions. The probabilities of each of these having shown up are:

`r pander( subset(hgmx, cluster==9 & x>0), caption="Cluster 9 probabilities are below 20%, or essentially random." )`

### Using the normalized values

So what about using the normalized segmented values for AP clustering instead?

```{r apclustN, echo=F, message=F, warning=F, fig.height=5, fig.width=5, fig.align='left'}
# Should this be performed on the normalized seg values instead?
sim3 = sum.patient.data[,c(2:4)]
rownames(sim3) = sum.patient.data[,1]
for (pt in sum.patient.data$Patient) {
  print(pt)
  x1 = patient.data[[pt]]$norm.seg.vals
  if (is.null(x1)) next
  ac = apcluster(negDistMat(x1, r=2), q=q, details=T, includeSim=T)
  #plot(ac)
  
  sim3[pt, 'ss'] = sum.similarity(ac)
  sim3[pt, 'ns'] = net.similarity(ac)
  sim3[pt, 'clusters'] = length(clusters(ac))
}

summarise(group_by(sim3, Status), mean.ns=mean(ns), sd.ns=sd(ns), mean.c=mean(clusters), sd.c=sd(clusters) )

# Doesn't show up here
ttp = t.test( subset(sim3, Status == 'NP')$ss, subset(sim3, Status == 'P')$ss )$p.value
#ttp = t.test( subset(sim3, Status == 'NP')$ns, subset(sim3, Status == 'P')$ns )$p.value
ttc = t.test( subset(sim3, Status == 'NP')$clusters, subset(sim3, Status == 'P')$clusters )$p.value

g1 = ggplot(sim3[,c('Status','ss')], aes(y=ss, x=Status, fill=Status)) + geom_boxplot() + geom_jitter(width=0.2) + 
  labs(title=paste("Norm seg values p-value", signif(ttp, 3)), y='sum similarity')

g2 = ggplot(sim3[,c('Status','clusters')], aes(y=clusters, x=Status, fill=Status)) + geom_boxplot() + geom_jitter(width=0.2) + 
  labs(title=paste("Norm seg values p-value", signif(ttp, 3)), y='n clusters')

grid.arrange(g1, g2, top='Normalized seg values')

```

The normalized values show no separation between progressor and non using either the summed similarity measure or the number of clusters. So for now we will use the raw segmentation value matrix.

### Variable Regions

Should we call differently?  One is regions that are not diploid, the second would be what changes between samples?

```{r varregions, echo=F, message=F, warning=F, eval=F}
pt = 'AHM0896'

se = round(apply(patient.data[[pt]]$seg.vals[,-(1:5)], 2, function(x) sd(x)/sqrt(length(x))), 3)
sdx = round(apply(patient.data[[pt]]$seg.vals[,-(1:5)], 2, sd), 3)

row=325
patient.data[[pt]]$seg.vals[row,]
round(patient.data[[pt]]$seg.vals[,-(1:5)][row,] + sdx, 1)
round(patient.data[[pt]]$seg.vals[,-(1:5)][row,] - sdx, 1)

lapply(patient.data, function(df) nrow(df$variable.regions))

```


### Cross-validation, LOO

So using the raw segmentation values (not the normalized ones) try some leave one out xval to see if the currently weak differences still hold.

#### Patients first

```{r xval_pt, echo=F, warning=F, message=F, fig.align='left', fig.height=4, fig.width=4, eval=F}
# Leave out patients
loo_xval = matrix(ncol=3, nrow=0, dimnames=list(c(), c('net.similarity', 'sum.similarity', 'clusters')))
for (i in 1:nrow(sum.patient.data)) {
  simL = sum.patient.data[-i,c(2:4)]
  rownames(simL) = sum.patient.data[-i,1]

  for (pt in sum.patient.data$Patient[-i]) {
    x1 = apclust.data(patient.data[[pt]]$seg.vals, patient.data[[pt]]$info$Samplename)
    ac = apcluster(negDistMat(x1, r=2), q=q)
  
    simL[pt, 'ns'] = net.similarity(ac)
    simL[pt, 'ss'] = sum.similarity(ac)
    simL[pt, 'clusters'] = length(clusters(ac))
  }
  
  ttps = t.test( subset(simL, Status == 'NP')$ss, subset(simL, Status == 'P')$ss )$p.value
  ttpn = t.test( subset(simL, Status == 'NP')$ns, subset(simL, Status == 'P')$ns )$p.value
  ttc = t.test( subset(simL, Status == 'NP')$clusters, subset(simL, Status == 'P')$clusters )$p.value

  loo_xval = rbind(loo_xval, cbind(ttpn, ttps, ttc))
}

ggplot(melt(data.frame(loo_xval[,c(3)])), aes(x=variable, y=value, fill=variable)) +  
  geom_boxplot() + geom_jitter(shape=16, width=0.2) + 
  geom_hline(yintercept=0.05, color='red', alpha=0.5) +
  stat_boxplot(geom ='errorbar') + labs(y='p-values', x='Iterations', title='Leave one (patient) out clustering') +
  scale_fill_manual(values=c('purple'), labels='clusters')

```

#### Single samples

```{r xval_smp, echo=F, warning=F, message=F, fig.align='left', fig.height=4, fig.width=4}
# Leave out samples?
looS_xval = matrix(ncol=3, nrow=0, dimnames=list(c(), c('net.similarity', 'sum.similarity', 'clusters')))
for (i in 1:20) { 
  simX = sum.patient.data[,c('Status','years')]
  rownames(simX) = sum.patient.data[,'Patient']

  for (pt in sum.patient.data$Patient) {
      if (length(patient.data[[pt]]$info$Samplename) <= 2) next
      x1 = apclust.data(patient.data[[pt]]$seg.vals, patient.data[[pt]]$info$Samplename[-sample(1:(length(patient.data[[pt]]$info$Samplename)), 1)])
      ac = apcluster(negDistMat(x1, r=2), q=q)
  
      simX[pt, 'ns'] = net.similarity(ac)
      simX[pt, 'ss'] = sum.similarity(ac)
      simX[pt, 'clusters'] = length(clusters(ac))
    }
  
  ttpn = t.test( subset(simX, Status == 'NP')$ns, subset(simX, Status == 'P')$ns )$p.value
  ttps = t.test( subset(simX, Status == 'NP')$ss, subset(simX, Status == 'P')$ss )$p.value
  ttc = t.test( subset(simX, Status == 'NP')$clusters, subset(simX, Status == 'P')$clusters )$p.value

  looS_xval = rbind(looS_xval, cbind(ttpn,ttps, ttc))
}

ggplot(melt(data.frame(looS_xval[,c(3)])), aes(x=variable, y=value, fill=variable)) +  
    geom_boxplot() + geom_jitter(shape=16, position=position_jitter(0.2)) +
  geom_hline(yintercept=0.05, color='red', alpha=0.5) +
    stat_boxplot(geom ='errorbar') + labs(y='p-values', x='Iterations', title='Leave one (sample) out clustering') + 
    scale_fill_manual(values=c('#CC99FF'), labels='clusters')

```


## No HGD samples

There is still a difference between the progressors and non, though the p-value does increase. Some of the progressors do start looking more like NPs in their similarity measure but not all of them.

```{r nofinal, echo=F, message=F, warning=F, fig.height=5, fig.width=5}

gain.threshold = 1.1
loss.threshold = 0.9

simX = sum.patient.data[,c('Status','years')]
rownames(simX) = sum.patient.data[,'Patient']
# Leave out final sample on the progressors 

for (pt in sum.patient.data$Patient) {
  x1 = apclust.data(patient.data[[pt]]$seg.vals, patient.data[[pt]]$info$Samplename)
  if (subset(sum.patient.data, Patient == pt)$Status == 'P' & length(which(grepl('HGD|IMC', patient.data[[pt]]$info$Pathology))) > 0) {
    final = grep('HGD|IMC', patient.data[[pt]]$info$Pathology)
    
    if (length(patient.data[[pt]]$info$Samplename[-final]) <= 1) next
    
    x1 = apclust.data(patient.data[[pt]]$seg.vals, patient.data[[pt]]$info$Samplename[-final])
  }
  ac = apcluster(negDistMat(x1, r=2), q=q)
  
  simX[pt, 'ns'] = net.similarity(ac)
  simX[pt, 'ss'] = sum.similarity(ac)
  simX[pt, 'clusters'] = length(clusters(ac))
  
  simX[pt, 'n.var'] = nrow(patient.data[[pt]]$variable.regions[,-(1:5)])
}
 
ct = cor.test(simX$clusters, simX$n.var)
 
ttpc = t.test( subset(simX, Status == 'NP')$clusters, 
               subset(simX, Status == 'P')$clusters )$p.value

ggplot(melt(data.frame(simX[c('Status','clusters')])), aes(x=Status, y=value, fill=Status)) +  
    geom_boxplot() + geom_jitter(shape=16, position=position_jitter(0.2)) +
    stat_boxplot(geom ='errorbar') + labs(y='num. clusters', x='', title=paste('Leave out HGD sample(s) p-value', signif(ttpc, 3)))


#fit = glm(Status~clusters, simX, family=binomial(link='logit') )

simX = simX[, c('Status', 'clusters', 'ns','ss','n.var')]
noHGDPR = as.data.frame(matrix(0,0,4,dimnames=list(c(), c('status','precision','recall', 'F1'))))
for (i in 1:100) {
  trainingRows = sample(1:nrow(simX), nrow(simX)-10)
  train = simX[trainingRows,]
  test = simX[-trainingRows,]
  
  fit = glm( Status~clusters, train, family=binomial(link='logit'))
  pred = predict(fit, test, se.fit=F) # probabilities -- of which outcome though?
  
  noHGDPR = rbind(noHGDPR, cbind('status'='P', precisionRecall(test$Status == 'P', pred>0)))
  noHGDPR = rbind(noHGDPR, cbind('status'='NP', precisionRecall(test$Status == 'NP', pred<0)))
  
}

g1 = ggplot(noHGDPR, aes(x=recall, y=precision, color=status, label=F1)) + 
  geom_point() + geom_text(aes(label=ifelse(precision>0.5 & recall>0.5, as.character(F1), ''), hjust=0, vjust=0)) +
   geom_hline(yintercept = 0.5, col='grey') + geom_vline(xintercept = 0.5, col='grey') +
   labs(title='Precision/Recall for Progressor/NP prediction without HGD')

g2 = ggplot(subset(actualMatrix, precision>0.5 & recall>0.5), aes(x=status, y=F1, group=status, fill=status)) + 
  geom_violin() + geom_jitter(alpha=0.5) + ylim(0.5,1) + labs(title="F1 statistic at prec+recall>0.5")

grid.arrange(g1, g2, top='100 iterations 10f xval no HGD')

```

Currently, without the HGD/IMC samples we can predict progressors `r nrow(subset(noHGDPR, precision > 0.5))/nrow(noHGDPR)*100`% of the time.
In comparison, including HGD/IMC samples our predictions improve to `r length(which(actualMatrix[,'precision'] > 0.5))/nrow(actualMatrix)*100`%.  It's also worth noting that the number of clusters has a significant (pval=`r signif(ct$p.value, 1)`) correlation to the number of variant regions identified across all samples for that patient (r=`r round(ct$estimate, 2)`).


There do appear to be some patients that are more similar to non-progressors without their HGD samples:
`r pander(subset(simX, Status == 'P' & clusters <= mean(subset(simX, Status == 'NP')$clusters)), caption="These progressor patients fall below the mean number of clusters in the non-progressor group, suggesting they may be more similar to NP before their HGD samples.")`

```{r}

pts = rownames(subset(simX, Status == 'P' & clusters <= mean(subset(simX, Status == 'NP')$clusters)))

subset(sum.patient.data, Patient %in% pts)


```

 
## AP cluster parameter selection

Just a quick note on the selection of the q parameter for apcluster. The q parameter initializes the exemplar preferences.  q=0.5 starts at the median, values between 0-1 use the sample quantile with threshold q (per documentation).

```{r whyq, echo=F, message=F, warning=F}
sum.patient.data = arrange(sum.patient.data, Status)

pvals = as.data.frame(matrix(ncol=3,nrow=0,dimnames=list(c(), c('p.clusters','mean.clusters', 'sd.clusters'))))
status.clusters = list()
for (qv in seq(0,0.9,0.1)) {
  ## Using the segmentation values
  simPC = sum.patient.data[,c(2:4)]
  rownames(simPC) = sum.patient.data[,1]
  for (pt in sum.patient.data$Patient) {
    patient.data[[pt]]$info = arrange(patient.data[[pt]]$info, Endoscopy.Year, desc(Pathology))
  
    x1 = apclust.data(patient.data[[pt]]$seg.vals, patient.data[[pt]]$info$Samplename)
    colnames(x1) = paste(patient.data[[pt]]$info$Endoscopy.Year, '(', patient.data[[pt]]$info$Pathology, ')', sep='' )
  
    ac = apcluster(negDistMat(x1, r=2), q=qv, convits=75)
    
    simPC[pt, 'ns'] = net.similarity(ac)
    #simPC[pt, 'ss'] = sum.similarity(ac)
    simPC[pt, 'clusters'] = length(clusters(ac))
  }
  
  # May be a difference
  #ttps = t.test( subset(simPC, Status == 'NP')$ss, subset(simPC, Status == 'P')$ss )$p.value
  ttpn = t.test( subset(simPC, Status == 'NP')$ns, subset(simPC, Status == 'P')$ns )$p.value
  ttpc = t.test( subset(simPC, Status == 'NP')$clusters, subset(simPC, Status == 'P')$clusters )$p.value

  pvals = rbind(pvals,cbind(ttpc, mean(simPC$clusters), sd(simPC$clusters)))
  
  by.clusters = simPC %>% group_by(Status) %>% summarise( n.mean.clusters=mean(clusters), n.sd.clusters=sd(clusters), se.clusters=sd(clusters)/sqrt(length(Status)) )
  
  status.clusters[[paste('q=',qv, sep='')]] = by.clusters
}
colnames(pvals) = c('p.clusters','mean.clusters', 'sd.clusters')
pvals$qv=seq(0,0.9,0.1)

clus = as.data.frame(do.call(rbind, lapply(status.clusters, function(x) c(x[['n.mean.clusters']], x[['n.sd.clusters']]  ))))
colnames(clus) = c('NP','P', 'NP.sd', 'P.sd')
clus$qv = seq(0,0.9,0.1)

m = melt(clus[,c(1:2,5)], measure.vars=c('NP','P'))
m[which(m$variable == 'NP'), 'sd'] = clus$NP.sd
m[which(m$variable == 'P'), 'sd'] = clus$P.sd

ggplot(m, aes(x=factor(qv), y=value, col=variable)) + geom_point() +
  geom_errorbar(aes(ymin=value-sd, ymax=value+sd), width=.02) +
  geom_hline(yintercept=5, alpha=0.3) + 
  geom_hline(yintercept=15, alpha=0.3, col='red') + 
  scale_y_continuous(breaks=seq(0,max(m$value)+5, 10),labels=seq(0,max(m$value)+5, 10)  ) +
  labs(title='Clusters vs q parameters', x='q parameter', y='mean(clusters)') 


```

The number of clusters identified increases with higher q values as would be expected: 0 finds 1 cluster, 1 finds n(regions) clusters. We need to see at least 2 clusters (gains/losses), and more than 15 clusters doesn't increase the information available to us right now. At every q value there is a separation between progressors and non, but the mean error increases in progressors with the number clusters in progressors as well. So we selected a q value to: 1) allow for a minimum of 2 clusters, but no more than 15; and minimize the mean error.

`r pander(clus[,c('qv','NP','P')], caption="Mean of clusters by patient status per q parameter.")`

```{r echo=F, warning=F, message=F, fig.align='left', fig.height=4, fig.width=4}

ggplot(melt(pvals, measure.vars=c('p.clusters'), id.vars='qv'),aes(x=factor(qv),y=value, color=variable)) +
  geom_point() + geom_hline(yintercept=0.05, alpha=0.3, col='red') + labs(title='P vs NP p-values per q parameters', y='pvalue NP vs P', x='q param')
```

As an additional note, the separation between progressors and non is stable at q values between 0.1 and 0.7.

## Investigating Cutoff selection

Currently we use the same cutoffs across all patients and samples for selecting variable regions. While it's clear based on the data that values ~1 are the average ploidy of the sample and are most likely to be diploid, some samples have a greater spread round 1 resulting in the possibility that we are both under and over calling variable regions.

In these plots there are several things occurring.  The values have been log-10 transformed in order to spread them out. These are then plotted in a histogram with two sets of lines representing potential cutoffs: one for EM derived means, the other for mean(+|-)sd*1.5.  In most cases EM does not provide much separation. This could potentially be improved by adding priors or other parameters to the model, but it's unclear how to initialize these usefully.


```{r varcall, echo=F, message=F, warning=F, fig.height=10, fig.width=10}
for (pt in names(patient.data)) {
  cols = intersect(arrange(patient.data[[pt]]$info, Pathology, Endoscopy.Year)$Samplename, colnames(patient.data[[pt]]$seg.vals[,-(1:5)]))
  segvals = patient.data[[pt]]$seg.vals[,cols]

  plot.seg.vals<-function(lt, title, hist.col='blue') {
    sdx = apply(lt, 2, sd)
    se = apply(lt, 2, function(x) sd(x)/sqrt(length(x)))
    means = t(do.call(rbind, lapply(lapply(lt, densityMclust, G=3), function(x) sort(x$parameters$mean))))
    m = merge(melt(lt), melt(means), by.x='variable', by.y='Var2')
    gg = ggplot(m,aes(value.x, ..density..)) + 
      facet_wrap(~variable,scales = "free_x") + 
        geom_histogram(bins=10, fill=hist.col, color='grey') +
        geom_density(aes(value.x), color='darkgrey', linetype='dotted') +
        geom_vline( aes(xintercept=value.y, col='EM'), linetype='twodash') +
        geom_vline(aes(xintercept = mean(value.x)+sd(value.x)*1.5, color='StdDev'), linetype='twodash') +
        geom_vline(aes(xintercept = mean(value.x)-sd(value.x)*1.5, color='StdDev'),linetype='twodash') +
        labs(title=title)
    return(gg)
  }
  
  gg1 = plot.seg.vals(segvals, title=paste(pt, ' (',patient.data[[pt]]$info$Status[1],')', sep=''))
  gg2 = plot.seg.vals(log2(segvals), title=paste(pt, ' log2  (',patient.data[[pt]]$info$Status[1],')', sep=''), hist.col='darkgreen')
  gg3 = plot.seg.vals(log10(segvals), title=paste(pt, ' log10  (',patient.data[[pt]]$info$Status[1],')', sep=''), hist.col='darkred')

  print(gg1)
  print(gg2)
  print(gg3)
  #do.call(grid.arrange, list(gg1, gg2, gg3))


}
```



# Conclusion

At the moment AP clustering appears to give us a reliable "diversity" measure between progressors and non. It also suggests greater complexity in the progressor patients which we would expect based on earlier analyses. The exemplars need to be further analyzed to see how they may relate across patients, but also exactly what they represent within a single patient. Finally, it may be worth assuming absolute copy number and discretizing the matrix to test it. 

