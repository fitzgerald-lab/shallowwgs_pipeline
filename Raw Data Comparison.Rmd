---
title: "Raw Data Comparison"
author: "Sarah Killcoyne"
date: "23/08/2019"
output: 
  html_document: 
    fig_height: 4
    fig_width: 6
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F)

library(tidyverse)
library(BarrettsProgressionRisk)
library(reshape2)
library(gridExtra)
library(kableExtra)
library(pander)
library(e1071)

chr.lengths = BarrettsProgressionRisk:::chrInfo()
blacklist = readr::read_tsv(system.file("extdata", "qDNAseq_blacklistedRegions.txt", package="BarrettsProgressionRisk"), col_names=T, col_types='cii')  

load('~/Data/BarrettsProgressionRisk/QDNAseq/training/merged_qdnaseq_output.Rdata',verbose=T)
tfit.data = fit.data %>% as_tibble %>% mutate(chrom = factor(chrom, levels = chr.lengths$chr, ordered=T)) %>% arrange(chrom, start)
traw.data = raw.data %>% as_tibble %>% mutate(chrom = factor(chrom, levels = chr.lengths$chr, ordered=T)) %>% arrange(chrom, start)
rm(fit.data, raw.data)

val.file = '~/Data/BarrettsProgressionRisk/QDNAseq/validation/sWGS_validation_batches.xlsx'
sheets = readxl::excel_sheets(val.file)[8:13]

pastefun<-function(x) {
  if ( !grepl('SLX-', x) ) x = paste0('SLX-',x)
  return(x)
}

all.val = do.call(bind_rows, lapply(sheets, function(s) {
  readxl::read_xlsx(val.file, s) %>% dplyr::select(`Hospital Research ID`, matches('Status'), `Block ID`,`Sample Type`, `SLX-ID`, `Index Sequence`, Cohort, Batch, RA, matches('Collection')) %>% 
    dplyr::filter(!is.na(`SLX-ID`)) %>% mutate_at(vars(`SLX-ID`, `Block ID`), list(as.character)) 
})) %>% mutate(Samplename = paste(`SLX-ID`,`Index Sequence`, sep='.')) %>% 
  rowwise %>% mutate_at(vars(`SLX-ID`), list(pastefun) ) %>% ungroup %>% 
  mutate(
  `Hospital Research ID` = str_replace_all( str_remove_all(`Hospital Research ID`, " "), '/', '_'), 
  `Index Sequence` = str_replace_all(`Index Sequence`, 'tp', ''),
  Samplename = paste(`SLX-ID`,`Index Sequence`,sep='.')
  )


load('~/Data/BarrettsProgressionRisk/QDNAseq/validation/merged_raw_fit.Rdata',verbose=T)
vfit.data = merged.fit %>% as_tibble %>% mutate(chrom = factor(chrom, levels = chr.lengths$chr, ordered=T)) %>% 
  arrange(chrom, start) %>% dplyr::select(location,chrom,start,end, all.val$Samplename)
vraw.data = merged.raw %>% as_tibble %>% mutate(chrom = factor(chrom, levels = chr.lengths$chr, ordered=T)) %>% 
  arrange(chrom, start) %>% dplyr::select(location,chrom,start,end, all.val$Samplename)
rm(merged.fit, merged.raw)

##dim(vfit.data)
##dim(tfit.data)
```



# Raw counts & fitted values

This is the raw data that gets segmented (after some processing).  It looks like there may be a slight shift in the distribution (per patient) of these values but it's not extremely high.

```{r}

summarise.per.pt<-function(df) {
  means = df %>% dplyr::select(-matches('location|chrom|start|end')) %>% summarise_all( list(~mean(.,na.rm=T))  ) 
  sds = df %>% dplyr::select(-matches('location|chrom|start|end')) %>% summarise_all( list(~sd(.,na.rm=T))  ) 
  vars = df %>% dplyr::select(-matches('location|chrom|start|end')) %>% summarise_all( list(~var(.,na.rm=T))  ) 
  iqrs = df %>% dplyr::select(-matches('location|chrom|start|end')) %>% summarise_all( list(~IQR(.,na.rm=T))  ) 

  disp = full_join(
           full_join( as_tibble(t(means), rownames='sample') %>% dplyr::rename('mean' = 'V1'), 
                      as_tibble(t(sds), rownames='sample') %>% dplyr::rename('sd' = 'V1'),  by='sample'),
           full_join(as_tibble(t(vars), rownames='sample') %>% dplyr::rename('var' = 'V1'), 
                     as_tibble(t(iqrs), rownames='sample') %>% dplyr::rename('IQR' = 'V1'),  by='sample'), 
           by='sample') 
  return(disp)
}

t.fit.disp = summarise.per.pt(tfit.data) %>% mutate(cohort='train')
v.fit.disp = summarise.per.pt(vfit.data) %>% mutate(cohort='val')

pander(wilcox.test(t.fit.disp$mean, v.fit.disp$mean), caption='Per pt mean comparison (fitted values)')

# Within the cohorts they're comparable
x = t.fit.disp$mean[sample(500,replace = F)]
pander(wilcox.test(x[1:250], x[251:500]),caption='Within training (sampled)') 

y = v.fit.disp$mean[sample(100,replace = F)]
pander(wilcox.test(y[1:50], y[51:50]), caption='Within validation (sampled')

pander(wilcox.test(t.fit.disp$var, v.fit.disp$var), catpion='Per pt var comparison (fitted values)')

fit.disp = bind_rows(t.fit.disp, v.fit.disp)

gridExtra::grid.arrange(
  ggplot(fit.disp, aes(x=cohort, y=mean, group=cohort, fill=cohort)) + geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none'),
  ggplot(fit.disp, aes(x=cohort, y=sd, group=cohort, fill=cohort)) + geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none'),
  ncol=2, top='Raw fitted values')

t.raw.disp = summarise.per.pt(traw.data) %>% mutate(cohort='train')
v.raw.disp = summarise.per.pt(vraw.data) %>% mutate(cohort='val')

pander(wilcox.test(t.raw.disp))

raw.disp = bind_rows(t.raw.disp, v.raw.disp)

gridExtra::grid.arrange(
  ggplot(raw.disp, aes(x=cohort, y=mean, group=cohort, fill=cohort)) + geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none'),
  ggplot(raw.disp, aes(x=cohort, y=sd, group=cohort, fill=cohort)) + geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none'), 
  ncol=2, top='Raw count values')


rm(tfit.data,traw.data,vfit.data,vraw.data)
```

The samples from the training cohort that have a mean>300 (outliers) are progressors, but did not all predict high risk in LOO.


# First processing steps

In the initial processing the raw counts are adjusted using the fitted values. 

```{r}
if (file.exists('prepped.Rdata')) {
  load('prepped.Rdata',verbose=T)
} else {
  t.prepped = BarrettsProgressionRisk:::.prepRawSWGS(traw.data, tfit.data, blacklist, verbose=T, plot=F)
  dim(t.prepped$data)
  
  v.prepped = BarrettsProgressionRisk:::.prepRawSWGS(vraw.data, vfit.data, blacklist, verbose=T, plot=F)
  dim(v.prepped$data)
  
  t.sdevs = t.prepped$sdevs
  v.sdevs = v.prepped$sdevs
  rm(t.prepped,v.prepped)
  
  save(t.sdevs, v.sdevs, file='prepped.Rdata')  
}

# The sdevs are used to adjust the gamma values for segmentation
wilcox.test(t.sdevs, v.sdevs)

sdevs = bind_rows(tibble('sdev'=t.sdevs, 'cohort'=rep('train',length(t.sdevs))),
          tibble('sdev'=v.sdevs, 'cohort'=rep('val',length(v.sdevs))))

# Validation has one extreme value, but this is per-sample so it should only affect that sample
ggplot(sdevs, aes(x=cohort, y=sdev, group=cohort, fill=cohort)) + ylim(0,0.5) +
  geom_jitter(width=0.1) + geom_violin(alpha=0.5) + 
  theme(legend.position = 'none') + labs(title='sdevs')

ggplot(sdevs, aes(sdev)) + geom_histogram(stat='density',bins=50, color='lightblue') + xlim(0,0.5) + facet_wrap(~cohort)
```

# Segmented Data

General distributions. These are per-patient on the segmented (but not yet binned) data. It looks like there is a shift in the per-patient distributions. 

```{r, fig.height=10, fig.width=10}

t.disp = read_tsv(list.files('~/Data/BarrettsProgressionRisk/Analysis/multipcf_perPatient', pattern='disp', full.names = T, recursive = T), col_types = 'cddddddd') %>% mutate(cohort='train')
v.disp = read_tsv(list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/multipcf', pattern='disp', full.names = T, recursive = T), col_types = 'cddddddd') %>% mutate(cohort='val')

v.disp %>% filter(sd > 0.15)


x = bind_rows(t.disp, v.disp)

gridExtra::grid.arrange(
  ggplot(x, aes(y=mean,x=cohort,group=cohort,fill=cohort)) + geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') + labs(main='mean', x=''),
  ggplot(x, aes(y=sd,x=cohort,group=cohort,fill=cohort)) + geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') + labs(main='StDev', x=''),
  ggplot(x, aes(y=var,x=cohort,group=cohort,fill=cohort)) + geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') + labs(main='var', x=''),
  ggplot(x, aes(y=IQR,x=cohort,group=cohort,fill=cohort)) + geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') + labs(main='IQR', x=''),
ncol=2, top='Segmented data distribution disp')

wilcox.test(t.disp$mean, v.disp$mean)
wilcox.test(t.disp$sd, v.disp$sd)

```


Looked at the fourier periodograms for both the training and validation data.  The RHS values are based on a frequency cutoff of 0.25. This does suggest that there's more noise in the validation data overall after segmentation.

I tried decreasing this by more agressively segmenting some samples with a higher RHS, but while the RHS drops it doesn't change the overall predictions significantly.  Given the upward shift in the raw and segmented values it may simply be that the shift is affecting it.

```{r rhs}
t.rhs = read_tsv(list.files('~/Data/BarrettsProgressionRisk/Analysis/multipcf_perPatient/', pattern='rhs', full.names = T, recursive = T), col_types = 'cdd') %>% mutate('cohort'='train')
v.rhs = read_tsv(list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/', pattern='rhs', full.names = T, recursive = T), col_types='cdd') %>% mutate('cohort'='val')

ggplot(bind_rows(t.rhs %>% dplyr::select(rhs,cohort), v.rhs %>% dplyr::select(rhs,cohort)), aes(x=cohort, y=rhs, group=cohort)) + 
  geom_jitter(width=0.2) + geom_violin(aes(fill=cohort),alpha=0.5, show.legend = F) + theme_minimal()

wilcox.test(t.rhs$rhs, v.rhs$rhs)
t.test(t.rhs$rhs, v.rhs$rhs)

```


Example
```{r loadrawval, echo=F, warning=F, message=F}
samples = v.rhs %>% arrange(-rhs) %>% dplyr::slice(1,nrow(v.rhs))

load('~/Data/BarrettsProgressionRisk/QDNAseq/validation/merged_raw_fit.Rdata', verbose=T)

fit.data = merged.fit %>% dplyr::select(location, chrom, start, end, samples$Sample)
raw.data = merged.raw %>% dplyr::select(location, chrom, start, end, samples$Sample)
rm(merged.fit, merged.raw)

```

Highest RHS
```{r highRHS}
if (file.exists('highRHS.Rdata')) {
  load('highRHS.Rdata')
} else {
  info = loadSampleInformation(tibble(Sample=samples$Sample[1], Endoscopy='2010/01/01'))

  # Default gamma is 250* the mad(std dev of the adjusted data)
  seg250 = BarrettsProgressionRisk::segmentRawData(info, raw.data[,c(1:5)], fit.data[,c(1:5)], cutoff = 0.011, gamma2=250, verbose = T)
  prr250 = predictRiskFromSegments(seg250, verbose=F)
  
  seg500 = BarrettsProgressionRisk::segmentRawData(info, raw.data[,c(1:5)], fit.data[,c(1:5)], cutoff = 0.011, gamma2=500, verbose = T)

  prr500 = predictRiskFromSegments(seg500, verbose=F)
  save(seg250,prr250,seg500,prr500,file='highRHS.Rdata')
}

# Gamma 250
pd250 = TSA::periodogram(seg250$seg.vals[[samples$Sample[1]]], plot=T, main='gamma 250') 
plotSegmentData(seg250) 
# number of segs
nrow(seg250$seg.vals)
# rhs
print( sum(pd250$freq[pd250$freq > 0.25]) )


# gamma 500
pd500 = TSA::periodogram(seg500$seg.vals[[samples$Sample[1]]], plot=T, main='gamma 500') 
plotSegmentData(seg500) 
# number of segs
nrow(seg500$seg.vals)
# rhs
print( sum(pd500$freq[pd500$freq > 0.25]) )


# Relative risks does drop, but not hugely
predictions(prr250)
predictions(prr500)

# These come from a NP
all.val %>% dplyr::filter(Samplename == samples$Sample[1])
```


Lowest RHS.  But again, a more aggressive segmentation isn't really altering things.  
```{r lowRHS}
if (file.exists('lowRHS.Rdata')) {
  load('lowRHS.Rdata')
} else {
  info = loadSampleInformation(tibble(Sample=samples$Sample[2], Endoscopy='2010/01/01'))
  
  seg250 = BarrettsProgressionRisk::segmentRawData(info, raw.data[,c(1:4,6)], fit.data[,c(1:4,6)], cutoff = 0.011, gamma2=250, verbose = F)
  prr250 = predictRiskFromSegments(seg250, verbose=F)
  
  seg500 = BarrettsProgressionRisk::segmentRawData(info,  raw.data[,c(1:4,6)], fit.data[,c(1:4,6)], cutoff = 0.011, gamma2=500, verbose = F)

  prr500 = predictRiskFromSegments(seg500, verbose=F)
  save(seg250,prr250,seg500,prr500,file='lowRHS.Rdata')
}

pd250 = TSA::periodogram(seg250$seg.vals[[samples$Sample[2]]], plot=T, main='gamma 250') 
plotSegmentData(seg250)
# number of segs
nrow(seg250$seg.vals)
# rhs
print( sum(pd250$freq[pd250$freq > 0.25]) )


pd500 = TSA::periodogram(seg500$seg.vals[[samples$Sample[2]]], plot=T, main='gamma 500') 
plotSegmentData(seg500)
# number of segs
nrow(seg500$seg.vals)
# rhs
print( sum(pd500$freq[pd500$freq > 0.25]) )

predictions(prr250)
predictions(prr500)

all.val %>% dplyr::filter(Samplename == samples$Sample[2])

rm(prr500, prr250, pd500, pd250, seg500, seg250)
```


# Binned Data

5MB bins.  For most bins this involves a weight average of 2 segments that overlap the bin.

```{r, echo=F, message=F, warning=F}
tile.dist.measure<-function(df) {
  means = as_tibble(df %>% dplyr::summarise_at(vars(-Sample), list(~mean(.,na.rm=T))) %>% t, rownames='bin')  %>% dplyr::rename('mean' = 'V1')
  sds = as_tibble(df %>% dplyr::summarise_at(vars(-Sample), list(~sd(.,na.rm=T))) %>% t, rownames='bin')  %>% dplyr::rename('sd' = 'V1')
  iqrs = as_tibble(df %>% dplyr::summarise_at(vars(-Sample), list(~IQR(.,na.rm=T))) %>% t, rownames='bin')  %>% dplyr::rename('iqrs' = 'V1')
  vars = as_tibble(df %>% dplyr::summarise_at(vars(-Sample), list(~var(.,na.rm=T))) %>% t, rownames='bin')  %>% dplyr::rename('var' = 'V1')
  
  skew = as_tibble(df %>% dplyr::summarise_at(vars(-Sample), list(~skewness(.,na.rm=T))) %>% t, rownames='bin')  %>% dplyr::rename('skew' = 'V1')
  kurt = as_tibble(df %>% dplyr::summarise_at(vars(-Sample), list(~kurtosis(.,na.rm=T))) %>% t, rownames='bin')  %>% dplyr::rename('kurtosis' = 'V1')
  
  
  dist = means %>% full_join(sds, by='bin') %>% full_join(iqrs, by='bin') %>% full_join(vars, by='bin') %>% full_join(skew, by='bin') %>% full_join(kurt, by='bin')
  return(dist)
}

files = list.files('~/Data/BarrettsProgressionRisk/Analysis/multipcf_perPatient/', '5e06_cleaned_tiled', recursive=T, full.names = T)
training.tiles = do.call(bind_rows, purrr::map(files, function(f) {
  read_tsv(f,col_types=c(.default=col_double()))
})) %>% dplyr::rename('Sample' = 'X1')

files = list.files('~/Data/BarrettsProgressionRisk/Analysis/multipcf_perPatient/', 'arms_cleaned', recursive=T, full.names = T)
training.arms = do.call(bind_rows, purrr::map(files, function(f) {
  read_tsv(f,col_types=c(.default=col_double()))
})) %>% dplyr::rename('Sample' = 'X1')

files = list.files('~/Data/BarrettsProgressionRisk/Analysis/multipcf_perPatient/', '5e06.*MSE', recursive=T, full.names = T)
training.resids = do.call(bind_rows, purrr::map(files, function(f) {
  read_tsv(f,col_types=c(.default=col_double()))
})) %>% dplyr::rename('Sample' = 'X1')

files = list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/multipcf/', '5e06_tiles', recursive=T, full.names = T)
val.tiles = do.call(bind_rows, purrr::map(files, function(f) {
  read_tsv(f,col_types=c(.default=col_double()))
})) %>% dplyr::rename('Sample' = 'X1')

files = list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/multipcf/', 'arm_tiles', recursive=T, full.names = T)
val.arms = do.call(bind_rows, purrr::map(files, function(f) {
  read_tsv(f,col_types=c(.default=col_double()))
})) %>% dplyr::rename('Sample' = 'X1')


files = list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/multipcf/', '5e06_MSE', recursive=T, full.names = T)
val.resids = do.call(bind_rows, purrr::map(files, function(f) {
  read_tsv(f,col_types=c(.default=col_double()))
})) %>% dplyr::rename('Sample' = 'X1')

```

## Looking at the distribution of values per sample

Per sample after tiling which merges the segments using a weighted mean.  Again, there's an upwards shift per-sample 
```{r, fig.width=10}

per.pt.tile<-function(df) {
  means = df [,-1] %>% t %>% as_tibble %>% summarise_all(list(~mean(.,na.rm=T))) %>% t %>% as_tibble %>% dplyr::rename('mean' = 'V1')
  sds = df [,-1] %>% t %>% as_tibble %>% summarise_all(list(~sd(.,na.rm=T))) %>% t %>% as_tibble %>% dplyr::rename('sd' = 'V1')
  vars = df [,-1] %>% t %>% as_tibble %>% summarise_all(list(~var(.,na.rm=T))) %>% t %>% as_tibble %>% dplyr::rename('var' = 'V1')
  iqrs = df [,-1] %>% t %>% as_tibble %>% summarise_all(list(~IQR(.,na.rm=T))) %>% t %>% as_tibble %>% dplyr::rename('IQR' = 'V1')
  
  return(add_column(bind_cols(means,sds,vars,iqrs), 'Sample'=df$Sample, .before=1))
}

t.pt = per.pt.tile(training.tiles) %>% mutate(cohort='train')
v.pt = per.pt.tile(val.tiles) %>% mutate(cohort='val')

gridExtra::grid.arrange(
  ggplot(bind_rows(t.pt, v.pt), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean per patient'),
  ggplot(bind_rows(t.pt, v.pt), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='StdDev per patient'),
  ncol=2, top='Per patient tiled values')
```

## Scale per sample

Scaling and centering per sample makes the samples directly comparable, but not sure what it will do to the bins.  
```{r}
t.per.pt.scale = as_tibble(t(apply(as.matrix(training.tiles[,-1]), 1, scale, center=T, scale=T))) %>% add_column('sample'=training.tiles$Sample, .before=1)
colnames(t.per.pt.scale) = colnames(training.tiles)
t.arm.pp.scale = as_tibble(t(apply(as.matrix(training.arms[,-1]), 1, scale, center=T, scale=T))) %>% add_column('sample'=training.arms$Sample, .before=1)
colnames(t.arm.pp.scale) = colnames(training.arms)

v.per.pt.scale = as_tibble(t(apply(as.matrix(val.tiles[,-1]), 1, scale, center=T, scale=T))) %>% add_column('sample'=val.tiles$Sample, .before=1)
colnames(v.per.pt.scale) = colnames(val.tiles)
v.arm.pp.scale = as_tibble(t(apply(as.matrix(val.arms[,-1]), 1, scale, center=T, scale=T))) %>% add_column('sample'=val.arms$Sample, .before=1)
colnames(v.arm.pp.scale) = colnames(val.arms)

t.pt = per.pt.tile(t.per.pt.scale) %>% mutate(cohort='train')
v.pt = per.pt.tile(v.per.pt.scale) %>% mutate(cohort='val')

gridExtra::grid.arrange(
  ggplot(bind_rows(t.pt, v.pt), aes(cohort, mean, group=cohort, fill=cohort)) + ylim(-.5,.5) +
    geom_jitter(width=0.1) + geom_boxplot(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean per patient'),
  ggplot(bind_rows(t.pt, v.pt), aes(x=cohort, y=sd, group=cohort, fill=cohort)) + 
    geom_point() + geom_boxplot(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='StdDev per patient'),
  ncol=2, top='Per patient tiled values, after scaling per patient')

```

### Per bin, after scaling per-sample

Now the variance of the cohorts appears to be nearly the same.

```{r}
t.dist = tile.dist.measure(t.per.pt.scale) %>% mutate(cohort='train')
v.dist = tile.dist.measure(v.per.pt.scale) %>% mutate(cohort='val')

gridExtra::grid.arrange(
  ggplot(bind_rows(t.dist, v.dist), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean'),
  ggplot(bind_rows(t.dist, v.dist), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='SD'), 
  ncol=2, top='Per bin tiled values on per-pt scaled')

grid.arrange(
  ggplot(full_join(t.dist, v.dist, by='bin'), aes(mean.x,mean.y)) + geom_point() + labs(x='train', y='val', title='mean'),
  ggplot(full_join(t.dist, v.dist, by='bin'), aes(sd.x,sd.y)) + geom_point() + labs(x='train', y='val', title='SD'),
  ggplot(full_join(t.dist, v.dist, by='bin'), aes(var.x,var.y)) + geom_point() + labs(x='train', y='val', title='var'),
  ggplot(full_join(t.dist, v.dist, by='bin'), aes(skew.x,skew.y)) + geom_point() + labs(x='train', y='val', title='Skew'),
ncol=2)

```


Just to check...after the arm segments have been subtracted as well.
```{r}

t.pp.scale = BarrettsProgressionRisk:::subtractArms(as.matrix(t.per.pt.scale[,-1]), as.matrix(t.arm.pp.scale[,-1])) %>% as_tibble %>% add_column('Sample' = 'x', .before=1)
v.pp.scale = BarrettsProgressionRisk:::subtractArms(as.matrix(v.per.pt.scale[,-1]), as.matrix(v.arm.pp.scale[,-1]))  %>% as_tibble %>% add_column('Sample' = 'x', .before=1)

t.dist = tile.dist.measure(t.pp.scale) %>% mutate(cohort='train')
v.dist = tile.dist.measure(v.pp.scale) %>% mutate(cohort='val')

gridExtra::grid.arrange(
  ggplot(bind_rows(t.dist, v.dist), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean'),
  ggplot(bind_rows(t.dist, v.dist), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='SD'), 
  ncol=2, top='Per bin tiled values on per-sample scaled')



```

### Predictions

Using a model built on per-sample scaled data. Unfortunately that has not resolved the predictions.

```{r}
preds = do.call(bind_rows, purrr::map(
  list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/multipcf/predictions_per_pt_scale/0.9/', pattern='predictions.tsv', recursive = T, full.names = T), 
  function(f) read_tsv(f, col_types = 'cddcccccccc'))) %>% mutate(Endoscopy = as.Date(Endoscopy)) %>% dplyr::select(-`Path Notes`) %>% mutate(`Hospital Research ID` = str_replace_all( str_remove_all(`Hospital Research ID`, " "), '/', '_'))

preds = left_join(preds,all.val, by=c('Sample'='Samplename', 'Hospital Research ID', 'SLX-ID', 'Index Sequence', 'Block ID'))

# Non progressors
# FPR is ~40% still
fpr = round(nrow(preds %>% filter(Status == 'NP' & Risk == 'High'))/nrow(preds %>% filter(Status == 'NP')),2)

preds %>% dplyr::filter(Status == 'NP') %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('NP risk predictions, FPR=',fpr)) %>% kable_styling(full_width = F) 

# Progressors
# FNR is still over 50% in the old progressors as well
fnr = round(nrow(preds %>% filter(Status == 'P' & Risk == 'Low' & !grepl('OCCAMS',`Hospital Research ID`)))/nrow(preds %>% filter(Status == 'P' &  !grepl('OCCAMS',`Hospital Research ID`))),2)

preds %>% dplyr::filter(Status == 'P' & !grepl('OCCAMS',`Hospital Research ID`)) %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('P risk predictions, FNR=',fnr)) %>% kable_styling(full_width = F) 

```

## Per bin variance

No scaling at this point.

Per 5MB bin (across patients).  There's a lot more variation in the means per bin in the validation data. This is before bin scaling, which is based on the training data mean/sd, so where the variance causes issues is starting to become more clear.

```{r}
t.dist = tile.dist.measure(training.tiles) %>% mutate(cohort='train')
v.dist = tile.dist.measure(val.tiles) %>% mutate(cohort='val')

gridExtra::grid.arrange(
  ggplot(bind_rows(t.dist, v.dist), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean'),
  ggplot(bind_rows(t.dist, v.dist), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='SD'), 
  ncol=2, top='Per bin tiled values')


gridExtra::grid.arrange(
  ggplot(bind_rows(t.dist, v.dist), aes(cohort, skew, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Skew'),
  ggplot(bind_rows(t.dist, v.dist), aes(cohort, kurtosis, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Kurtosis'), 
  ncol=2, top='Per bin tiled values')


grid.arrange(
  ggplot(full_join(t.dist, v.dist, by='bin'), aes(mean.x,mean.y)) + geom_point() + labs(x='train', y='val', title='mean'),
  ggplot(full_join(t.dist, v.dist, by='bin'), aes(sd.x,sd.y)) + geom_point() + labs(x='train', y='val', title='SD'),
  ggplot(full_join(t.dist, v.dist, by='bin'), aes(var.x,var.y)) + geom_point() + labs(x='train', y='val', title='var'),
  ggplot(full_join(t.dist, v.dist, by='bin'), aes(skew.x,skew.y)) + geom_point() + labs(x='train', y='val', title='Skew'),
ncol=2)

```

After scaling and centering per-bin using the z scores from the training data. And sure enough, it spreads the values out considerably for the validation data.  So the question is if the validation data were adjusted would it do this?

```{r}
#load('~/Data/BarrettsProgressionRisk/Analysis/5e6_arms/model_data.Rdata')
#rm(dysplasia.df,labels,z.arms.sd,z.arms.mean,mn.cx,sd.cx)

means = apply(as.matrix(training.tiles[,-1]),2, mean, na.rm=T)
sd = apply(as.matrix(training.tiles[,-1]),2, sd, na.rm=T)

tt.norm = as.matrix(training.tiles[-1])
for (i in 1:ncol(tt.norm))
  tt.norm[,i] = BarrettsProgressionRisk:::unit.var(tt.norm[,i], means[i], sd[i])

val.norm = as.matrix(val.tiles[-1])
for (i in 1:ncol(val.norm))
  val.norm[,i] = BarrettsProgressionRisk:::unit.var(val.norm[,i], means[i], sd[i])

tt.norm = as_tibble(tt.norm) %>% add_column('Sample'=training.tiles$Sample,.before=T)
val.norm = as_tibble(val.norm) %>% add_column('Sample'=val.tiles$Sample,.before=T)

tnorm.dist = tile.dist.measure(tt.norm) %>% mutate(cohort='train')
vnorm.dist = tile.dist.measure(val.norm) %>% mutate(cohort='val')

gridExtra::grid.arrange(
  ggplot(bind_rows(tnorm.dist, vnorm.dist), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean'),
  ggplot(bind_rows(tnorm.dist, vnorm.dist), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='SD'),
  ncol=2, top='Per bin normalized tile values')
```

This strongly suggests that I need to try re-evaluate how to normalize bins.


## Scale validation cohort within itself

Instead scaling using z-values from the training data, I can do it from within the validation cohort. This looks better here, but when I use this for predictions it doesn't significantly change the results.

```{r}
v.means = apply(as.matrix(val.tiles[,-1]),2, mean, na.rm=T)
v.sd = apply(as.matrix(val.tiles[,-1]),2, sd, na.rm=T)

val.int.norm = as.matrix(val.tiles[,-1])
for (i in 1:ncol(val.int.norm)) 
  val.int.norm[,i] = BarrettsProgressionRisk:::unit.var(val.int.norm[,i], v.means[i], v.sd[i])

val.int.norm = as_tibble(val.int.norm) %>% add_column('Sample'=val.tiles$Sample,.before=T)
vinorm.dist = tile.dist.measure(val.int.norm) %>% mutate(cohort='val int')

gridExtra::grid.arrange(
  ggplot(bind_rows(tnorm.dist, vinorm.dist), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean'),
  ggplot(bind_rows(tnorm.dist, vinorm.dist), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='SD'),
  ncol=2, top='Per bin normalized tile values', bottom='Val int = validation bins normalized using internal z-scores')
```

### Predictions

```{r}
preds = do.call(bind_rows, purrr::map(
  list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/multipcf/predictions_scale_within_cohort/0.9/', pattern='predictions.tsv', recursive = T, full.names = T), 
  function(f) read_tsv(f, col_types = 'cddcccccccc'))) %>% mutate(Endoscopy = as.Date(Endoscopy)) %>% dplyr::select(-`Path Notes`) %>% mutate(`Hospital Research ID` = str_replace_all( str_remove_all(`Hospital Research ID`, " "), '/', '_'))

preds = left_join(preds,all.val, by=c('Sample'='Samplename', 'Hospital Research ID', 'SLX-ID', 'Index Sequence', 'Block ID'))

# Non progressors
# FPR is ~40% still
fpr = round(nrow(preds %>% filter(Status == 'NP' & Risk == 'High'))/nrow(preds %>% filter(Status == 'NP')),2)

preds %>% dplyr::filter(Status == 'NP') %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('NP risk predictions, FPR=',fpr)) %>% kable_styling(full_width = F) 

# Progressors
# FNR is still over 50% in the old progressors as well
fnr = round(nrow(preds %>% filter(Status == 'P' & Risk == 'Low' & !grepl('OCCAMS',`Hospital Research ID`)))/nrow(preds %>% filter(Status == 'P' &  !grepl('OCCAMS',`Hospital Research ID`))),2)

preds %>% dplyr::filter(Status == 'P' & !grepl('OCCAMS',`Hospital Research ID`)) %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('P risk predictions, FNR=',fnr)) %>% kable_styling(full_width = F) 
```

## Scale validation using global z scores

In this case the z-scores are drawn from the entire cohort similarly to what I would do before training a model with all samples.

```{r}
v.means = apply(as.matrix(bind_rows(training.tiles, val.tiles)[,-1]),2, mean, na.rm=T)
v.sd = apply(as.matrix(bind_rows(training.tiles, val.tiles)[,-1]),2, sd, na.rm=T)

tr.int.norm = as.matrix(training.tiles[,-1])
for (i in 1:ncol(tr.int.norm)) 
  tr.int.norm[,i] = BarrettsProgressionRisk:::unit.var(tr.int.norm[,i], v.means[i], v.sd[i])

tr.int.norm = as_tibble(tr.int.norm) %>% add_column('Sample'=training.tiles$Sample,.before=T)
trinorm.dist = tile.dist.measure(tr.int.norm) %>% mutate(cohort='training glob')


val.int.norm = as.matrix(val.tiles[,-1])
for (i in 1:ncol(val.int.norm)) 
  val.int.norm[,i] = BarrettsProgressionRisk:::unit.var(val.int.norm[,i], v.means[i], v.sd[i])

val.int.norm = as_tibble(val.int.norm) %>% add_column('Sample'=val.tiles$Sample,.before=T)
vinorm.dist = tile.dist.measure(val.int.norm) %>% mutate(cohort='val glob')


gridExtra::grid.arrange(
  ggplot(bind_rows(trinorm.dist, vinorm.dist), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean'),
  ggplot(bind_rows(trinorm.dist, vinorm.dist), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='SD'),
  ncol=2, top='Per bin scaled tile values', bottom='Val glob = validation bins normalized using global z-scores')
```

### Predictions

Starts looking a little better in non-progressors.  

```{r}
preds = do.call(bind_rows, purrr::map(
  list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/multipcf/predictions_5e6_all_var/0.9/', pattern='predictions.tsv', recursive = T, full.names = T), 
  function(f) read_tsv(f, col_types = 'cddcccccccc'))) %>% mutate(Endoscopy = as.Date(Endoscopy)) %>% dplyr::select(-`Path Notes`) %>% mutate(`Hospital Research ID` = str_replace_all( str_remove_all(`Hospital Research ID`, " "), '/', '_'))

preds = left_join(preds,all.val, by=c('Sample'='Samplename', 'Hospital Research ID', 'SLX-ID', 'Index Sequence', 'Block ID'))

# Non progressors
# FPR is ~40% still
fpr = round(nrow(preds %>% filter(Status == 'NP' & Risk == 'High'))/nrow(preds %>% filter(Status == 'NP')),2)

preds %>% dplyr::filter(Status == 'NP') %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('NP risk predictions, FPR=',fpr)) %>% kable_styling(full_width = F) 

# Progressors
# FNR is still over 50% in the old progressors as well
fnr = round(nrow(preds %>% filter(Status == 'P' & Risk == 'Low' & !grepl('OCCAMS',`Hospital Research ID`)))/nrow(preds %>% filter(Status == 'P' &  !grepl('OCCAMS',`Hospital Research ID`))),2)

preds %>% dplyr::filter(Status == 'P' & !grepl('OCCAMS',`Hospital Research ID`)) %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('P risk predictions, FNR=',fnr)) %>% kable_styling(full_width = F) 
```


## Rank adjust validation values then scale?

This moved the non-progressors the right direction with the predictions, but there was still a 30% FPR for non-progressors.  The age-bias reappeared for the progressors though.

```{r}

find.rank<-function(r,x) {
  if ( nrow(x %>% filter(rank == r)) > 0 ) return(x %>% filter(rank == r))
  find.rank(r-1,x)
}

adj.v = val.tiles
for (n in 2:ncol(training.tiles)) {
  col = colnames(training.tiles)[n]
  
  rk = sapply(seq(1, length(training.tiles[[col]]), 4), function(s) mean(training.tiles[[col]][s:(s+4)], na.rm=T))

  tt.rank = tibble(val =rk, rank=rank(rk)) %>%  arrange(rank)
  vt.rank = floor(rank(val.tiles[[col]]))

  for (i in 1:length(vt.rank)) {
    r = vt.rank[i]
    if (is.na(mean(find.rank(r,tt.rank)$val,na.rm=T))) stop(paste(col, r))
    adj.v[i, col] = mean(find.rank(r,tt.rank)$val,na.rm=T)
  }
}
  

plot( training.tiles %>% dplyr::select(-Sample) %>% dplyr::summarise_each(funs(sd)) %>% unlist,
      val.tiles %>% dplyr::select(-Sample) %>% dplyr::summarise_each(funs(sd)) %>% unlist, main='SD', xlab='train', ylab='val tiles')

plot( training.tiles %>% dplyr::select(-Sample) %>% dplyr::summarise_each(funs(sd)) %>% unlist,
      adj.v %>% dplyr::select(-Sample) %>% dplyr::summarise_each(funs(sd)) %>% unlist, main='SD', xlab='train', ylab='rank adj val')

plot( training.tiles %>% dplyr::select(-Sample) %>% dplyr::summarise_each(funs(mean)) %>% unlist,
      adj.v %>% dplyr::select(-Sample) %>% dplyr::summarise_each(funs(mean)) %>% unlist, main='mean', xlab='train', ylab='rank adj val')

# After normalization
adj.v.norm = as.matrix(adj.v[-1])
for (i in 1:ncol(adj.v.norm))
  adj.v.norm[,i] = BarrettsProgressionRisk:::unit.var(adj.v.norm[,i], means[i], sd[i])
adj.v.norm = as_tibble(adj.v.norm) %>% add_column('Sample'=val.tiles$Sample,.before=T)
adj.vnorm.dist = tile.dist.measure(adj.v.norm) %>% mutate(cohort='adj val')

gridExtra::grid.arrange(
  ggplot(bind_rows(tnorm.dist, vnorm.dist, adj.vnorm.dist), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean'),
  ggplot(bind_rows(tnorm.dist, vnorm.dist,adj.vnorm.dist), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='SD'),
  ncol=2, top='Per bin normalized tile values')

```


### Predictions

```{r}
preds = do.call(bind_rows, purrr::map(
  list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/multipcf/predictions-rankadj/0.9/', pattern='predictions.tsv', recursive = T, full.names = T), 
  function(f) read_tsv(f, col_types = 'cddcccccccc'))) %>% mutate(Endoscopy = as.Date(Endoscopy)) %>% dplyr::select(-`Path Notes`)

preds = left_join(preds,all.val, by=c('Sample'='Samplename', 'Hospital Research ID', 'SLX-ID', 'Index Sequence', 'Block ID'))

# Non progressors
# FPR is ~40% still
fpr = round(nrow(preds %>% filter(Status == 'NP' & Risk == 'High'))/nrow(preds %>% filter(Status == 'NP')),2)

preds %>% dplyr::filter(Status == 'NP') %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('NP risk predictions, FPR=',fpr)) %>% kable_styling(full_width = F) 

# Progressors
# FNR is still over 50% in the old progressors as well
fnr = round(nrow(preds %>% filter(Status == 'P' & Risk == 'Low' & !grepl('OCCAMS',`Hospital Research ID`)))/nrow(preds %>% filter(Status == 'P' &  !grepl('OCCAMS',`Hospital Research ID`))),2)

preds %>% dplyr::filter(Status == 'P' & !grepl('OCCAMS',`Hospital Research ID`)) %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('P risk predictions, FNR=',fnr)) %>% kable_styling(full_width = F) 
```


## Distributions
   
How does it change if I use a global variance learned from both cohorts, or a rolling-bin variance (to preserve some local info) to adjust?   
   
I think a weight-adjusted global~local variance isn't going to help.  The variance is consistently and stubbornly greater in the validation cohort no matter which direction I weight the scale factor.
   
```{r}
wt = c(0.5,0.5)

# global variance
all.tiles = bind_rows(training.tiles, val.tiles)
global.sd = sd(as.matrix(all.tiles[,-1]))

means = apply(training.tiles[,-1], 2, mean)
sds = apply(training.tiles[,-1], 2, sd)
wt.sds = sapply(sds, function(x) weighted.mean(c(x,global.sd),w=wt))

tt.norm.gv = as.matrix(training.tiles[-1])
for (i in 1:ncol(tt.norm.gv))
  tt.norm.gv[,i] = BarrettsProgressionRisk:::unit.var(tt.norm.gv[,i], means[i], wt.sds[i])


means = apply(val.tiles[,-1], 2, mean)
sds = apply(val.tiles[,-1], 2, sd)
wt.sds = sapply(sds, function(x) weighted.mean(c(x,global.sd),w=wt))

val.norm.gv = as.matrix(val.tiles[-1])
for (i in 1:ncol(tt.norm.gv))
  val.norm.gv[,i] = BarrettsProgressionRisk:::unit.var(val.norm.gv[,i], means[i], wt.sds[i])

means = apply(all.tiles[,-1], 2, mean)
sds = apply(all.tiles[,-1], 2, sd)
wt.sds = sapply(sds, function(x) weighted.mean(c(x,global.sd),w=wt))

all.norm.gv = as.matrix(all.tiles[-1])
for (i in 1:ncol(all.norm.gv))
  all.norm.gv[,i] = BarrettsProgressionRisk:::unit.var(all.norm.gv[,i], means[i], wt.sds[i])


tt.norm.gv = as_tibble(tt.norm.gv) %>% add_column('Sample'=training.tiles$Sample,.before=T)
val.norm.gv = as_tibble(val.norm.gv) %>% add_column('Sample'=val.tiles$Sample,.before=T)
all.norm.gv = as_tibble(all.norm.gv) %>% add_column('Sample'=all.tiles$Sample,.before=T)

tt.norm.dist = tile.dist.measure(tt.norm.gv) %>% mutate(cohort='train, wt var')
vp.norm.dist = tile.dist.measure(val.norm.gv) %>% mutate(cohort='val, wt var')
all.norm.dist = tile.dist.measure(val.norm.gv) %>% mutate(cohort='all, wt var')

gridExtra::grid.arrange(
  ggplot(bind_rows(tt.norm.dist, vp.norm.dist, all.norm.dist), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean'),
  ggplot(bind_rows(tt.norm.dist, vp.norm.dist, all.norm.dist), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='SD'),
  ncol=2, top='Per bin wt var scale values')





```
   
## Retrained model, no scaling

Shows nearly the same problem as before.  The variance is clearly at issue.

```{r}

preds = do.call(bind_rows, purrr::map(
  list.files('~/Data/BarrettsProgressionRisk/Analysis/validation/multipcf/predictions_no_scale/0.9/', pattern='predictions.tsv', recursive = T, full.names = T), 
  function(f) read_tsv(f, col_types = 'cddcccccccc'))) %>% mutate(Endoscopy = as.Date(Endoscopy)) %>% dplyr::select(-`Path Notes`)

preds = left_join(preds,all.val, by=c('Sample'='Samplename', 'Hospital Research ID', 'SLX-ID', 'Index Sequence', 'Block ID'))

# Non progressors
# FPR is ~40% still
fpr = round(nrow(preds %>% filter(Status == 'NP' & Risk == 'High'))/nrow(preds %>% filter(Status == 'NP')),2)

preds %>% dplyr::filter(Status == 'NP') %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('NP risk predictions, FPR=',fpr)) %>% kable_styling(full_width = F) 

# Progressors
# FNR is still over 50% in the old progressors as well
fnr = round(nrow(preds %>% filter(Status == 'P' & Risk == 'Low' & !grepl('OCCAMS',`Hospital Research ID`)))/nrow(preds %>% filter(Status == 'P' &  !grepl('OCCAMS',`Hospital Research ID`))),2)

preds %>% dplyr::filter(Status == 'P' & !grepl('OCCAMS',`Hospital Research ID`)) %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('P risk predictions, FNR=',fnr)) %>% kable_styling(full_width = F) 
```


# Model trained on all samples

In this case I'm looking at the leave-one-out predictions, so somewhat different but at least gives an idea. While the progressors don't change, the non-progressors see a signficant shift.

```{r}

load('~/Data/BarrettsProgressionRisk/Analysis/all-samples/loo.Rdata')

pg.samp = pg.samp %>% filter(Hospital.Research.ID %in% all.val$`Hospital Research ID` & !grepl('OCCAMS',Hospital.Research.ID)) %>%
  mutate(qt = cut(Prediction, breaks=seq(0,1,.1)))

preds = pg.samp %>% left_join(BarrettsProgressionRisk:::pred.confidence %>% dplyr::select(quant,Risk), by = c('qt'='quant'))

# Non progressors
# FPR is ~40% still
fpr = round(nrow(preds %>% filter(Status == 'NP' & Risk == 'High'))/nrow(preds %>% filter(Status == 'NP')),2)

preds %>% dplyr::filter(Status == 'NP') %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('NP risk predictions, FPR=',fpr)) %>% kable_styling(full_width = F) 

# Progressors
# FNR is still over 50% in the old progressors as well
fnr = round(nrow(preds %>% filter(Status == 'P' & Risk == 'Low' & !grepl('OCCAMS',`Hospital.Research.ID`)))/nrow(preds %>% filter(Status == 'P' &  !grepl('OCCAMS',`Hospital.Research.ID`))),2)

preds %>% dplyr::filter(Status == 'P' & !grepl('OCCAMS',`Hospital.Research.ID`)) %>% group_by(Risk) %>% tally() %>% spread(Risk, n) %>% 
  kable(caption=paste0('P risk predictions, FNR=',fnr)) %>% kable_styling(full_width = F) 
```

   
   
```{r eval=F}

tx = melt(training.tiles)
vx = melt(val.tiles)

grid.arrange(ggplot(tx, aes(value)) + xlim(0.4, 1.5) + geom_histogram(bins=50, color='lightblue') + labs(title='Training'),
             ggplot(vx, aes(value)) + xlim(0.4,1.5) + geom_histogram(bins=50, color='lightblue') + labs(title='Validation'))

# The extreme values in the validation set come from the OACs
all.val %>% filter(Samplename %in% unique(vx[which(vx$value > 1.5),'Sample']))

vx = vx %>% filter(Sample %in% (all.val %>% filter( !grepl('OCCAMS',`Hospital Research ID`)) %>% dplyr::select(Samplename) %>% pull) )

tx.normal = fitdistrplus::fitdist(tx$value, 'norm')
vx.normal = fitdistrplus::fitdist(vx$value, 'norm')

#From training data
plot(tx.normal)

#From validation data
plot(vx.normal)
```

```{r eval=F}

train.P = round(pnorm(tx$value,mean=tx.normal$estimate['mean'], sd=tx.normal$estimate['sd']),4)
val.P = round(pnorm(as.matrix(val.tiles[-1]),mean=tx.normal$estimate['mean'], sd=tx.normal$estimate['sd']),4)
#val.P = round(pnorm(as.matrix(val.tiles[-1]),mean=vx.normal$estimate['mean'], sd=vx.normal$estimate['sd']),4)


p.transform = as.matrix(val.tiles[,-1])
for (i in 1:nrow(p.transform)) {
  for (j in 1:ncol(p.transform)) {
    p.transform[i,j] = mean(tx[which(train.P == val.P[i,j]),'value'])
  }
}

n = 20
hist(as.matrix(training.tiles[,-1])[,n] ,breaks=20)
hist(as.matrix(val.tiles[,-1])[,n] ,breaks=20)
hist(p.transform[,n], breaks=20)


# so now if I scale...

vp.norm = p.transform
for (i in 1:ncol(vp.norm))
  vp.norm[,i] = BarrettsProgressionRisk:::unit.var(vp.norm[,i], means[i], sd[i])
vp.norm = as_tibble(vp.norm) %>% add_column('Sample'=val.tiles$Sample,.before=T)

vp.norm.dist = tile.dist.measure(vp.norm) %>% mutate(cohort='val p')

gridExtra::grid.arrange(
  ggplot(bind_rows(tnorm.dist, vnorm.dist, vp.norm.dist), aes(cohort, mean, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='Mean'),
  ggplot(bind_rows(tnorm.dist, vnorm.dist,vp.norm.dist), aes(cohort, sd, group=cohort, fill=cohort)) + 
    geom_jitter(width=0.1) + geom_violin(alpha=0.5) + theme(legend.position = 'none') +
    labs(title='SD'),
  ncol=2, top='Per bin normalized tile values')


```






