---
title: "RegressionPvNP"
author: "Sarah Killcoyne"
date: "23 May 2017"
output: 
  html_document: 
    toc: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
library(ggplot2)
library(ggfortify)
library(plyr)
library(pander)
library(reshape2)
library(gridExtra)
library(GenomicRanges)
library(glmnet)


load("Test_patients.Rdata", verbose=T)
validation.patient.data = patient.data

dataset = 'Training'
load(paste(dataset, '_patients.Rdata', sep=''), verbose=T)

source('lib/load_patient_metadata.R')

data = '~/Data/Ellie'

data.files = list.files(paste(data, 'QDNAseq',sep='/'), full.names=T)
analysis.files = list.files(paste(data, 'Analysis', sep='/'), full.names=T)


## Patient info file
patient.file = grep('All_patient_info.txt', data.files, value=T)
if (length(patient.file) != 1)
  stop(paste("Missing/too many patient info file(s) in", data))

patient.info = read.patient.info(patient.file, set=dataset)
patient.info$Patient = gsub("/", "_", patient.info$Patient)
head(patient.info)

validation.patient.info = read.patient.info(patient.file, set='Test')
sum.validation.info = summarise.patient.info(validation.patient.info)

patient.info = arrange(patient.info, Status, Patient, Endoscopy.Year, Pathology)

sum.patient.data = summarise.patient.info(patient.info)


if (length(which(sapply(patient.data, function(df) nrow(df$seg.vals)) == 0)) > 0)
  warning(paste("Patients missing data:",
                paste(names(which(sapply(patient.data, function(df) nrow(df$seg.vals)) == 0)), collapse=', ')))

patient.data[which(sapply(patient.data, function(df) nrow(df$seg.vals)) == 0)] = NULL
sum.patient.data = as.data.frame(subset(sum.patient.data, Patient %in% names(patient.data))) ## For now

# Missing some of the samples as they aren't all sequenced yet
for (pt in names(patient.data)) {
  #print(pt)
  #print(patient.data[[pt]]$info$Samplename %in% colnames(patient.data[[pt]]$seg.vals)[-(1:5)])
  patient.data[[pt]]$info = patient.data[[pt]]$info[patient.data[[pt]]$info$Samplename %in% colnames(patient.data[[pt]]$seg.vals)[-(1:5)],]
}

pander(sum.patient.data[,c('Patient', 'Status', 'start.year', 'end.year','total.samples','total.endos','highest.path')], justify='left')


```


The previous analysis employed AP clustering on a per-patient basis to look for evidence that it was possible to see differences between the progressors and non-progressors, and to quantify that in some sort of complexity measure.  It was able to separate them, and continued to do so after removing HGD samples, but quickly failed to model any difference as I removed timepoints from the patient.  

As suggested by Moritz it is possible to merge all of the patient samples and use GLMs to model the differences, and to select regions that may be most reflective of the differences. In this report that's what I've done, and it worked very well.  However, I'm a bit suspicious that it's worked too well.  I'm looking for biases in the data, or mistakes I might have made in setting up the model.

# Encode the large data matrix

To use GLMs across the patients I merge all samples from all patients and overlap the copy number segments. Where segments from a sample get split, the value of the original sample is retained for each of the split samples.


```{r tile, echo=F, message=F, warning=F, include=F}
tile.w=1e07
#tile.w=5e06
tile.files = grep('tile_.*1e\\+07', analysis.files, value=T)
#tile.files = grep('tile_.*5e\\+06', analysis.files, value=T)
load(tile.files[1], verbose=T)
dim(mergedDf)

#cache.dir = paste(data, 'Analysis/5e06', sep='/')
cache.dir = paste(data, 'Analysis/1e07', sep='/')
if (!dir.exists(cache.dir)) dir.create(cache.dir)

#validatioMergedDf = tile.sample.segments(paste(as.character(tile.w),'validation_tiledpts.Rdata', sep=''), validation.patient.data)

```

Using a very large segment size for tiling across all patients (`r tile.w`) I get the following binomial models.  None of them fit well, and decreasing the size of the segments results in poor (or no) fits with mostly NA coefficients, or a lot of P(0 or 1).

# y = Progressors (1) vs Non (0)

The labels are split such that all samples from progressor patients are (1) and all samples from non-progressors are (0).

```{r labelsPNP, echo=F, message=F, include=F}
## binomial: dysplasia 1, BE 0
labels = unlist(sapply(patient.data, function(df) {
  df$info = arrange(df$info, Endoscopy.Year, Pathology)
  label = as.integer(df$info$Status == 'P') #as.integer(df$info$Pathology %in% c('HGD', 'IMC'))
  names(label) = df$info$Samplename
  return(label)
}))
names(labels) = sub('.*\\.', '',  names(labels))

pts = do.call(rbind, lapply(patient.data, function(df) {
  df$info = arrange(df$info, Endoscopy.Year, Pathology)
  cbind(df$info[,c('Patient','Samplename')])
}))
rownames(pts) = 1:nrow(pts)

# sort in label order
if (length(setdiff(colnames(mergedDf), names(labels))) > 3)
  warning("Labels vector is missing samples")

dysplasia.df = t(mergedDf[,names(labels)])
dim(dysplasia.df)
```

We have `r table(labels)[1]` samples from non-progressors and `r table(labels)[2]` samples from progressors.

Example subset of the data matrix:
`r pander(dysplasia.df[1:5, 1:5], caption=paste("dimensions:", paste(dim(dysplasia.df), collapse=', ')), justify='left')`

```{r cvglmnetPNP, echo=F, warning=F, message=F, eval=F}
## cv.glmnet Ridge vs Lasso
#With all `r ncol(dysplasia.df)` regions of the genome, first check if/which regression may be appropriate. It appears that pure lasso may provide the best balance for number of features with non-zero coefficients.

plot.multi.cv.glmnet<-function(x,y,alpha=c(0,1), family="binomial", title='cv.glmnet') {
  plots <- list(  )
  cv.models = data.frame(matrix(ncol=3,nrow=0,dimnames=list(c(), c('class','lambda','cvm'))))
  for (a in alpha) {
    cv = cv.glmnet(x, y, alpha=a, family=family)
    cv.models = rbind(cv.models, cbind('class'=paste('cv',a,sep=''), 'lambda'=log(cv$lambda), 'cvm'=cv$cvm))
    plots[[as.character(a)]] = autoplot(cv, main=paste('alpha',a), ylab=cv$name)
  }
  cv.models[c('lambda','cvm')] = lapply(cv.models[c('lambda','cvm')], function(x) as.numeric(as.character(x)))
  
  gg = ggplot(cv.models, aes(x=lambda, y=cvm, color=class)) + geom_point() + geom_line() + 
    labs(x='log(Lambda)', y='Binomial Deviance', title=title)
  
  plots[['all']] = gg
  
  return(plots)
}

gp = plot.multi.cv.glmnet(x=dysplasia.df, y=labels, alpha=c(0,0.5,0.8,1), title='cv.glmnet P vs NP')

cv1 = cv.glmnet(dysplasia.df, labels, alpha=1, family='binomial')
gg = autoplot(cv1$glmnet.fit, xvar='lambda', main='alpha=1 glmnet.fit from cv.glmnet') + theme(legend.position='none') 

gp[['coef.1']] = gg

do.call(grid.arrange, c(gp, ncol=2, nrow=3))

```

## Build/Cross-validate by patient

Keeping in mind that the matrix is built on a sample, not patient, basis while the labels (Progressor/Non) are on a per-patient basis I generated the cross validated models for various values of alpha by running 10 or more iterations of 5-fold cross validation that pulled out all the samples for 5 patients at each fold.  1000 lambda values.

```{r cv.funcs, message=F, warning=F, echo=T, fig.height=6, fig.width=6}
pi.hat<-function(x) exp(x)/(1+exp(x))

non.zero.coef<-function(fit, s) {
  cf =  as.matrix(coef(fit, s))
  cf[which(cf != 0),][-1]
}

coef.stability<-function(opt, nz.list) {
  for (i in 1:length(nz.list)) {
    df = as.data.frame(nz.list[[i]])
    colnames(df) = i
    opt = merge(opt, df, by='row.names', all.x=T)
    rownames(opt) = opt$Row.names
    opt$Row.names = NULL
    opt[,(i+1)] = as.integer(!is.na(opt[,(i+1)]))
  }
  opt = opt[order(sapply(rownames(opt), function(x) as.numeric(unlist(strsplit(x, ':'))[1])  )),]
  return(opt)
}

create.patient.sets<-function(pts, n, splits, minR=0.2) {
  # This function just makes sure the sets don't become too unbalanced with regards to the labels.
  check.sets<-function(df, grpCol, min) {
    sets = table(cbind.data.frame('set'=df[[grpCol]], 'labels'=labels[df$Samplename]))
    while ( (length(which(sets/rowSums(sets) < minR) ) >= 2 | length(which(sets/rowSums(sets) == 0)) > 0) ) {
      #print(sets/rowSums(sets))
      s = sample(rep(seq(5), length = length(unique(df$Patient))))
      df2 = merge(df, cbind('Patient'=unique(df$Patient), 'tmpgrp'=s), by="Patient")
      df[[grpCol]] = df2$tmpgrp
      sets = table(cbind.data.frame('set'=df[[grpCol]], 'labels'=labels[df$Samplename]))
    }
    return(df[[grpCol]])
  }
  
  s = sample(rep(seq(splits), length = length(unique(pts$Patient))))
  patients = merge(pts, cbind('Patient'=unique(pts$Patient), 'group'=s), by="Patient")  
  colnames(patients)[3] = c('fold.1')
  patients$fold.1 = check.sets(patients, 'fold.1', minR)
  
  for (i in 2:n) {
    s = sample(rep(seq(5), length = length(unique(pts$Patient))))
    patients = merge(patients, cbind('Patient'=unique(patients$Patient), 'group'=s), by="Patient")  
    foldcol = grep('group',colnames(patients))
    colnames(patients)[foldcol] = paste('fold',i,sep='.')
    patients[[paste('fold',i,sep='.')]] = check.sets(patients, paste('fold',i,sep='.'))
  }
  return(patients)
}

binomial.deviance<-function(pmat, y) {
  # Binomial deviance, lifted from cv.lognet
  prob_min = 1e-05; prob_max = 1 - prob_min
  pmat = pmin(pmax(pmat, prob_min), prob_max)
  dev = apply(pmat, 2, function(p)  -2*((y==1)*log(p)+(y==0)*log(1-p)) )
  return(dev)
}

crossvalidate.by.patient<-function(x,y,lambda,pts,a=1,nfolds=10, splits=5, fit=NULL, minR=0.2, select='classification') {
  if (nfolds > 5) minR = 0.1
  message(paste("Running", splits, "splits",nfolds,"times on", paste(dim(x), collapse=':'), 'alpha=',a ))
  fit.e = list()
  tpts = create.patient.sets(pts, nfolds, splits, minR)
  cv.pred = (matrix(nrow=0, ncol=length(lambda)))
  cv.binomial.deviance = (matrix(nrow=0, ncol=length(lambda)))
  for (n in 1:nfolds) {  
    message(paste(n, "fold"))
    setCol = grep(paste('^fold.',n,'$',sep=''), colnames(tpts))
    
    cv.class = matrix(nrow=splits, ncol=length(lambda))
    deviance = matrix(nrow=splits, ncol=length(lambda))
    for (i in 1:splits) { 
      message(paste(i, "split"))
      test.rows = which(rownames(x) %in% tpts[which(tpts[,setCol] == i), 'Samplename'])
      test = x[test.rows,]
      training = x[-test.rows,]
      # pre-spec lambda seq
      fitCV <- glmnet(training, y[-test.rows], lambda=lambda, family='binomial', alpha=a) 
      # autoplot(fit) + theme(legend.position="none")
      
      # Confusion matrix: quantitiative, pos results + neg results / number of test rows
      pred <- pi.hat(predict(fitCV, test, type='link')) 
      cv.class[i,] = apply(pred, 2, function(p) {
        (p%*%y[test.rows] + (1-p) %*% (1-y[test.rows]))/length(test.rows)
      })
      
      # Binomial deviance, lifted from cv.lognet
      dev = binomial.deviance(predict(fitCV, test, type='response'), as.factor(y[test.rows]))
      deviance[i,] = apply(dev, 2, weighted.mean, w=rep(1, nrow(dev)), na.rm=T)
      
      fit.e[[length(fit.e)+1]] = fitCV
    }
    cv.pred = rbind(cv.pred, cv.class)    
    cv.binomial.deviance = rbind(cv.binomial.deviance, deviance)
  }
  
  # Not really sure what to do with the binomial deviance now...
  df = cbind.data.frame('lambda.at'=1:ncol(cv.pred),
                        'mean'= colMeans(cv.pred), 
                        'sme'= apply(cv.pred, 2, sd)/sqrt(nrow(cv.pred)), 
                        'sd'= apply(cv.pred, 2, sd), 
                        'lambda'= lambda,
                        'log.lambda' = log(lambda),
                        'mean.b.dev' = colMeans(cv.binomial.deviance),
                        'sd.b.dev' = apply(cv.binomial.deviance, 2,sd),
                        'sme.b.dev' = apply(cv.binomial.deviance, 2, sd)/sqrt(nrow(cv.binomial.deviance)),
                        'lambda.min' = F, 'lambda.1se' = F)
  
  if (select == 'classification') {
    df$lambda.min = df$mean == max(df$mean)
    se1 = df[df$lambda.min == T, 'mean'] - df[df$lambda.min == T, 'sme']
    
    arrange(subset(df, mean < se1 & !lambda.min & log.lambda > subset(df, lambda.min)$log.lambda ), -mean)[1:10,]
    df[arrange(subset(df, mean < se1 & !lambda.min & log.lambda > subset(df, lambda.min)$log.lambda ), -mean)[1, 'lambda.at'], 'lambda.1se'] = T
    #df[df$log.lambda < subset(df, mean == max(mean))$log.lambda-sd(df$log.lambda),][1, 'lambda-1se'] = TRUE
  } else if (select == 'deviance') {
    df$lambda.min = df$mean.b.dev == min(df$mean.b.dev)
    se1 = df[df$lambda.min == T, 'mean.b.dev'] - df[df$lambda.min == T, 'sme.b.dev']
    df[arrange(subset(df, mean.b.dev <= se1 & !lambda.min), -mean.b.dev)[1, 'lambda.at'], 'lambda.1se'] = T
    #df[df$mean.b.dev >= df[df$lambda.min == T, 'mean.b.dev'] + sd(df$mean.b.dev), ][1, 'lambda.1se'] = T
  }
  
  lambda.min = lambda[subset(df, lambda.min == T)$lambda.at]
  lambda.1se = lambda[subset(df, lambda.1se == T)$lambda.at]
  
  nzcf = lapply(fit.e, non.zero.coef, s=lambda.1se)
  
  plots = plot.patient.cv(df, fit)
  plots$performance = plots$performance + theme(legend.position='bottom') + scale_colour_discrete(name = "") +
    labs(title=paste(splits,' splits, ',nfolds,' folds, alpha=',a, sep=''), y='mean pred.', x='log(lambda)') 
  plots$deviance = plots$deviance + theme(legend.position='bottom') + scale_colour_discrete(name="") + 
    labs(title=paste(splits,' splits, ',nfolds,' folds, alpha=',a, sep=''), y='mean Binomial Deviance', x='log(lambda)') 
  
  return(list('max.cm'=df[df$lambda.1se ==T,'mean'], 
              'lambda.min'=lambda.min, 'lambda.1se'=lambda.1se, 'lambdas'=df, 'plot'=plots$performance, 'deviance.plot'=plots$deviance, 'non.zero.cf'=nzcf))
}

plot.patient.cv<-function(df, fit=NULL) {
  gp = ggplot(df, aes(y=mean,x=log.lambda)) + geom_point() + geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + 
    geom_point(data=subset(df, lambda.min == T), aes(y=mean, x=log.lambda, colour="lambda.min"), size=2 ) +  
    geom_vline(xintercept = subset(df, lambda.min == T)$log.lambda, colour='grey') +
    annotate("text", x=subset(df, lambda.min == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
             label=round(df[subset(df, lambda.min == T)$lambda.at,'mean'],3)) 
  
  gpD = ggplot(df, aes(y=mean.b.dev,x=log.lambda)) + geom_point() +
    geom_errorbar(aes(ymin=mean.b.dev-sme.b.dev, ymax=mean.b.dev+sme.b.dev)) + 
    geom_point(data=subset(df, lambda.min == T), aes(y=mean.b.dev, x=log.lambda, colour="lambda.min"), size=2 ) +          
    geom_vline(xintercept = subset(df, lambda.min == T)$log.lambda, colour='grey') +
    annotate("text", x=subset(df, lambda.min == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
             label=round(df[subset(df, lambda.min == T)$lambda.at,'mean'],3)) 
  
  if (nrow(subset(df, lambda.1se == T)) > 0) {
    gp = gp + geom_point(data=subset(df, lambda.1se == T), aes(y=mean, x=log.lambda, colour="lambda.1se"), size=2 ) +
      geom_vline(xintercept = subset(df, lambda.1se == T)$log.lambda, colour='grey') +
      annotate("text", x=subset(df, lambda.1se == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
               label=round(df[subset(df, lambda.1se == T)$lambda.at,'mean'],3)) 
    
    gpD = gpD + geom_point(data=subset(df, lambda.1se == T), aes(y=mean.b.dev, x=log.lambda, colour="lambda.1se"), size=2 ) +
      geom_vline(xintercept = subset(df, lambda.1se == T)$log.lambda, colour='grey') +
      annotate("text", x=subset(df, lambda.1se == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
               label=round(df[subset(df, lambda.1se == T)$lambda.at,'mean'],3)) 
  }
  
  
  gp = gp + theme(legend.position='bottom') + scale_colour_discrete(name = "") 
  gpD = gpD + theme(legend.position='bottom') + scale_colour_discrete(name="") 
  
  if (!is.null(fit)) {
    df$nzcoef = sapply(df$lambda, function(l) length(non.zero.coef(fit, l)))
    
    coef.min = subset(df, lambda.min == T)$nzcoef
    coef.1se = subset(df, lambda.1se == T)$nzcoef
    
    d = coef.1se-coef.min
    
    coef.text = arrange(subset(df,nzcoef %in% c(0,  coef.min, coef.1se, max(nzcoef) )), -lambda.min, -lambda.1se)
    coef.text = arrange(coef.text[!duplicated(coef.text$nzcoef),], -nzcoef)
    
    gp = gp + annotate("text", y=min(df$mean), x=coef.text$log.lambda, label=coef.text$nzcoef, color='darkblue')
    gpD = gpD + annotate("text", y=max(df$mean.b.dev), x=coef.text$log.lambda, label=coef.text$nzcoef, color='darkblue')
  }
  return(list('performance'=gp, 'deviance'=gpD))
}

```


```{r xvalpt, message=F, warning=F, echo=T, fig.height=16, fig.width=16}
nl = 3000

coefs = list(); plots = list(); performance.at.1se = list()
folds = 10; splits = 5
alpha.values = c(0,0.5,0.7,0.8,0.9,1)

file = paste(cache.dir, 'all.pt.alpha.Rdata', sep='/')
if (file.exists(file)) {
  load(file, verbose=T)
} else {
  for (a in alpha.values) {
    fit0 <- glmnet(dysplasia.df, labels, alpha=a, nlambda=nl, family='binomial') # all patients
    #autoplot(fit0, xvar='lambda', main=paste('fit0, all samples, alpha=',a,sep='')) + theme(legend.position='none') 
    
    cv.patient = crossvalidate.by.patient(x=dysplasia.df, y=labels, lambda=fit0$lambda, pts=pts, a=a, nfolds=folds, splits=splits, fit=fit0)
    
    lambda.opt = ifelse( length(cv.patient$lambda.1se) > 0, cv.patient$lambda.1se, cv.patient$lambda.min )
    
    coef.opt = as.data.frame(non.zero.coef(fit0, lambda.opt))
    coefs[[as.character(a)]] = coef.stability(coef.opt, cv.patient$non.zero.cf)
    
    plots[[as.character(a)]] = arrangeGrob(cv.patient$plot+ggtitle('Classification'), cv.patient$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
    
    performance.at.1se[[as.character(a)]] = subset(cv.patient$lambdas, lambda.1se == T)
  }
  save(plots, coefs, performance.at.1se, file=file)
}

do.call(grid.arrange, c(plots, top='All samples, 10fold, 5 splits'))
```


```{r xvalpt2, message=F, warning=F, echo=T}
performance.at.1se = do.call(rbind, performance.at.1se)
performance.at.1se$alpha = rownames(performance.at.1se)

performance.at.1se = cbind(performance.at.1se, do.call(rbind, lapply(coefs, function(x) cbind('n.Coef'=nrow(x), '75%'=length(which(rowSums(x)/50 >= 0.75))))))

ggplot(performance.at.1se, aes(alpha, mean)) + geom_text( aes(label=round(mean, 3)), hjust=-0.5) +
  geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme), color='grey') + geom_point() + ylim(0.5,0.8) +
  geom_text(aes(label=n.Coef), nudge_y=0.03) +
  geom_text(aes(label=paste('(',`75%`,')',sep='')), nudge_y=0.02) +
  labs(x='Elasticnet penalty value', y='Model performance at lambda.1se', 
       title=paste('All samples,', folds, 'folds,', splits, 'patient splits'))

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=length(names(coef.stable)), 
                           dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) 
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1

table(rowSums(cfs))

coef.stable = lapply(coef.stable, function(cf) cf/(folds*splits))

stability = do.call(rbind, lapply(coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(stability) = names(coef.stable)

pander(as.data.frame(sort(names(which((coef.stable[['1']]) >= 0.75)))), justify='left')
```

Values that are closer to full lasso (alpha=1) are pretty similar, however at full lasso we see fewer features than if we regularize it at 0.8. They share all non-zero features as well.
`r pander(sapply(coef.stable, length), caption='Total features at each value of alpha')`

### Feature stability

`r pander(table(rowSums(cfs)), caption="Number of features found in 1, 2, or all 3 models by alpha 0.8, 0.9 and 1")`


`r length(unique(unlist(lapply(coef.stable, function(cf)  names(which(cf > 0.8)) ))))` features are shared at all values of alpha and were non-zero in at least 80% of the folds.

### Features in samples

```{r echo=F, warning=F, message=F}

length(which( coef.stable[['1']] >= 0.5) )

regions = rownames(coefs[['1']])

dysplasia.df[ labels == 1 ,regions]

dysplasia.df[ labels == 0 ,regions]

```


### Feature Information 

```{r, warning=F, message=F, eval=F}
#topF = do.call(rbind, strsplit( names(which((coef.stable[['1']]) >= 0.75)), ':|-'))
#colnames(topF) = c('chr','start','end')
#topF = apply(topF, 2, as.numeric)

topF = names(which((coef.stable[['1']]) >= 0.75))

library(biomaRt)
mart = biomaRt::useMart("ENSEMBL_MART_ENSEMBL", host="grch37.ensembl.org")
biomart = useDataset("hsapiens_gene_ensembl", mart)

attr = c('ensembl_gene_id','hgnc_symbol','start_position','end_position', 'chromosome_name', 'go_id', 'entrezgene')
bm = getBM(mart=biomart, attributes=attr, filters=list('chromosomal_region' = topF))
bm = bm[-which(bm$hgnc_symbol == ""),] 
#head(bm)

bm2 = ddply(bm, .(entrezgene, ensembl_gene_id, hgnc_symbol), summarise, 'chrom'=unique(chromosome_name), 'start'=min(start_position), 'end'=max(end_position), 'GO.IDs'=paste(go_id, collapse=','))

ccgenes = read.table('~/Data/CosmicCensusGenes.tsv', sep='\t', header=T, stringsAsFactors=F)

bm2 = subset(bm2, hgnc_symbol %in% ccgenes$Gene.Symbol)

features.in.prog = dysplasia.df[names(which(labels == 1)),topF]

#features.in.prog <= 0.9
#features.in.prog >= 1.1

```



### Remove HGD/IMC Samples

Just as a point, the initial model is trained against `r nrow(dysplasia.df)` samples with `r ncol(dysplasia.df)` features.  What happens if we retrain it without HGD?

```{r noHGD, echo=T, message=F, warning=F, fig.height=16, fig.width=16}
`%nin%` <- Negate(`%in%`)

info = do.call(rbind, lapply(patient.data, function(df) df$info))

no.hgd.plots = list(); coefs = list(); performance.at.1se = list()

file = paste(cache.dir, 'nohgd.Rdata', sep='/')
if (file.exists(file)) {
  load(file, verbose=T)
} else {
  # No HGD/IMC
  samples = intersect(rownames(dysplasia.df), subset(info, Pathology %nin% c('HGD', 'IMC'))$Samplename)
  for (a in alpha.values) {
    # all patients
    fitNoHGD <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, family='binomial', nlambda=nl) 
    
    cv.nohgd = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitNoHGD$lambda, pts=subset(pts, Samplename %in% samples), a=a, nfolds=folds, splits=splits, fit=fitNoHGD)
    
    no.hgd.plots[[as.character(a)]] = arrangeGrob(cv.nohgd$plot+ggtitle('Classification'), cv.nohgd$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
    
    coef.1se = as.data.frame(non.zero.coef(fitNoHGD, cv.nohgd$lambda.1se))
    coefs[[as.character(a)]] = coef.stability(coef.1se, cv.nohgd$non.zero.cf)
    
    performance.at.1se[[as.character(a)]] = subset(cv.nohgd$lambdas, lambda.1se == T)
  }
  save(no.hgd.plots, coefs, performance.at.1se, file=file)
}

do.call(grid.arrange, c(no.hgd.plots, top="No HGD/IMC samples", ncol=2))
```

```{r noHGD2, echo=T, message=F, warning=F}
performance.at.1se = do.call(rbind, performance.at.1se)
performance.at.1se$alpha = rownames(performance.at.1se)

performance.at.1se = cbind(performance.at.1se, do.call(rbind, lapply(coefs, function(x) cbind('n.Coef'=nrow(x), '75%'=length(which(rowSums(x)/50 >= 0.75))))))

ggplot(performance.at.1se, aes(alpha, mean)) + geom_point() + geom_text( aes(label=round(mean, 2)), hjust=-0.5) +
  geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + ylim(0.5,0.8) + 
  geom_text(aes(label=n.Coef), nudge_y=0.03) +
  geom_text(aes(label=paste('(',`75%`,')',sep='')), nudge_y=0.02) +
  labs(x='Elasticnet penalty value', y='Model performance at lambda.1se', 
       title=paste('No HGD/IMC samples,', folds, 'folds,', splits, 'patient splits'))

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=3, 
                           dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) {
  if (is.null(names(coef.stable$`0.8`))) next
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1
}

coef.stable = lapply(coef.stable, function(cf) cf/(folds*splits))

stability = do.call(rbind, lapply(coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(stability) = names(coef.stable)
```

#### Feature Stability

`r pander(stability, caption='Total features found at each value of alpha in 50% or more of the folds run')`

They also share all of the non-zero coefficients (`r which.max(sapply(coef.stable, length))` identifies the most non-zero coefs `r max(sapply(coef.stable, length))`).

`r pander(table(rowSums(cfs)), justify='left')`


### Remove HGD/IMC & LGD Samples

Now remove all LGD samples from the progressor's and retrain.

```{r noLGD, echo=T, message=F, warning=F, fig.height=16, fig.width=16}
file = paste(cache.dir, 'nolgd.Rdata', sep='/')

nolgd.plots = list(); coefs = list(); performance.at.1se = list()
if (file.exists(file)) {
  load(file, verbose=T)
} else {
  # No LGD
  samples = intersect(rownames(dysplasia.df), c(subset(info, Status == 'NP')$Samplename, subset(info, Pathology %nin% c('HGD', 'IMC', 'LGD') & Status == 'P')$Samplename))
  
  # No HGD/IMC/LGD in all patients
  samples = intersect(rownames(dysplasia.df), subset(info, Pathology %nin% c('HGD', 'IMC', 'LGD'))$Samplename)
  
  for (a in alpha.values) {
    fitNoLGD <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, family='binomial', nlambda=nl) # all patients
    
    cv.nolgd = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitNoLGD$lambda, pts=subset(pts, Samplename %in% samples), a=a, nfolds=folds, splits=splits, fit=fitNoLGD)
    
    nolgd.plots[[as.character(a)]] = arrangeGrob(cv.nolgd$plot+ggtitle('Classification'), cv.nolgd$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
    
    coef.1se = as.data.frame(non.zero.coef(fitNoLGD, cv.nolgd$lambda.1se))
    coefs[[as.character(a)]] = coef.stability(coef.1se, cv.nolgd$non.zero.cf)
    
    performance.at.1se[[as.character(a)]] = subset(cv.nolgd$lambdas, lambda.1se == T)
  }
  save(nolgd.plots, coefs, performance.at.1se, file=file)
}

do.call(grid.arrange, c(nolgd.plots, top="No HGD/IMC/LGD samples", ncol=2))
```

```{r noLGD2, echo=T, message=F, warning=F}
if (class(performance.at.1se) == 'list')
  performance.at.1se = do.call(rbind, performance.at.1se)

performance.at.1se$alpha = rownames(performance.at.1se)

performance.at.1se = cbind(performance.at.1se, do.call(rbind, lapply(coefs, function(x) cbind('n.Coef'=nrow(x), '75%'=length(which(rowSums(x)/50 >= 0.75))))))

ggplot(performance.at.1se, aes(alpha, mean)) + geom_point() + geom_text( aes(label=round(mean, 2)), hjust=-0.5) +
  geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + ylim(0.5,0.8) +
  geom_text(aes(label=n.Coef), nudge_y=0.03) +
  geom_text(aes(label=paste('(',`75%`,')',sep='')), nudge_y=0.02) +
  labs(x='Elasticnet penalty value', y='Model performance at lambda.1se', 
       title=paste('No HGD/IMC/LGD samples,', folds, 'folds,', splits, 'patient splits'))

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=3, 
                           dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) 
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1

coef.stable = lapply(coef.stable, function(cf) cf/(folds*splits))

stability = do.call(rbind, lapply(coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(stability) = names(coef.stable)

```

#### Feature Stability

`r pander(stability, caption='Total features found at each value of alpha in 50% or more of the folds run')`

They also share all of the non-zero coefficients (`r which.max(sapply(coef.stable, length))` identifies the most non-zero coefs `r max(sapply(coef.stable, length))`).

`r pander(table(rowSums(cfs)), justify='left')`


## Compare progressors with many samples vs few

```{r, echo=T, message=F, warning=F, fig.height=16, fig.width=16}
m = median(subset(sum.patient.data, Status == 'P')$total.samples)
nonps = as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'NP')$Patient], function(df) df$info$Samplename)))

file = paste(cache.dir, 'few.Rdata', sep='/')

if (file.exists(file)) {
  load(file, verbose=T)
} else {
  # Few
  samples = c(nonps, as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'P' & total.samples < m)$Patient], function(pt) pt$info$Samplename))))
  
  few.plots = list()
  for (a in alpha.values) {
    fitF <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, nlambda=nl, family='binomial') # all patients
    cv.back.few = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitF$lambda, pts=subset(pts, Samplename %in% samples), splits=5, nfolds=20, a=a, fit=fitF, minR=0.1)
    #few.plots[[as.character(a)]] = cv.back.few$plot + ggtitle(paste('alpha=',a,sep=''))
    
    few.plots[[as.character(a)]] = arrangeGrob(cv.back.few$plot+ggtitle('Classification'), cv.back.few$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
  }
  save(few.plots, file=file)
} 
do.call(grid.arrange, c(few.plots, ncol=2, top=paste(nrow(subset(sum.patient.data, Status == 'P' & total.samples < m))," Progressors with < median # samples (",m,")")))

# Many
file = paste(cache.dir, 'many.Rdata', sep='/')

if (file.exists(file)) {
  load(file, verbose=T)
} else {
  samples = c(nonps, as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'P' & total.samples >= m)$Patient], function(pt) pt$info$Samplename))))
  
  many.plots = list()
  for (a in alpha.values) {
    fitM <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, nlambda=nl, family='binomial') # all patients
    
    cv.back.many = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitM$lambda, pts=subset(pts, Samplename %in% samples), splits=5, nfolds=20, a=a, fit=fitM)
    
    many.plots[[as.character(a)]] = arrangeGrob(cv.back.many$plot+ggtitle('Classification'), cv.back.many$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
  }
  save(many.plots, file=file)
}

do.call(grid.arrange, c(many.plots, ncol=2, top=paste(nrow(subset(sum.patient.data, Status == 'P' & total.samples >= m))," Progressors with >= median # samples (",m,")")))

```

# Leave one out 

Leave out each patient (both P and NP) and run a new CV fit each time then predict the samples for the patient that was left out.  Fitted model includes HGD samples.

```{r leaveoneout, echo=T, message=F, warning=F}

info = do.call(rbind, lapply(patient.data, function(df) df$info))
pg.samp = lapply(patient.data, function(pt) {
  info = pt$info
  info$SampleSD = NA
  info$SampleMEAN = NA
  if (length(info$Samplename) > 1) {
    info$SampleSD = apply(pt$seg.vals[,info$Samplename], 2, sd)
    info$SampleMEAN =  apply(pt$seg.vals[,info$Samplename], 2, mean)
  }
  info$Prediction = NA
  info$Prediction.Dev.Resid = NA
  info$PID = unlist(lapply(info$Path.ID, function(x) unlist(strsplit(x, 'B'))[1]))
  return(info)
})

file = paste(cache.dir, 'loo.Rdata', sep='/')

if (file.exists(file)) {
  load(file, verbose=T)
} else {
  performance.at.1se = c(); coefs = list(); plots = list(); fits = list()
  # Remove each patient (LOO)
  for (pt in names(pg.samp)) {
    print(pt)
    tmp.patient.data = patient.data[subset(sum.patient.data, Patient != pt)$Patient]
    samples = as.vector(unlist(sapply(tmp.patient.data, function(df) df$info$Samplename )))
    
    train.rows = which(rownames(dysplasia.df) %in% samples)
    training = dysplasia.df[train.rows,]
    test = as.matrix(dysplasia.df[-train.rows,])
    if ( nrow(test) == ncol(dysplasia.df) ) test = t(test)
    
    # Predict function giving me difficulty when I have only a single sample, this ensures the dimensions are the same
    sparsed_test_data <- Matrix(data=0, nrow=ifelse(length(pg.samp[[pt]]$Samplename) > 1, nrow(test), 1),  ncol=ncol(training),
                                dimnames=list(pg.samp[[pt]]$Samplename,colnames(training)), sparse=T)
    for(i in colnames(dysplasia.df)) sparsed_test_data[,i] = test[,i]
    
    # Fit generated on all samples, including HGD
    a = 1
    fitLOO <- glmnet(training, labels[train.rows], alpha=a, family='binomial', nlambda=nl) # all patients
    cv = crossvalidate.by.patient(x=training, y=labels[train.rows], lambda=fitLOO$lambda, 
                                  pts=subset(pts, Samplename %in% samples), a=a, nfolds=10, splits=5, fit=fitLOO)
    #plots[[pt]] = cv$plot + ggtitle(pt)
    
    plots[[pt]] = arrangeGrob(cv$plot+ggtitle('Classification'), cv$deviance.plot+ggtitle('Binomial Deviance'), top=pt, ncol=2)
    
    fits[[pt]] = cv  
    
    if ( length(cv$lambda.1se) > 0 ) {
      performance.at.1se = c(performance.at.1se, subset(cv$lambdas, lambda == cv$lambda.1se)$mean)
      
      coef.1se = as.data.frame(non.zero.coef(fitLOO, cv$lambda.1se))
      coefs[[pt]] = coef.stability(coef.1se, cv$non.zero.cf)
      
      logit <- function(p){log(p/(1-p))}
      inverse.logit <- function(or){1/(1 + exp(-or))}
      
      pm = predict(fitLOO, newx=sparsed_test_data, s=cv$lambda.1se, type='response')
      sy = as.matrix(sqrt(binomial.deviance(pm, labels[pg.samp[[pt]]$Samplename])))
      
      #df = cbind.data.frame(pm, sy, 1:length(sy))
      #colnames(df) = c('pred','dev','x')
      #ggplot(df, aes(x, pred)) + geom_point() + geom_errorbar(aes(ymin=pred-dev, ymax=pred+dev))
      
      pg.samp[[pt]]$Prediction = pm[,1]
      pg.samp[[pt]]$Prediction.Dev.Resid = sy[,1] 
      
    } else {
      warning(paste("Patient", pt, "did not have a 1se"))
    }
  }
  save(plots, performance.at.1se, coefs, fits, pg.samp, file=file)
}

ggplot(as.data.frame(performance.at.1se), aes(y=performance.at.1se, x='LOO')) + ylim(0.5,0.9) +
  geom_boxplot( fill='lightblue', color='darkgrey', outlier.fill=NA, outlier.color=NA) + geom_jitter() +
  labs(y='performance at lambda.1se', x='', title='Performance for Leave One (patient) Out, at alpha=1')

```


## How do the predictions look at each time point?

Providing a single sample at a time to predict is unnecessary as each sample is independently predicted from the test matrix. Predictions are exactly the same even when a single sample is provided (checked).  

```{r timepoints, echo=F, message=F, warning=F}
# For HGD/IMC samples, progressors only
hgd = sapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  hgd = subset(pt, Pathology %in% c('HGD', 'IMC'))
  sum(as.integer(hgd$Prediction > 0.5))/nrow(hgd)
})


hgd = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  hgd = subset(pt, Pathology %in% c('HGD', 'IMC'))
  cbind.data.frame('correctly.predicted'=sum(as.integer(hgd$Prediction > 0.5)), 'total.hgd.samples'=nrow(hgd))
}))
rownames(hgd) = subset(sum.patient.data, Status == 'P')$Patient
hgd = subset(hgd, total.hgd.samples > 0) # Some patients had no hgd samples

hgd$pred.ratio = hgd$correctly.predicted/hgd$total.hgd.samples

hgd$Patients = rownames(hgd)
hgd = transform(hgd, Patients=reorder(Patients, -pred.ratio) ) 

ggplot(hgd[,c('total.hgd.samples','pred.ratio', 'Patients')], aes(x=Patients, y=pred.ratio)) + 
  geom_bar(stat='identity', fill='darkblue', alpha=0.6) + geom_label( aes(label=total.hgd.samples) ) +
  coord_flip() + labs(title='Correctly predicted HGD samples', y='Correct prediction ratio')


lgd = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  e = subset(pt, Pathology  == 'LGD')
  cbind.data.frame('predicted'=sum(as.integer(e$Prediction > 0.5)), 'total.lgd'=nrow(e))
}))
lgd = subset(lgd, total.lgd > 0)

lgd$pred.ratio = round(lgd$predicted/lgd$total.lgd, 2)

lgd$Patients = rownames(lgd)
lgd = transform(lgd, Patients=reorder(Patients, -pred.ratio) ) 

ggplot(lgd[,c('total.lgd','pred.ratio', 'Patients')], aes(x=Patients, y=pred.ratio)) + 
  geom_bar(stat='identity', fill='darkgreen', alpha=0.6) + geom_label( aes(label=total.lgd) ) +
  coord_flip() + labs(title='Correctly predicted LGD samples', y='Correct prediction ratio')


# All samples
all.predictions = sapply(pg.samp, function(pt) {
  if (unique(pt$Status) == 'NP') {
    as.integer(pt$Prediction < 0.5)
  } else {
    as.integer(pt$Prediction > 0.5)
  }
})


all.predictions = do.call(rbind, lapply(all.predictions, function(x) {
  cbind.data.frame('correctly.predicted'=sum(x), 'total.samples'=length(x))
}))

all.predictions$Status = ifelse (rownames(all.predictions) %in% subset(sum.patient.data, Status == 'P')$Patient, 'P', 'NP')

#ggplot(melt(all.predictions, id.vars='Status'), aes(y=value, x=Status, fill=variable)) +
#  geom_bar(stat='identity', position='dodge') + labs(x='', y='Sample Counts', main='Total Sample Predictions')

all.predictions$pred.ratio = with(all.predictions, correctly.predicted/total.samples)
all.predictions$Patient = rownames(all.predictions)
all.predictions = transform(all.predictions, Patient=reorder(Patient, pred.ratio) ) 

m = melt(all.predictions, id.vars=c('Patient','Status'), measure.vars=c('pred.ratio'))


grid.arrange(
  ggplot() + geom_bar(data=subset(m, Status == 'P'), aes(Patient, value), fill='darkblue', color='grey', stat='identity') +  
    theme(legend.position='none', axis.text.x = element_text(angle = 45, hjust = 1)) + coord_flip() +
    labs(title='Progressors', x='', y='Ratio predicted samples'),
  ggplot() + geom_bar(data=subset(m, Status == 'NP'), aes(Patient, value), fill='darkgreen', color='grey', stat='identity') + 
    theme(legend.position='none', axis.text.x = element_text(angle = 45, hjust = 1)) + coord_flip() +
    labs(title='Non Progressors', x='', y='Ratio predicted samples'),
  ncol=2)

pg = subset(all.predictions, Status == 'P')
np = subset(all.predictions, Status == 'NP')

```

### Progressors

In progressors, `r round(sum(hgd$correctly.predicted)/sum(hgd$total.hgd.samples), 2)*100`% of the HGD samples were correctly predicted.

When predicting each sample individually `r round(nrow(subset(pg, pred.ratio == 1))/nrow(pg))*100`% patients were correctly predicted in all samples. `r round(nrow(subset(pg, pred.ratio < 1 & pred.ratio > 0))/nrow(pg),2)*100`% of patients were correctly predicted in some samples

`r nrow(subset(pg, pred.ratio == 0))` patients had no correctly predicted samples.

### Non-Progressors

In non-progressors `r round(nrow(subset(np, pred.ratio == 1))/nrow(np), 2)*100`% of patients were predicted to be non-progressors in _all_ of their samples. `r nrow(subset(np, pred.ratio >= 0.7))` patients were predicted correctly in 70% or more of their samples. Only `r nrow(subset(np, pred.ratio < 0.7 & pred.ratio > 0))` patients had fewer than 70% of their samples predicted correctly, and both of these have consistently been predicting _progression_ across multiple endoscopies.


```{r timepoints2, echo=T, message=F, warning=F}
# Only BE, Progressors
ndbe.only = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  e = subset(pt, Pathology %in% c('BE', '', '?'))
  cbind.data.frame('predicted'=sum(as.integer(e$Prediction > 0.5)), 'total.be'=nrow(e))
}))
ndbe.only = subset(ndbe.only, total.be > 0)
ndbe.only$pred.ratio = round(ndbe.only$predicted/ndbe.only$total.be, 2)

ndbe.only$Patients = rownames(ndbe.only)
ndbe.only = transform(ndbe.only, Patients=reorder(Patients, -pred.ratio) ) 

ggplot(ndbe.only[,c('total.be','pred.ratio', 'Patients')], aes(x=Patients, y=pred.ratio)) + 
  geom_bar(stat='identity', fill='darkred', alpha=0.6) + geom_label( aes(label=total.be) ) +
  coord_flip() + labs(title='Correctly predicted NDBE samples', y='Correct prediction ratio')


total = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
    cbind.data.frame('predicted'=sum(as.integer(pt$Prediction > 0.5)), 'total'=nrow(pt))
}))

total$pred.ratio = round(total$predicted/total$total, 2)

total$Patients = rownames(total)
total = transform(total, Patients=reorder(Patients, -pred.ratio) ) 
ggplot(total[,c('total','pred.ratio', 'Patients')], aes(x=Patients, y=pred.ratio)) + 
  geom_bar(stat='identity', fill='purple', alpha=0.6) + geom_label( aes(label=total) ) +
  coord_flip() + labs(title='Correctly predicted Progressor samples', y='Correct prediction ratio')


# For all pre-HGD/IMC/LGD in progressors
pre.hgd = sapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  e = subset(pt, Pathology %nin% c('HGD', 'IMC', 'LGD'))
  e$Prediction
  sum(as.integer(e$Prediction > 0.5))/nrow(e)
})

pred.by.endo<-function(pt, nyears) {
  e = subset(pt, PID %in% unique(pt$PID)[nyears])
  if (  length(which(e$Pathology %in% c('HGD','IMC'))) > 0 ) return(NA)
  p = as.vector(as.integer(e$Prediction > 0.5))
  names(p) = e$Endoscopy.Year
  return(p)
}


predicted.progression<-function(x, pos=1) { 
  if(length(which(is.na(x))) == length(x)) return(NA)
  as.integer(length(which(x == pos)) > 0)  
}

cols = c('%.Predicted','n.Endoscopies','n.Patients', 'FNR','FPR', 'median.pred.samp','median.total.samp', 'max.samples', 'sd.samples')
p.predictions = as.data.frame(matrix(data=0,nrow=6, ncol=length(cols), dimnames=list(c(), cols)))
for (i in 1:nrow(p.predictions)) {
  p = sapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], pred.by.endo, i)
  pp = sapply(p, predicted.progression)
  pp = pp[!is.na(pp)]
  
  fpr = sapply(pg.samp[subset(sum.patient.data, Status == 'NP')$Patient], pred.by.endo, i)
  fpr = sapply(fpr, predicted.progression)
  fpr = fpr[!is.na(fpr)]

  p.predictions[i,] =  rbind(round(sum(pp[!is.na(pp)]/length(pp[!is.na(pp)])), 2), i, length(pp), 
                             round(length(which(pp == 0))/length(pp), 2),
                             round(sum(fpr)/length(fpr), 2), 
                             median(sapply(p[names(which(pp > 0) )], function(x) length(which(x == 1)) )),
                             median(sapply(p[names(which(pp > 0))], length)), 
                             max(sapply(p[names(which(pp > 0))], length)),
                             sd(sapply(p[names(which(pp > 0))], length))
                             )
}

ggplot(p.predictions, aes(y=`%.Predicted`,x=factor(n.Endoscopies))) + 
  geom_bar(stat="identity", fill='dodgerblue3') + 
  geom_text(aes(label=paste(`%.Predicted`*100, '%', sep='')), nudge_y=-0.02, size=8) + 
  geom_text(aes(label=paste(n.Patients, 'patients')), nudge_y=0.01, size=8) + 
  geom_bar(stat='identity', fill='darkred', alpha=0.3, aes(y=FPR)) +
  geom_text(aes(label=FPR, y=FPR)) +
  labs(title="Progression predicted per endoscopy", x='Number of Endoscopies', y='% Patients') +
  theme(text = element_text(size = 14))

```


## Backwards from HGD

Clinicians always ask to see the timepoints peeled backwards to see how early prediction is possible.

```{r, warning=F, message=F, echo=F, fig.width=16, fig.height=8}

back = lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  pt = pt %>% group_by(PID, Endoscopy.Year) %>% summarise(
                  'pred'=sum(as.integer(Prediction > 0.5)), 'samples'=length(PID), 
                  'final.endo'= length(which(grepl('HGD|IMC', Pathology))) > 0,
                  'months.before.final'= (max(pt$Endoscopy.Year) - unique(Endoscopy.Year))*12 )
  arrange(pt, -Endoscopy.Year, PID)
})

endos = do.call(rbind, lapply(1:max(sapply(back,nrow)), function(i) {
  df = do.call(rbind, lapply(back, function(x) {
    if (i > nrow(x)) return(NA)
    cbind(x[i, c('Endoscopy.Year','pred','samples','final.endo', 'months.before.final')], i)
  }))
  df$Patient = rownames(df)
  return(df)
}))

endos = endos[complete.cases(endos),]
endos$i = endos$i-1

endos$pred.ratio = round(endos$pred/endos$samples, 2)
#endos$i = as.factor(endos$i)

#endos$brks <- cut(endos$pred.ratio, include.lowest=T,
#                   breaks=c(0, 0.1, 0.25, 0.5, 0.75, 0.9, 1), 
#                   labels=c('0', '0.1-.25', '.25-.5', '.5-.75','.75-0.9', '1'))
                     
endos = transform(endos, Patient=reorder(Patient, -months.before.final) ) 
endos$months.before.final = endos$months.before.final*-1

plot.endo<-function(df, minM=NULL, maxM=NULL) {
  if (is.null(minM)) minM = min(df$months.before.final)
  if (is.null(maxM)) maxM = max(df$months.before.final)

  df$i = as.factor(df$i)
  ggplot(df, aes(months.before.final, pred.ratio, group=1,color=Patient)) + facet_grid(Patient ~., scales='fixed') + scale_x_continuous(breaks=seq(minM, maxM, by = 12)) +
    geom_line() + geom_point() + labs(x='Months before diagnosis', y='Sample prediction ratio') + theme(legend.position='none') 
}


me = endos %>% group_by(Patient) %>% summarise('max.endo' = max(i))

grid.arrange(
  plot.endo( subset(endos, Patient %in% subset(me, max.endo >=5)$Patient), min(endos$months.before.final), max(endos$months.before.final) ),
  plot.endo( subset(endos, Patient %in% subset(me, max.endo < 5 & max.endo > 2)$Patient),min(endos$months.before.final), max(endos$months.before.final)  ),
  ncol=2)

```

```{r, echo=F, warning=F, message=F, fig.height=8, fig.width=8}
plot.endo( subset(endos, Patient %in% subset(me, max.endo <= 2 & max.endo > 0)$Patient), min(endos$months.before.final), max(endos$months.before.final)  )
```


```{r, echo=F, warning=F, message=F, fig.height=4, fig.width=4}
plot.endo( subset(endos, Patient %in% subset(me, max.endo == 0)$Patient),min(endos$months.before.final), max(endos$months.before.final) )
```

### How many samples?  TBD

Clinicians asked how many samples they needed to take.  


```{r, warning=F, message=F, fig.height=4, fig.width=4, eval=F}
# How do I evaluate the predictions?

#Basically I need to to a FNR and FPR calculation that is extremely conservative and use that somehow

all = do.call(rbind,lapply(pg.samp, function(df) {
  df[,c('Status','Prediction','Prediction.Dev.Resid')]
}))

binomial.deviance

ggplot(all, aes(Prediction.Dev.Resid, Prediction, color=Status)) + geom_point()  + 
  labs(y='Predicted (response)', x='Binomial dev residual?')
```


# Randomize the labels

By patient.  This should break the models...and as would be expected, the models are really poor with perfomance at 50% or less (coin toss).

```{r, echo=T, warning=F, message=F, fig.height=10, fig.width=10}

file = paste(cache.dir, 'rand.Rdata', sep='/')

if (file.exists(file)) {
  load(file, verbose=T)
} else {
  rand.patients = sum.patient.data
  rand.patients$Status = sample( c('NP','P'), nrow(sum.patient.data), replace=T, prob=c(0.5,0.5))

  r.labels = unlist(apply(rand.patients[,c('Patient','Status')],1, function(st) {
    info = patient.data[[ st['Patient'] ]]$info
    lbl = rep(as.integer( st['Status'] == 'P' ), nrow(info))
    names(lbl) = info$Samplename
    return(lbl)
  }))
  names(r.labels) = sub('.*\\.', '',  names(r.labels))
  
  # sort in label order
  r.dysplasia.df = t(mergedDf[,names(r.labels)])

  r.coefs = list(); r.plots = list()
  folds = 10; splits=5
  for (a in c(0,0.5,1)) {
    fitR <- glmnet(r.dysplasia.df, r.labels, alpha=a, nlambda=nl, family='binomial') # all patients
    
    cv.R = crossvalidate.by.patient(x=r.dysplasia.df, y=r.labels, lambda=fitR$lambda, pts=pts, a=a, nfolds=folds, splits=splits, fit=fitR)
    
    lambda.opt = ifelse( length(cv.R$lambda.1se) > 0, cv.R$lambda.1se, cv.R$lambda.min )
    
    coef.opt = as.data.frame(non.zero.coef(fitR, lambda.opt))
    r.coefs[[as.character(a)]] = coef.stability(coef.opt, cv.R$non.zero.cf)
    
    r.plots[[as.character(a)]] = arrangeGrob(cv.R$plot+ggtitle('Classification'), cv.R$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
  }
  save(r.plots, r.coefs, file=file) 
}

do.call(grid.arrange, c(r.plots, top='All samples, Randomized labels (by patient)'))


```



