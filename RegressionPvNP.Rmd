---
title: "RegressionPvNP"
author: "Sarah Killcoyne"
date: "31 March 2017"
output: 
  html_document: 
    fig_caption: yes
    fig_height: 10
    fig_width: 10
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
library(ggplot2)
library(ggfortify)
library(plyr)
library(pander)
library(reshape2)
library(gridExtra)
library(GenomicRanges)
library(glmnet)


load("Test_patients.Rdata", verbose=T)
validation.patient.data = patient.data

dataset = 'Training'
load(paste(dataset, '_patients.Rdata', sep=''), verbose=T)

source('lib/load_patient_metadata.R')

data = '~/Data/Ellie'

data.files = list.files(paste(data, 'QDNAseq',sep='/'), full.names=T)
plot.dir = paste(data, 'Analysis/multipcf_plots_fitted_perPatient', sep='/')

if (length(list.files(plot.dir)) <= 0)
  stop(paste("No analysis files found in", plot.dir ))

## Patient info file
patient.file = grep('All_patient_info.txt', data.files, value=T)
if (length(patient.file) != 1)
  stop(paste("Missing/too many patient info file(s) in", data))

patient.info = read.patient.info(patient.file, set=dataset)
patient.info$Patient = gsub("/", "_", patient.info$Patient)
head(patient.info)

validation.patient.info = read.patient.info(patient.file, set='Test')
sum.validation.info = summarise.patient.info(validation.patient.info)

patient.info = arrange(patient.info, Status, Patient, Endoscopy.Year, Pathology)

sum.patient.data = summarise.patient.info(patient.info)


if (length(which(sapply(patient.data, function(df) nrow(df$seg.vals)) == 0)) > 0)
  warning(paste("Patients missing data:",
    paste(names(which(sapply(patient.data, function(df) nrow(df$seg.vals)) == 0)), collapse=', ')))

patient.data[which(sapply(patient.data, function(df) nrow(df$seg.vals)) == 0)] = NULL
sum.patient.data = as.data.frame(subset(sum.patient.data, Patient %in% names(patient.data))) ## For now

# Missing some of the samples as they aren't all sequenced yet
for (pt in names(patient.data)) {
  #print(pt)
  #print(patient.data[[pt]]$info$Samplename %in% colnames(patient.data[[pt]]$seg.vals)[-(1:5)])
  patient.data[[pt]]$info = patient.data[[pt]]$info[patient.data[[pt]]$info$Samplename %in% colnames(patient.data[[pt]]$seg.vals)[-(1:5)],]
}

pander(sum.patient.data[,c('Patient', 'Status', 'start.year', 'end.year','total.samples','total.endos','highest.path')], justify='left')


```


```{r apclust-func, echo=F, message=F, warning=F}
apclust.data<-function(segdata, samples) {
  x1 = segdata[,intersect(colnames(segdata), samples)]
  x1 = x1[, samples]
  rownames(x1) = (segdata[,c(1:4)] %>%
                    rowwise() %>%
                    mutate(location=paste(paste(chrom, arm, sep=''), '.', start.pos, '-', end.pos, sep='')))$location
  return(x1)  
}
```

The previous analysis employed AP clustering on a per-patient basis to look for evidence that it was possible to see differences between the progressors and non-progressors, and to quantify that in some sort of complexity measure.  It was able to separate them, and continued to do so after removing HGD samples, but quickly failed to model any difference as I removed timepoints from the patient.  

As suggested by Moritz it is possible to merge all of the patient samples and use GLMs to model the differences, and to select regions that may be most reflective of the differences. In this report that's what I've done, and it worked very well.  However, I'm a bit suspicious that it's worked too well.  I'm looking for biases in the data, or mistakes I might have made in setting up the model.

# Encode the large data matrix

To use GLMs across the patients I merge all samples from all patients and overlap the copy number segments. Where segments from a sample get split, the value of the original sample is retained for each of the split samples.


```{r tile, echo=F, message=F, warning=F, include=F}

chr.lengths = get.chr.lengths()
chr.lengths$chrom = sub('chr','',chr.lengths$chrom)
chr.lengths$start = 1

tile.w=1e7

genome = makeGRangesFromDataFrame(chr.lengths[1:22,], seqnames.field = 'chrom', end.field='chr.length')
tiles = tile(genome, width=tile.w)

tile.sample.segments<-function(tileFile, sample.data) {
  if (file.exists(tileFile)) {
    message(paste("Loading", tileFile))
    load(tileFile, verbose=T)
  } else {
    message("Tiling patient data...")
    grList = lapply(sample.data, function(df) {
      print(unique(df$info$Patient))
      df$info = arrange(df$info, Endoscopy.Year, Pathology)
      
      x1 = apclust.data(df$seg.vals, df$info$Samplename)
      
      segnames = as.data.frame(do.call(rbind, sapply(rownames(x1), strsplit, '[p|q].|-')))
      colnames(segnames) = c('chr','start','end')
      segnames[c('start','end')] = lapply(segnames[c('start','end')], function(x) as.numeric(as.character(x)))
      
      pt = unique(df$info$Patient)
      return( makeGRangesFromDataFrame(cbind(segnames,pt,x1), keep.extra.columns=T) )
    })
    
    sampleNames = unlist(sapply(sample.data, function(df) {
      df$info = arrange(df$info, Endoscopy.Year, Pathology)
      x1 = apclust.data(df$seg.vals, df$info$Samplename)
      colnames(x1)
    }))
    
    tile.segments<-function(tiles, gr, mergedSegments) {
      ov = findOverlaps(tiles, gr)
      for (qh in unique(queryHits(ov))) {
        #print(qh)
        currentTile = tiles[[qh]]
        curov = findOverlaps(currentTile, gr)
        
        for (i in 1:length(curov)) {
          segment = currentTile[ queryHits(curov)[i]  ]
          #print(segment)
          rows = with(mergedDf, which( chr==as.character(seqnames(segment)) & start == start(segment) & end == end(segment)))
          segmentVals = as.data.frame(elementMetadata(gr[ subjectHits(curov)[i] ]))
          mergedSegments[rows, names(segmentVals)[-1]] = segmentVals[,-1]
        }
      }
      return(mergedSegments)  
    }
    
    mergedDf = do.call(rbind, lapply(tiles, function(tile) { 
      cbind('chr'=as.character(seqnames(tile)), as.data.frame(ranges(tile))[1:2]) 
    }) )
    mergedDf[,sampleNames] = NA
    
    for (gr in grList) {
      #print(unique(gr$pt))
      mergedDf = tile.segments(tiles, gr, mergedDf)
    }
    
    #tmp = mergedDf
    mergedDf[is.na(mergedDf)] = 0
    rownames(mergedDf) = with(mergedDf, paste(chr, ':', start, '-', end, sep=''))
    
    save(mergedDf, file=tileFile)
  }
  return(mergedDf)
}

mergedDf = tile.sample.segments(paste(as.character(tile.w),'_tiledpts.Rdata', sep=''), patient.data)
dim(mergedDf)
#validatioMergedDf = tile.sample.segments(paste(as.character(tile.w),'validation_tiledpts.Rdata', sep=''), validation.patient.data)

```

Using a very large segment size for tiling across all patients (`r tile.w`) I get the following binomial models.  None of them fit well, and decreasing the size of the segments results in poor (or no) fits with mostly NA coefficients, or a lot of P(0 or 1).

# y = Progressors (1) vs Non (0)

The labels are split such that all samples from progressor patients are (1) and all samples from non-progressors are (0).

```{r labelsPNP, echo=F, message=F, include=F}
## binomial: dysplasia 1, BE 0
labels = unlist(sapply(patient.data, function(df) {
  df$info = arrange(df$info, Endoscopy.Year, Pathology)
  label = as.integer(df$info$Status == 'P') #as.integer(df$info$Pathology %in% c('HGD', 'IMC'))
  names(label) = df$info$Samplename
  return(label)
}))
names(labels) = sub('.*\\.', '',  names(labels))

pts = do.call(rbind, lapply(patient.data, function(df) {
  df$info = arrange(df$info, Endoscopy.Year, Pathology)
  cbind(df$info[,c('Patient','Samplename')])
}))
rownames(pts) = 1:nrow(pts)

# sort in label order
if (length(setdiff(colnames(mergedDf), names(labels))) > 3)
  warning("Labels vector is missing samples")

dysplasia.df = t(mergedDf[,names(labels)])
dim(dysplasia.df)
```

We have `r table(labels)[1]` samples from non-progressors and `r table(labels)[2]` samples from progressors.

Example subset of the data matrix:
`r pander(dysplasia.df[1:5, 1:5], caption=paste("dimensions:", paste(dim(dysplasia.df), collapse=', ')), justify='left')`



```{r cvglmnetPNP, echo=F, warning=F, message=F, eval=F}
## cv.glmnet Ridge vs Lasso
#With all `r ncol(dysplasia.df)` regions of the genome, first check if/which regression may be appropriate. It appears that pure lasso may provide the best balance for number of features with non-zero coefficients.

plot.multi.cv.glmnet<-function(x,y,alpha=c(0,1), family="binomial", title='cv.glmnet') {
  plots <- list(  )
  cv.models = data.frame(matrix(ncol=3,nrow=0,dimnames=list(c(), c('class','lambda','cvm'))))
  for (a in alpha) {
    cv = cv.glmnet(x, y, alpha=a, family=family)
    cv.models = rbind(cv.models, cbind('class'=paste('cv',a,sep=''), 'lambda'=log(cv$lambda), 'cvm'=cv$cvm))
    plots[[as.character(a)]] = autoplot(cv, main=paste('alpha',a), ylab=cv$name)
  }
  cv.models[c('lambda','cvm')] = lapply(cv.models[c('lambda','cvm')], function(x) as.numeric(as.character(x)))
  
  gg = ggplot(cv.models, aes(x=lambda, y=cvm, color=class)) + geom_point() + geom_line() + 
    labs(x='log(Lambda)', y='Binomial Deviance', title=title)
  
  plots[['all']] = gg
  
  return(plots)
}

gp = plot.multi.cv.glmnet(x=dysplasia.df, y=labels, alpha=c(0,0.5,0.8,1), title='cv.glmnet P vs NP')

cv1 = cv.glmnet(dysplasia.df, labels, alpha=1, family='binomial')
gg = autoplot(cv1$glmnet.fit, xvar='lambda', main='alpha=1 glmnet.fit from cv.glmnet') + theme(legend.position='none') 

gp[['coef.1']] = gg

do.call(grid.arrange, c(gp, ncol=2, nrow=3))

```

## Build/Cross-validate by patient

Keeping in mind that the matrix is built on a sample, not patient, basis while the labels (Progressor/Non) are on a per-patient basis I generated the cross validated models for various values of alpha by running 10 or more iterations of 5-fold cross validation that pulled out all the samples for 5 patients at each fold.  1000 lambda values.

```{r cv.funcs, message=F, warning=F, echo=T, fig.height=6, fig.width=6}
pi.hat<-function(x) exp(x)/(1+exp(x))

non.zero.coef<-function(fit, s) {
  cf =  as.matrix(coef(fit, s))
  cf[which(cf != 0),][-1]
}

coef.stability<-function(opt, nz.list) {
  for (i in 1:length(nz.list)) {
    df = as.data.frame(nz.list[[i]])
    colnames(df) = i
    opt = merge(opt, df, by='row.names', all.x=T)
    rownames(opt) = opt$Row.names
    opt$Row.names = NULL
    opt[,(i+1)] = as.integer(!is.na(opt[,(i+1)]))
  }
  opt = opt[order(sapply(rownames(opt), function(x) as.numeric(unlist(strsplit(x, ':'))[1])  )),]
  return(opt)
}

create.patient.sets<-function(pts, n, splits, minR=0.2) {
  # This function just makes sure the sets don't become too unbalanced with regards to the labels.
  check.sets<-function(df, grpCol, min) {
    sets = table(cbind.data.frame('set'=df[[grpCol]], 'labels'=labels[df$Samplename]))
    while ( (length(which(sets/rowSums(sets) < minR) ) >= 2 | length(which(sets/rowSums(sets) == 0)) > 0) ) {
      #print(sets/rowSums(sets))
      s = sample(rep(seq(5), length = length(unique(df$Patient))))
      df2 = merge(df, cbind('Patient'=unique(df$Patient), 'tmpgrp'=s), by="Patient")
      df[[grpCol]] = df2$tmpgrp
      sets = table(cbind.data.frame('set'=df[[grpCol]], 'labels'=labels[df$Samplename]))
    }
    return(df[[grpCol]])
  }
  
  s = sample(rep(seq(splits), length = length(unique(pts$Patient))))
  patients = merge(pts, cbind('Patient'=unique(pts$Patient), 'group'=s), by="Patient")  
  colnames(patients)[3] = c('fold.1')
  patients$fold.1 = check.sets(patients, 'fold.1', minR)
  
  for (i in 2:n) {
    s = sample(rep(seq(5), length = length(unique(pts$Patient))))
    patients = merge(patients, cbind('Patient'=unique(patients$Patient), 'group'=s), by="Patient")  
    foldcol = grep('group',colnames(patients))
    colnames(patients)[foldcol] = paste('fold',i,sep='.')
    patients[[paste('fold',i,sep='.')]] = check.sets(patients, paste('fold',i,sep='.'))
  }
  return(patients)
}

binomial.deviance<-function(pmat, y) {
  # Binomial deviance, lifted from cv.lognet
  prob_min = 1e-05; prob_max = 1 - prob_min
  pmat = pmin(pmax(pmat, prob_min), prob_max)
  dev = apply(pmat, 2, function(p)  -2*((y==1)*log(p)+(y==0)*log(1-p)) )
  return(dev)
}

crossvalidate.by.patient<-function(x,y,lambda,pts,a=1,nfolds=10, splits=5, fit=NULL, minR=0.2, select='classification') {
  if (nfolds > 5) minR = 0.1
  message(paste("Running", splits, "splits",nfolds,"times on", paste(dim(x), collapse=':'), 'alpha=',a ))
  fit.e = list()
  tpts = create.patient.sets(pts, nfolds, splits, minR)
  cv.pred = (matrix(nrow=0, ncol=length(lambda)))
  cv.binomial.deviance = (matrix(nrow=0, ncol=length(lambda)))
  for (n in 1:nfolds) {  
    message(paste(n, "fold"))
    setCol = grep(paste('^fold.',n,'$',sep=''), colnames(tpts))
    
    cv.class = matrix(nrow=splits, ncol=length(lambda))
    deviance = matrix(nrow=splits, ncol=length(lambda))
    for (i in 1:splits) { 
      message(paste(i, "split"))
      test.rows = which(rownames(x) %in% tpts[which(tpts[,setCol] == i), 'Samplename'])
      test = x[test.rows,]
      training = x[-test.rows,]
      # pre-spec lambda seq
      fitCV <- glmnet(training, y[-test.rows], lambda=lambda, family='binomial', alpha=a) 
      # autoplot(fit) + theme(legend.position="none")
      
      # Confusion matrix: quantitiative, pos results + neg results / number of test rows
      pred <- pi.hat(predict(fitCV, test, type='link')) 
      cv.class[i,] = apply(pred, 2, function(p) {
        (p%*%y[test.rows] + (1-p) %*% (1-y[test.rows]))/length(test.rows)
      })
      
      # Binomial deviance, lifted from cv.lognet
      dev = binomial.deviance(predict(fitCV, test, type='response'), as.factor(y[test.rows]))
      deviance[i,] = apply(dev, 2, weighted.mean, w=rep(1, nrow(dev)), na.rm=T)
      
      fit.e[[length(fit.e)+1]] = fitCV
    }
    cv.pred = rbind(cv.pred, cv.class)    
    cv.binomial.deviance = rbind(cv.binomial.deviance, deviance)
  }
  
  # Not really sure what to do with the binomial deviance now...
  df = cbind.data.frame('lambda.at'=1:ncol(cv.pred),
                        'mean'= colMeans(cv.pred), 
                        'sme'= apply(cv.pred, 2, sd)/sqrt(nrow(cv.pred)), 
                        'sd'= apply(cv.pred, 2, sd), 
                        'lambda'= lambda,
                        'log.lambda' = log(lambda),
                        'mean.b.dev' = colMeans(cv.binomial.deviance),
                        'sd.b.dev' = apply(cv.binomial.deviance, 2,sd),
                        'sme.b.dev' = apply(cv.binomial.deviance, 2, sd)/sqrt(nrow(cv.binomial.deviance)),
                        'lambda.min' = F, 'lambda.1se' = F)

  if (select == 'classification') {
    df$lambda.min = df$mean == max(df$mean)
    se1 = df[df$lambda.min == T, 'mean'] - df[df$lambda.min == T, 'sme']
    
    arrange(subset(df, mean < se1 & !lambda.min & log.lambda > subset(df, lambda.min)$log.lambda ), -mean)[1:10,]
    df[arrange(subset(df, mean < se1 & !lambda.min & log.lambda > subset(df, lambda.min)$log.lambda ), -mean)[1, 'lambda.at'], 'lambda.1se'] = T
    #df[df$log.lambda < subset(df, mean == max(mean))$log.lambda-sd(df$log.lambda),][1, 'lambda-1se'] = TRUE
  } else if (select == 'deviance') {
    df$lambda.min = df$mean.b.dev == min(df$mean.b.dev)
    se1 = df[df$lambda.min == T, 'mean.b.dev'] - df[df$lambda.min == T, 'sme.b.dev']
    df[arrange(subset(df, mean.b.dev <= se1 & !lambda.min), -mean.b.dev)[1, 'lambda.at'], 'lambda.1se'] = T
    #df[df$mean.b.dev >= df[df$lambda.min == T, 'mean.b.dev'] + sd(df$mean.b.dev), ][1, 'lambda.1se'] = T
  }

  lambda.min = lambda[subset(df, lambda.min == T)$lambda.at]
  lambda.1se = lambda[subset(df, lambda.1se == T)$lambda.at]
  
  nzcf = lapply(fit.e, non.zero.coef, s=lambda.1se)
  
  plots = plot.patient.cv(df, fit)
  plots$performance = plots$performance + theme(legend.position='bottom') + scale_colour_discrete(name = "") +
    labs(title=paste(splits,' splits, ',nfolds,' folds, alpha=',a, sep=''), y='mean pred.', x='log(lambda)') 
  plots$deviance = plots$deviance + theme(legend.position='bottom') + scale_colour_discrete(name="") + 
    labs(title=paste(splits,' splits, ',nfolds,' folds, alpha=',a, sep=''), y='mean Binomial Deviance', x='log(lambda)') 

  return(list('max.cm'=df[df$lambda.1se ==T,'mean'], 
              'lambda.min'=lambda.min, 'lambda.1se'=lambda.1se, 'lambdas'=df, 'plot'=plots$performance, 'deviance.plot'=plots$deviance, 'non.zero.cf'=nzcf))
}

plot.patient.cv<-function(df, fit=NULL) {
    gp = ggplot(df, aes(y=mean,x=log.lambda)) + geom_point() + geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + 
    geom_point(data=subset(df, lambda.min == T), aes(y=mean, x=log.lambda, colour="lambda.min"), size=2 ) +  
    geom_vline(xintercept = subset(df, lambda.min == T)$log.lambda, colour='grey') +
    annotate("text", x=subset(df, lambda.min == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
             label=round(df[subset(df, lambda.min == T)$lambda.at,'mean'],3)) 
  
    gpD = ggplot(df, aes(y=mean.b.dev,x=log.lambda)) + geom_point() +
    geom_errorbar(aes(ymin=mean.b.dev-sme.b.dev, ymax=mean.b.dev+sme.b.dev)) + 
    geom_point(data=subset(df, lambda.min == T), aes(y=mean.b.dev, x=log.lambda, colour="lambda.min"), size=2 ) +          
    geom_vline(xintercept = subset(df, lambda.min == T)$log.lambda, colour='grey') +
    annotate("text", x=subset(df, lambda.min == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
             label=round(df[subset(df, lambda.min == T)$lambda.at,'mean'],3)) 
  
  if (nrow(subset(df, lambda.1se == T)) > 0) {
    gp = gp + geom_point(data=subset(df, lambda.1se == T), aes(y=mean, x=log.lambda, colour="lambda.1se"), size=2 ) +
      geom_vline(xintercept = subset(df, lambda.1se == T)$log.lambda, colour='grey') +
      annotate("text", x=subset(df, lambda.1se == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
               label=round(df[subset(df, lambda.1se == T)$lambda.at,'mean'],3)) 

    gpD = gpD + geom_point(data=subset(df, lambda.1se == T), aes(y=mean.b.dev, x=log.lambda, colour="lambda.1se"), size=2 ) +
      geom_vline(xintercept = subset(df, lambda.1se == T)$log.lambda, colour='grey') +
      annotate("text", x=subset(df, lambda.1se == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
               label=round(df[subset(df, lambda.1se == T)$lambda.at,'mean'],3)) 
  }
  

  gp = gp + theme(legend.position='bottom') + scale_colour_discrete(name = "") 
  gpD = gpD + theme(legend.position='bottom') + scale_colour_discrete(name="") 

  if (!is.null(fit)) {
    df$nzcoef = sapply(df$lambda, function(l) length(non.zero.coef(fit, l)))
    
    coef.min = subset(df, lambda.min == T)$nzcoef
    coef.1se = subset(df, lambda.1se == T)$nzcoef

    d = coef.1se-coef.min
    
    coef.text = arrange(subset(df,nzcoef %in% c(0,  coef.min, coef.1se, max(nzcoef) )), -lambda.min, -lambda.1se)
    coef.text = arrange(coef.text[!duplicated(coef.text$nzcoef),], -nzcoef)
    
    gp = gp + annotate("text", y=min(df$mean), x=coef.text$log.lambda, label=coef.text$nzcoef, color='darkblue')
    gpD = gpD + annotate("text", y=max(df$mean.b.dev), x=coef.text$log.lambda, label=coef.text$nzcoef, color='darkblue')
  }
return(list('performance'=gp, 'deviance'=gpD))
}

```


```{r xvalpt, message=F, warning=F, echo=T, fig.height=16, fig.width=16}
coefs = list(); plots = list(); performance.at.1se = list()
folds = 10; splits = 5
alpha.values = c(0,0.5,0.7,0.8,0.9,1)
if (file.exists('/tmp/all.pt.alpha.Rdata')) {
  load('/tmp/all.pt.alpha.Rdata', verbose=F)
} else {
  for (a in alpha.values) {
    fit0 <- glmnet(dysplasia.df, labels, alpha=a, nlambda=1000, family='binomial') # all patients
    #autoplot(fit0, xvar='lambda', main=paste('fit0, all samples, alpha=',a,sep='')) + theme(legend.position='none') 
    
    cv.patient = crossvalidate.by.patient(x=dysplasia.df, y=labels, lambda=fit0$lambda, pts=pts, a=a, nfolds=folds, splits=splits, fit=fit0)
    
    lambda.opt = ifelse( length(cv.patient$lambda.1se) > 0, cv.patient$lambda.1se, cv.patient$lambda.min )
    
    coef.opt = as.data.frame(non.zero.coef(fit0, lambda.opt))
    coefs[[as.character(a)]] = coef.stability(coef.opt, cv.patient$non.zero.cf)
    
    plots[[as.character(a)]] = arrangeGrob(cv.patient$plot+ggtitle('Classification'), cv.patient$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
    
    performance.at.1se[[as.character(a)]] = subset(cv.patient$lambdas, lambda.1se == T)
  }
  save(plots, coefs, performance.at.1se, file='/tmp/all.pt.alpha.Rdata')
}
  
do.call(grid.arrange, c(plots, top='All samples, 10fold, 5 splits'))
```


```{r xvalpt2, message=F, warning=F, echo=T}

performance.at.1se = do.call(rbind, performance.at.1se)
performance.at.1se$alpha = rownames(performance.at.1se)

performance.at.1se = cbind(performance.at.1se, do.call(rbind, lapply(coefs, function(x) cbind('n.Coef'=nrow(x), '75%'=length(which(rowSums(x)/50 >= 0.75))))))

ggplot(performance.at.1se, aes(alpha, mean)) + geom_point() +
  geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + ylim(0.5,0.8) +
  geom_text(aes(label=n.Coef), nudge_y=0.03) +
  geom_text(aes(label=paste('(',`75%`,')',sep='')), nudge_y=0.02) +
  labs(x='Elasticnet penalty value', y='Model performance at lambda.1se', 
       title=paste('All samples,', folds, 'folds,', splits, 'patient splits'))
  

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=length(names(coef.stable)), 
                           dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) 
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1

table(rowSums(cfs))

coef.stable = lapply(coef.stable, function(cf) cf/(folds*splits))

stability = do.call(rbind, lapply(coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(stability) = names(coef.stable)
```

Values that are closer to full lasso (alpha=1) are pretty similar, however at full lasso we see fewer features than if we regularize it at 0.8. They share all non-zero features as well.
`r pander(sapply(coef.stable, length), caption='Total features at each value of alpha')`

### Feature stability

`r pander(table(rowSums(cfs)), caption="Number of features found in 1, 2, or all 3 models by alpha 0.8, 0.9 and 1")`


`r length(unique(unlist(lapply(coef.stable, function(cf)  names(which(cf > 0.8)) ))))` features are shared at all values of alpha and were non-zero in at least 80% of the folds.


### Remove HGD/IMC Samples

Just as a point, the initial model is trained against `r nrow(dysplasia.df)` samples with `r ncol(dysplasia.df)` features.  What happens if we retrain it without HGD?

```{r noHGD, echo=T, message=F, warning=F, fig.height=16, fig.width=16}
`%nin%` <- Negate(`%in%`)

info = do.call(rbind, lapply(patient.data, function(df) df$info))

no.hgd.plots = list(); coefs = list(); performance.at.1se = list()
if (file.exists('/tmp/nohgd.Rdata')) {
  load('/tmp/nohgd.Rdata', verbose=T)
} else {
  # No HGD/IMC
  samples = intersect(rownames(dysplasia.df), subset(info, Pathology %nin% c('HGD', 'IMC'))$Samplename)
  for (a in alpha.values) {
    # all patients
    fitNoHGD <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, family='binomial', nlambda=1000) 

    cv.nohgd = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitNoHGD$lambda, pts=subset(pts, Samplename %in% samples), a=a, nfolds=folds, splits=splits, fit=fitNoHGD)
    
    no.hgd.plots[[as.character(a)]] = arrangeGrob(cv.nohgd$plot+ggtitle('Classification'), cv.nohgd$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
    
    coef.1se = as.data.frame(non.zero.coef(fitNoHGD, cv.nohgd$lambda.1se))
    coefs[[as.character(a)]] = coef.stability(coef.1se, cv.nohgd$non.zero.cf)
    
    performance.at.1se[[as.character(a)]] = subset(cv.nohgd$lambdas, lambda.1se == T)
  }
  save(no.hgd.plots, coefs, performance.at.1se, file='/tmp/nohgd.Rdata')
}
  
do.call(grid.arrange, c(no.hgd.plots, top="No HGD/IMC samples", ncol=2))
```

```{r noHGD2, echo=T, message=F, warning=F}
performance.at.1se = do.call(rbind, performance.at.1se)
performance.at.1se$alpha = rownames(performance.at.1se)

performance.at.1se = cbind(performance.at.1se, do.call(rbind, lapply(coefs, function(x) cbind('n.Coef'=nrow(x), '75%'=length(which(rowSums(x)/50 >= 0.75))))))

ggplot(performance.at.1se, aes(alpha, mean)) + geom_point() +
  geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + ylim(0.5,0.8) +
  geom_text(aes(label=n.Coef), nudge_y=0.03) +
  geom_text(aes(label=paste('(',`75%`,')',sep='')), nudge_y=0.02) +
  labs(x='Elasticnet penalty value', y='Model performance at lambda.1se', 
       title=paste('No HGD/IMC samples,', folds, 'folds,', splits, 'patient splits'))

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=3, 
                           dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) {
  if (is.null(names(coef.stable$`0.8`))) next
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1
}

coef.stable = lapply(coef.stable, function(cf) cf/(folds*splits))

stability = do.call(rbind, lapply(coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(stability) = names(coef.stable)
```

#### Feature Stability

`r pander(stability, caption='Total features found at each value of alpha in 50% or more of the folds run')`

They also share all of the non-zero coefficients (`r which.max(sapply(coef.stable, length))` identifies the most non-zero coefs `r max(sapply(coef.stable, length))`).

`r pander(table(rowSums(cfs)), justify='left')`


### Remove HGD/IMC & LGD Samples

Now remove all LGD samples from the progressor's and retrain.

```{r noLGD, echo=T, message=F, warning=F, fig.height=16, fig.width=16}
nolgd.plots = list(); coefs = list(); performance.at.1se = list()
if (file.exists('/tmp/nolgd.Rdata')) {
  load('/tmp/nolgd.Rdata', verbose=T)
} else {
  # No LGD
  samples = intersect(rownames(dysplasia.df), c(subset(info, Status == 'NP')$Samplename, subset(info, Pathology %nin% c('HGD', 'IMC', 'LGD') & Status == 'P')$Samplename))
  
  # No HGD/IMC/LGD in all patients
  samples = intersect(rownames(dysplasia.df), subset(info, Pathology %nin% c('HGD', 'IMC', 'LGD'))$Samplename)
  
  for (a in alpha.values) {
    fitNoLGD <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, family='binomial', nlambda=1000) # all patients

    cv.nolgd = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitNoLGD$lambda, pts=subset(pts, Samplename %in% samples), a=a, nfolds=folds, splits=splits, fit=fitNoLGD)
    
    nolgd.plots[[as.character(a)]] = arrangeGrob(cv.nolgd$plot+ggtitle('Classification'), cv.nolgd$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
    
    coef.1se = as.data.frame(non.zero.coef(fitNoLGD, cv.nolgd$lambda.1se))
    coefs[[as.character(a)]] = coef.stability(coef.1se, cv.nolgd$non.zero.cf)
    
    performance.at.1se[[as.character(a)]] = subset(cv.nolgd$lambdas, lambda.1se == T)
  }
  save(nolgd.plots, coefs, performance.at.1se, file='/tmp/nolgd.Rdata')
}
  
do.call(grid.arrange, c(plots, top="No HGD/IMC/LGD samples", ncol=2))
```

```{r noLGD2, echo=T, message=F, warning=F}
performance.at.1se = do.call(rbind, performance.at.1se)
performance.at.1se$alpha = rownames(performance.at.1se)

performance.at.1se = cbind(performance.at.1se, do.call(rbind, lapply(coefs, function(x) cbind('n.Coef'=nrow(x), '75%'=length(which(rowSums(x)/50 >= 0.75))))))

ggplot(performance.at.1se, aes(alpha, mean)) + geom_point() +
  geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + ylim(0.5,0.8) +
  geom_text(aes(label=n.Coef), nudge_y=0.03) +
  geom_text(aes(label=paste('(',`75%`,')',sep='')), nudge_y=0.02) +
  labs(x='Elasticnet penalty value', y='Model performance at lambda.1se', 
       title=paste('No HGD/IMC/LGD samples,', folds, 'folds,', splits, 'patient splits'))


## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=3, 
                           dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) 
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1

coef.stable = lapply(coef.stable, function(cf) cf/(folds*splits))

stability = do.call(rbind, lapply(coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(stability) = names(coef.stable)

```

#### Feature Stability

`r pander(stability, caption='Total features found at each value of alpha in 50% or more of the folds run')`

They also share all of the non-zero coefficients (`r which.max(sapply(coef.stable, length))` identifies the most non-zero coefs `r max(sapply(coef.stable, length))`).

`r pander(table(rowSums(cfs)), justify='left')`


## Compare progressors with many samples vs few

```{r, echo=T, message=F, warning=F, fig.height=16, fig.width=16}
m = median(subset(sum.patient.data, Status == 'P')$total.samples)
nonps = as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'NP')$Patient], function(df) df$info$Samplename)))

if (file.exists('/tmp/few.Rdata')) {
  load('/tmp/few.Rdata', verbose=T)
} else {
  # Few
  samples = c(nonps, as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'P' & total.samples < m)$Patient], function(pt) pt$info$Samplename))))

  few.plots = list()
  for (a in alpha.values) {
    fitF <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, nlambda=500, family='binomial') # all patients
    cv.back.few = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitF$lambda, pts=subset(pts, Samplename %in% samples), splits=5, nfolds=20, a=a, fit=fitF, minR=0.1)
    #few.plots[[as.character(a)]] = cv.back.few$plot + ggtitle(paste('alpha=',a,sep=''))
    
    few.plots[[as.character(a)]] = arrangeGrob(cv.back.few$plot+ggtitle('Classification'), cv.back.few$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
  }
  save(few.plots, file='/tmp/few.Rdata')
} 
do.call(grid.arrange, c(few.plots, ncol=2, top=paste(nrow(subset(sum.patient.data, Status == 'P' & total.samples < m))," Progressors with < median # samples (",m,")")))

# Many
if (file.exists('/tmp/many.Rdata')) {
  load('/tmp/many.Rdata', verbose=T)
} else {
  samples = c(nonps, as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'P' & total.samples >= m)$Patient], function(pt) pt$info$Samplename))))
  
  many.plots = list()
  for (a in alpha.values) {
    fitM <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, nlambda=500, family='binomial') # all patients
    
    cv.back.many = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitM$lambda, pts=subset(pts, Samplename %in% samples), splits=5, nfolds=20, a=a, fit=fitM)

    many.plots[[as.character(a)]] = arrangeGrob(cv.back.many$plot+ggtitle('Classification'), cv.back.many$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
  }
  save(many.plots, file='/tmp/many.Rdata')
}

do.call(grid.arrange, c(many.plots, ncol=2, top=paste(nrow(subset(sum.patient.data, Status == 'P' & total.samples >= m))," Progressors with >= median # samples (",m,")")))

```

# Leave one out 

Leave out each patient (both P and NP) and run a new CV fit each time then predict the samples for the patient that was left out.  Fitted model includes HGD samples.

```{r leaveoneout, echo=T, message=F, warning=T}

info = do.call(rbind, lapply(patient.data, function(df) df$info))
pg.samp = lapply(patient.data, function(pt) {
  info = pt$info
  info$SampleSD = apply(pt$seg.vals[,info$Samplename], 2, sd)
  info$SampleMEAN =  apply(pt$seg.vals[,info$Samplename], 2, mean)
  info$Prediction = NA
  info$Prediction.Dev.Resid = NA
  info$PID = unlist(lapply(info$Path.ID, function(x) unlist(strsplit(x, 'B'))[1]))
  return(info)
})


if (file.exists('/tmp/loo.Rdata')) {
  load('/tmp/loo.Rdata', verbose=T)
} else {
  performance.at.1se = c(); coefs = list(); plots = list(); fits = list()
  # Remove each patient (LOO)
  for (pt in names(pg.samp)) {
    tmp.patient.data = patient.data[subset(sum.patient.data, Patient != pt)$Patient]
    samples = as.vector(unlist(sapply(tmp.patient.data, function(df) df$info$Samplename )))
    length(samples)
    
    # Fit generated on all samples, including HGD
    a = 1
    fitLOO <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, family='binomial', nlambda=1000) # all patients
    cv = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitLOO$lambda, 
                                  pts=subset(pts, Samplename %in% samples), a=a, nfolds=10, splits=5, fit=fitLOO)
    #plots[[pt]] = cv$plot + ggtitle(pt)
    
    plots[[pt]] = arrangeGrob(cv$plot+ggtitle('Classification'), cv$deviance.plot+ggtitle('Binomial Deviance'), top=pt, ncol=2)
    
    fits[[pt]] = cv  
    
    if ( length(cv$lambda.1se) > 0 ) {
      performance.at.1se = c(performance.at.1se, subset(cv$lambdas, lambda == cv$lambda.1se)$mean)
      
      coef.1se = as.data.frame(non.zero.coef(fitLOO, cv$lambda.1se))
      coefs[[pt]] = coef.stability(coef.1se, cv$non.zero.cf)
      
      logit <- function(p){log(p/(1-p))}
      inverse.logit <- function(or){1/(1 + exp(-or))}

      pm = predict(fitLOO, dysplasia.df[pg.samp[[pt]]$Samplename,], s=cv$lambda.1se, type='response')
      sy = sqrt(binomial.deviance(pm, labels[pg.samp[[pt]]$Samplename]))
    
      #df = cbind.data.frame(pm, sy, 1:length(sy))
      #colnames(df) = c('pred','dev','x')
      #ggplot(df, aes(x, pred)) + geom_point() + geom_errorbar(aes(ymin=pred-dev, ymax=pred+dev))
      
    pg.samp[[pt]]$Prediction = pm[,1]
    pg.samp[[pt]]$Prediction.Dev.Resid = sy[,1] 
      
    } else {
      warning(paste("Patient", pt, "did not have a 1se"))
    }
  }
  save(plots, performance.at.1se, coefs, fits, pg.samp, file='/tmp/loo.Rdata')
}

ggplot(as.data.frame(performance.at.1se), aes(y=performance.at.1se, x='LOO')) + ylim(0.5,0.9) +
  geom_boxplot( fill='lightblue', color='darkgrey', outlier.fill=NA, outlier.color=NA) + geom_jitter() +
  labs(y='performance at lambda.1se', x='', title='Performance for Leave One (patient) Out, at alpha=1')

```


## How do the predictions look at each time point?

Providing a single sample at a time to predict is unnecessary as each sample is independently predicted from the test matrix. Predictions are exactly the same even when a single sample is provided (checked).  

```{r timepoints, echo=F, message=F, warning=F}
# For HGD/IMC samples, progressors only
hgd = sapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  hgd = subset(pt, Pathology %in% c('HGD', 'IMC'))
  sum(as.integer(hgd$Prediction > 0.5))/nrow(hgd)
})

# All samples
all.predictions = sapply(pg.samp, function(pt) {
  if (unique(pt$Status) == 'NP') {
    as.integer(pt$Prediction < 0.5)
  } else {
    as.integer(pt$Prediction > 0.5)
  }
})

np = sapply(all.predictions[subset(sum.patient.data, Status == 'NP')$Patient], function(x) round(sum(x)/length(x),2))
pg = sapply(all.predictions[subset(sum.patient.data, Status == 'P')$Patient], function(x) round(sum(x)/length(x),2))

```

### Progressors

In progressors, `r round(sum(hgd,na.rm=T)/length(hgd[!is.na(hgd)]), 2)*100`% of the HGD samples were correctly predicted.

When predicting each sample individually `r length(pg[pg == 1])` patients were correctly predicted in all samples. The remaining patients were predicted in `r max(pg[pg != 1])*100`% or fewer samples.   

`r pander( as.data.frame(sort(pg)), caption='Proportion of progressor patients samples correctly predicted')` 

### Non-Progressors

In non-progressors `r length(which(np == 1))` patients were predicted to be non-progressors in _all_ of their samples. `r length(which(np >= 0.75))` were predicted in 75% or more of their samples. `r length(np[which(np < 0.75 )])` patients had less than 75% of their samples predicted as NP.

`r pander( as.data.frame(sort(np)), caption='Proportion of NP patients samples correctly predicted') ` 


```{r timepoints2, echo=T, message=F, warning=F}
# For all pre-HGD/IMC/LGD in progressors
pre.hgd = sapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  e = subset(pt, Pathology %nin% c('HGD', 'IMC', 'LGD'))
  e$Prediction
  sum(as.integer(e$Prediction > 0.5))/nrow(e)
})

pred.by.endo<-function(pt, nyears) {
  e = subset(pt, PID %in% unique(pt$PID)[1:nyears])
  if (  length(which(e$Pathology %in% c('HGD','IMC'))) > 0 ) return(NA)
  p = as.vector(as.integer(e$Prediction > 0.5))
  names(p) = e$Endoscopy.Year
  return(p)
}

predicted.progression<-function(x, pos=1) { 
  if(length(which(is.na(x))) == length(x)) return(NA)
  as.integer(length(which(x == pos)) > 0)  
}

p.predictions = as.data.frame(matrix(data=0,nrow=6, ncol=3, dimnames=list(c(), c('%.Predicted','n.Endoscopies','n.Patients'))))
for (i in 1:nrow(p.predictions)) {
  p = sapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], pred.by.endo, i)
  pp = sapply(p, predicted.progression)
  pp = pp[!is.na(pp)]
  p.predictions[i,] =  rbind(round(sum(pp[!is.na(pp)]/length(pp[!is.na(pp)])), 2), i, length(pp))
}

ggplot(p.predictions, aes(y=`%.Predicted`,x=n.Endoscopies)) + 
  geom_bar(stat="identity", fill='dodgerblue3') + 
  geom_text(aes(label=paste(`%.Predicted`*100, '%', sep='')), nudge_y=-0.02) + 
  geom_text(aes(label=paste(n.Patients, 'patients')), nudge_y=0.005) + ggtitle("Progression predicted per endoscopy")
```


`r pander(p.predictions, justify='left', caption=paste("For any given endoscopy if a single sample (same pathology ID, different blocks) is predicted as 'progressor' that endoscopy is considered to be indicative of progression. At each endoscopy the percentage is calculated excluding any patients that are diagnosed HGD at that point. There are ",length(subset(sum.patient.data, Status == 'P')$Patient), " progressor patients. Based on that, ", p.predictions[1,1]*100, "% are correctly predicted at the first endoscopy.", sep=''))`

What about the non-progressors that had progressor predictions?
```{r, warning=F, message=F}

np.predictions = as.data.frame(matrix(data=0,nrow=6, ncol=3, dimnames=list(c(), c('%.Predicted','n.Endoscopies','n.Patients'))))
for (i in 1:nrow(np.predictions)) {
  p = sapply(pg.samp[subset(sum.patient.data, Status == 'NP')$Patient], pred.by.endo, i)
  pp = sapply(p, predicted.progression)
  pp = pp[!is.na(pp)]
  np.predictions[i,] =  rbind(round(sum(pp[!is.na(pp)]/length(pp[!is.na(pp)])), 2), i, length(pp))
}


```

# How do I evaluate the predictions?

I'm a bit at a loss here. I understand why penalized regression wouldn't have a CI, but then I'm not sure how best to evaluate the predictions.  Do I just take the predictions at face value and leave it at that?

```{r, warning=F, message=F, fig.height=4, fig.width=4}
all = do.call(rbind,lapply(pg.samp, function(df) {
  df[,c('Status','Prediction','Prediction.Dev.Resid')]
}))

binomial.deviance

ggplot(all, aes(Prediction.Dev.Resid, Prediction, color=Status)) + geom_point()  + 
  labs(y='Predicted (response)', x='Binomial dev residual?')
```


# Randomize the labels

By patient.  This should break the models...and as would be expected, the models are really poor with perfomance at 50% or less (coin toss).

```{r, echo=T, warning=F, message=F}

rand.patients = sum.patient.data
rand.patients$Status = sample( c('NP','P'), nrow(sum.patient.data), replace=T, prob=c(0.5,0.5))

r.labels = unlist(apply(rand.patients[,c('Patient','Status')],1, function(st) {
  info = patient.data[[ st['Patient'] ]]$info
  lbl = rep(as.integer( st['Status'] == 'P' ), nrow(info))
  names(lbl) = info$Samplename
  return(lbl)
}))


# sort in label order
r.dysplasia.df = t(mergedDf[,names(r.labels)])

r.coefs = list(); r.plots = list()
folds = 10; splits=5
for (a in c(0,0.5,1)) {
  fitR <- glmnet(r.dysplasia.df, r.labels, alpha=a, nlambda=1000, family='binomial') # all patients

  cv.R = crossvalidate.by.patient(x=r.dysplasia.df, y=r.labels, lambda=fitR$lambda, pts=pts, a=a, nfolds=folds, splits=splits, fit=fitR)
    
  lambda.opt = ifelse( length(cv.R$lambda.1se) > 0, cv.R$lambda.1se, cv.R$lambda.min )
    
  coef.opt = as.data.frame(non.zero.coef(fitR, lambda.opt))
  r.coefs[[as.character(a)]] = coef.stability(coef.opt, cv.R$non.zero.cf)
    
  r.plots[[as.character(a)]] = arrangeGrob(cv.R$plot+ggtitle('Classification'), cv.R$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
}
  
do.call(grid.arrange, c(r.plots, top='All samples, Randomized labels (by patient)'))

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
r.coef.stable = lapply( r.coefs, function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(unlist(sapply(r.coef.stable, names)))), ncol=length(names(r.coef.stable)), 
                           dimnames=list(unique(unlist(sapply(r.coef.stable, names))), names(r.coef.stable) )))

for (i in names(r.coef.stable)) 
  cfs[intersect(rownames(cfs), names(r.coef.stable[[i]])), i] = 1

r.coef.stable = lapply(r.coef.stable, function(cf) cf/(folds*splits))

r.stability = do.call(rbind, lapply(r.coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(r.stability) = names(r.coef.stable)

```


`r pander(table(rowSums(cfs)))`



