---
title: "RegressionPvNP"
author: "Sarah Killcoyne"
date: "23 May 2017"
output: 
  html_document: 
    toc: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
library(biomaRt)
library(ggplot2)
library(ggfortify)
library(GGally)
library(plyr)
library(pander)
library(reshape2)
library(gridExtra)
library(GenomicRanges)
library(glmnet)


load("Test_patients.Rdata", verbose=T)
validation.patient.data = patient.data

dataset = 'Training'
load(paste(dataset, '_patients.Rdata', sep=''), verbose=T)

source('lib/load_patient_metadata.R')

data = '~/Data/Ellie'

data.files = list.files(paste(data, 'QDNAseq',sep='/'), full.names=T)
analysis.files = list.files(paste(data, 'Analysis', sep='/'), full.names=T)


## Patient info file
patient.file = grep('All_patient_info.txt', data.files, value=T)
if (length(patient.file) != 1)
  stop(paste("Missing/too many patient info file(s) in", data))

patient.info = read.patient.info(patient.file, set=dataset)
patient.info$Patient = gsub("/", "_", patient.info$Patient)
head(patient.info)

validation.patient.info = read.patient.info(patient.file, set='Test')
sum.validation.info = summarise.patient.info(validation.patient.info)

patient.info = arrange(patient.info, Status, Patient, Endoscopy.Year, Pathology)

sum.patient.data = summarise.patient.info(patient.info)


if (length(which(sapply(patient.data, function(df) nrow(df$seg.vals)) == 0)) > 0)
  warning(paste("Patients missing data:",
                paste(names(which(sapply(patient.data, function(df) nrow(df$seg.vals)) == 0)), collapse=', ')))

patient.data[which(sapply(patient.data, function(df) nrow(df$seg.vals)) == 0)] = NULL
sum.patient.data = as.data.frame(subset(sum.patient.data, Patient %in% names(patient.data))) ## For now

# Missing some of the samples as they aren't all sequenced yet
for (pt in names(patient.data)) {
  #print(pt)
  #print(patient.data[[pt]]$info$Samplename %in% colnames(patient.data[[pt]]$seg.vals)[-(1:5)])
  patient.data[[pt]]$info = patient.data[[pt]]$info[patient.data[[pt]]$info$Samplename %in% colnames(patient.data[[pt]]$seg.vals)[-(1:5)],]
}

pander(sum.patient.data[,c('Patient', 'Status', 'start.year', 'end.year','total.samples','total.endos','highest.path')], justify='left')


```


The previous analysis employed AP clustering on a per-patient basis to look for evidence that it was possible to see differences between the progressors and non-progressors, and to quantify that in some sort of complexity measure.  It was able to separate them, and continued to do so after removing HGD samples, but quickly failed to model any difference as I removed timepoints from the patient.  

As suggested by Moritz it is possible to merge all of the patient samples and use GLMs to model the differences, and to select regions that may be most reflective of the differences. In this report that's what I've done, and it worked very well.  However, I'm a bit suspicious that it's worked too well.  I'm looking for biases in the data, or mistakes I might have made in setting up the model.

# Encode the large data matrix

To use GLMs across the patients I merge all samples from all patients and overlap the copy number segments. Where segments from a sample get split, the value of the original sample is retained for each of the split samples.


```{r tile, echo=F, message=F, warning=F, include=F}
tile.w=1e07
#tile.w=5e06
tile.files = grep(as.character(tile.w), analysis.files, value=T, fixed=T)
load(tile.files[1], verbose=T)
dim(mergedDf)

cache.dir = paste(data, 'Analysis',sub('\\+', '', tile.w), sep='/')
if (!dir.exists(cache.dir)) dir.create(cache.dir)

#validatioMergedDf = tile.sample.segments(paste(as.character(tile.w),'validation_tiledpts.Rdata', sep=''), validation.patient.data)

```

Using a very large segment size for tiling across all patients (`r tile.w`) I get the following binomial models.  None of them fit well, and decreasing the size of the segments results in poor (or no) fits with mostly NA coefficients, or a lot of P(0 or 1).

# y = Progressors (1) vs Non (0)

The labels are split such that all samples from progressor patients are (1) and all samples from non-progressors are (0).

```{r labelsPNP, echo=F, message=F, include=F}
## binomial: dysplasia 1, BE 0
labels = unlist(sapply(patient.data, function(df) {
  df$info = arrange(df$info, Endoscopy.Year, Pathology)
  label = as.integer(df$info$Status == 'P') #as.integer(df$info$Pathology %in% c('HGD', 'IMC'))
  names(label) = df$info$Samplename
  return(label)
}))
names(labels) = sub('.*\\.', '',  names(labels))

pts = do.call(rbind, lapply(patient.data, function(df) {
  df$info = arrange(df$info, Endoscopy.Year, Pathology)
  cbind(df$info[,c('Patient','Samplename')])
}))
rownames(pts) = 1:nrow(pts)

# sort in label order
if (length(setdiff(colnames(mergedDf), names(labels))) > 3)
  warning("Labels vector is missing samples")

dysplasia.df = t(mergedDf[,names(labels)])
dim(dysplasia.df)
```

We have `r table(labels)[1]` samples from non-progressors and `r table(labels)[2]` samples from progressors.

Example subset of the data matrix:
`r pander(dysplasia.df[1:5, 1:5], caption=paste("dimensions:", paste(dim(dysplasia.df), collapse=', ')), justify='left')`

```{r cvglmnetPNP, echo=F, warning=F, message=F, eval=F}
## cv.glmnet Ridge vs Lasso
#With all `r ncol(dysplasia.df)` regions of the genome, first check if/which regression may be appropriate. It appears that pure lasso may provide the best balance for number of features with non-zero coefficients.

plot.multi.cv.glmnet<-function(x,y,alpha=c(0,1), family="binomial", title='cv.glmnet') {
  plots <- list(  )
  cv.models = data.frame(matrix(ncol=3,nrow=0,dimnames=list(c(), c('class','lambda','cvm'))))
  for (a in alpha) {
    cv = cv.glmnet(x, y, alpha=a, family=family)
    cv.models = rbind(cv.models, cbind('class'=paste('cv',a,sep=''), 'lambda'=log(cv$lambda), 'cvm'=cv$cvm))
    plots[[as.character(a)]] = autoplot(cv, main=paste('alpha',a), ylab=cv$name)
  }
  cv.models[c('lambda','cvm')] = lapply(cv.models[c('lambda','cvm')], function(x) as.numeric(as.character(x)))
  
  gg = ggplot(cv.models, aes(x=lambda, y=cvm, color=class)) + geom_point() + geom_line() + 
    labs(x='log(Lambda)', y='Binomial Deviance', title=title)
  
  plots[['all']] = gg
  
  return(plots)
}

gp = plot.multi.cv.glmnet(x=dysplasia.df, y=labels, alpha=c(0,0.5,0.8,1), title='cv.glmnet P vs NP')

cv1 = cv.glmnet(dysplasia.df, labels, alpha=1, family='binomial')
gg = autoplot(cv1$glmnet.fit, xvar='lambda', main='alpha=1 glmnet.fit from cv.glmnet') + theme(legend.position='none') 

gp[['coef.1']] = gg

do.call(grid.arrange, c(gp, ncol=2, nrow=3))

```

## Build/Cross-validate by patient

Keeping in mind that the matrix is built on a sample, not patient, basis while the labels (Progressor/Non) are on a per-patient basis I generated the cross validated models for various values of alpha by running 10 or more iterations of 5-fold cross validation that pulled out all the samples for 5 patients at each fold.  1000 lambda values.

```{r cv.funcs, message=F, warning=F, echo=T, fig.height=6, fig.width=6}
pi.hat<-function(x) exp(x)/(1+exp(x))

non.zero.coef<-function(fit, s) {
  cf =  as.matrix(coef(fit, s))
  cf[which(cf != 0),][-1]
}

coef.stability<-function(opt, nz.list) {
  for (i in 1:length(nz.list)) {
    df = as.data.frame(nz.list[[i]])
    colnames(df) = i
    opt = merge(opt, df, by='row.names', all.x=T)
    rownames(opt) = opt$Row.names
    opt$Row.names = NULL
    opt[,(i+1)] = as.integer(!is.na(opt[,(i+1)]))
  }
  opt = opt[order(sapply(rownames(opt), function(x) as.numeric(unlist(strsplit(x, ':'))[1])  )),]
  return(opt)
}

create.patient.sets<-function(pts, n, splits, minR=0.2) {
  # This function just makes sure the sets don't become too unbalanced with regards to the labels.
  check.sets<-function(df, grpCol, min) {
    sets = table(cbind.data.frame('set'=df[[grpCol]], 'labels'=labels[df$Samplename]))
    while ( (length(which(sets/rowSums(sets) < minR) ) >= 2 | length(which(sets/rowSums(sets) == 0)) > 0) ) {
      #print(sets/rowSums(sets))
      s = sample(rep(seq(5), length = length(unique(df$Patient))))
      df2 = merge(df, cbind('Patient'=unique(df$Patient), 'tmpgrp'=s), by="Patient")
      df[[grpCol]] = df2$tmpgrp
      sets = table(cbind.data.frame('set'=df[[grpCol]], 'labels'=labels[df$Samplename]))
    }
    return(df[[grpCol]])
  }
  
  s = sample(rep(seq(splits), length = length(unique(pts$Patient))))
  patients = merge(pts, cbind('Patient'=unique(pts$Patient), 'group'=s), by="Patient")  
  colnames(patients)[3] = c('fold.1')
  patients$fold.1 = check.sets(patients, 'fold.1', minR)
  
  for (i in 2:n) {
    s = sample(rep(seq(5), length = length(unique(pts$Patient))))
    patients = merge(patients, cbind('Patient'=unique(patients$Patient), 'group'=s), by="Patient")  
    foldcol = grep('group',colnames(patients))
    colnames(patients)[foldcol] = paste('fold',i,sep='.')
    patients[[paste('fold',i,sep='.')]] = check.sets(patients, paste('fold',i,sep='.'))
  }
  return(patients)
}

binomial.deviance<-function(pmat, y) {
  # Binomial deviance, lifted from cv.lognet
  prob_min = 1e-05; prob_max = 1 - prob_min
  pmat = pmin(pmax(pmat, prob_min), prob_max)
  dev = apply(pmat, 2, function(p)  -2*((y==1)*log(p)+(y==0)*log(1-p)) )
  return(dev)
}

crossvalidate.by.patient<-function(x,y,lambda,pts,a=1,nfolds=10, splits=5, fit=NULL, minR=0.2, select='classification') {
  if (nfolds > 5) minR = 0.1
  message(paste("Running", splits, "splits",nfolds,"times on", paste(dim(x), collapse=':'), 'alpha=',a ))
  fit.e = list()
  tpts = create.patient.sets(pts, nfolds, splits, minR)
  cv.pred = (matrix(nrow=0, ncol=length(lambda)))
  cv.binomial.deviance = (matrix(nrow=0, ncol=length(lambda)))
  for (n in 1:nfolds) {  
    message(paste(n, "fold"))
    setCol = grep(paste('^fold.',n,'$',sep=''), colnames(tpts))
    
    cv.class = matrix(nrow=splits, ncol=length(lambda))
    deviance = matrix(nrow=splits, ncol=length(lambda))
    for (i in 1:splits) { 
      message(paste(i, "split"))
      test.rows = which(rownames(x) %in% tpts[which(tpts[,setCol] == i), 'Samplename'])
      test = x[test.rows,]
      training = x[-test.rows,]
      # pre-spec lambda seq
      fitCV <- glmnet(training, y[-test.rows], lambda=lambda, family='binomial', alpha=a) 
      # autoplot(fit) + theme(legend.position="none")
      
      # Confusion matrix: quantitiative, pos results + neg results / number of test rows
      pred <- pi.hat(predict(fitCV, test, type='link')) 
      cv.class[i,] = apply(pred, 2, function(p) {
        (p%*%y[test.rows] + (1-p) %*% (1-y[test.rows]))/length(test.rows)
      })
      
      # Binomial deviance, lifted from cv.lognet
      dev = binomial.deviance(predict(fitCV, test, type='response'), as.factor(y[test.rows]))
      deviance[i,] = apply(dev, 2, weighted.mean, w=rep(1, nrow(dev)), na.rm=T)
      
      fit.e[[length(fit.e)+1]] = fitCV
    }
    cv.pred = rbind(cv.pred, cv.class)    
    cv.binomial.deviance = rbind(cv.binomial.deviance, deviance)
  }
  
  # Not really sure what to do with the binomial deviance now...
  df = cbind.data.frame('lambda.at'=1:ncol(cv.pred),
                        'mean'= colMeans(cv.pred), 
                        'sme'= apply(cv.pred, 2, sd)/sqrt(nrow(cv.pred)), 
                        'sd'= apply(cv.pred, 2, sd), 
                        'lambda'= lambda,
                        'log.lambda' = log(lambda),
                        'mean.b.dev' = colMeans(cv.binomial.deviance),
                        'sd.b.dev' = apply(cv.binomial.deviance, 2,sd),
                        'sme.b.dev' = apply(cv.binomial.deviance, 2, sd)/sqrt(nrow(cv.binomial.deviance)),
                        'lambda.min' = F, 'lambda.1se' = F)
  
  if (select == 'classification') {
    # Minimize the classification mean error & select min lambda
    df[which.max(subset(df, sme < median(sme) & sme > min(sme))$mean), 'lambda.min'] = T
    
    #df$lambda.min = df$mean == max(df$mean)
    se1 = df[df$lambda.min == T, 'mean'] - df[df$lambda.min == T, 'sme']
    
    arrange(subset(df, mean < se1 & !lambda.min & log.lambda > subset(df, lambda.min)$log.lambda ), -mean)[1:10,]
    df[arrange(subset(df, mean < se1 & !lambda.min & log.lambda > subset(df, lambda.min)$log.lambda ), -mean)[1, 'lambda.at'], 'lambda.1se'] = T
    #df[df$log.lambda < subset(df, mean == max(mean))$log.lambda-sd(df$log.lambda),][1, 'lambda-1se'] = TRUE
  } else if (select == 'deviance') {
    df$lambda.min = df$mean.b.dev == min(df$mean.b.dev)
    se1 = df[df$lambda.min == T, 'mean.b.dev'] - df[df$lambda.min == T, 'sme.b.dev']
    df[arrange(subset(df, mean.b.dev <= se1 & !lambda.min), -mean.b.dev)[1, 'lambda.at'], 'lambda.1se'] = T
    #df[df$mean.b.dev >= df[df$lambda.min == T, 'mean.b.dev'] + sd(df$mean.b.dev), ][1, 'lambda.1se'] = T
  }
  
  lambda.min = lambda[subset(df, lambda.min == T)$lambda.at]
  lambda.1se = lambda[subset(df, lambda.1se == T)$lambda.at]
  
  nzcf = lapply(fit.e, non.zero.coef, s=lambda.1se)
  
  plots = plot.patient.cv(df, fit)
  plots$performance = plots$performance + theme(legend.position='bottom') + scale_colour_discrete(name = "") +
    labs(title=paste(splits,' splits, ',nfolds,' folds, alpha=',a, sep=''), y='mean pred.', x='log(lambda)') 
  plots$deviance = plots$deviance + theme(legend.position='bottom') + scale_colour_discrete(name="") + 
    labs(title=paste(splits,' splits, ',nfolds,' folds, alpha=',a, sep=''), y='mean Binomial Deviance', x='log(lambda)') 
  
  return(list('max.cm'=df[df$lambda.1se ==T,'mean'], 
              'lambda.min'=lambda.min, 'lambda.1se'=lambda.1se, 'lambdas'=df, 'plot'=plots$performance, 'deviance.plot'=plots$deviance, 'non.zero.cf'=nzcf))
}

plot.patient.cv<-function(df, fit=NULL) {
  gp = ggplot(df, aes(y=mean,x=log.lambda)) + geom_point() + geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + 
    geom_point(data=subset(df, lambda.min == T), aes(y=mean, x=log.lambda, colour="lambda.min"), size=2 ) +  
    geom_vline(xintercept = subset(df, lambda.min == T)$log.lambda, colour='grey') +
    annotate("text", x=subset(df, lambda.min == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
             label=paste(round(df[subset(df, lambda.min == T)$lambda.at,c('mean', 'sme')],3), collapse='\n+/-')) 

  gpD = ggplot(df, aes(y=mean.b.dev,x=log.lambda)) + geom_point() +
    geom_errorbar(aes(ymin=mean.b.dev-sme.b.dev, ymax=mean.b.dev+sme.b.dev)) + 
    geom_point(data=subset(df, lambda.min == T), aes(y=mean.b.dev, x=log.lambda, colour="lambda.min"), size=2 ) +          
    geom_vline(xintercept = subset(df, lambda.min == T)$log.lambda, colour='grey') +
    annotate("text", x=subset(df, lambda.min == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
             label=paste(round(df[subset(df, lambda.min == T)$lambda.at,c('mean', 'sme')],3), collapse='\n+/-')) 
      

  if (nrow(subset(df, lambda.1se == T)) > 0) {
    gp = gp + geom_point(data=subset(df, lambda.1se == T), aes(y=mean, x=log.lambda, colour="lambda.1se"), size=2 ) +
      geom_vline(xintercept = subset(df, lambda.1se == T)$log.lambda, colour='grey') +
      annotate("text", x=subset(df, lambda.1se == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
               label=paste(round(df[subset(df, lambda.1se == T)$lambda.at,c('mean', 'sme')],3), collapse='\n+/-')) 
    
    gpD = gpD + geom_point(data=subset(df, lambda.1se == T), aes(y=mean.b.dev, x=log.lambda, colour="lambda.1se"), size=2 ) +
      geom_vline(xintercept = subset(df, lambda.1se == T)$log.lambda, colour='grey') +
      annotate("text", x=subset(df, lambda.1se == T)$log.lambda, y=min(df$mean)+sd(df$mean), 
               label=paste(round(df[subset(df, lambda.1se == T)$lambda.at,c('mean', 'sme')],3), collapse='\n+/-')) 
  }
  
  
  gp = gp + theme(legend.position='bottom') + scale_colour_discrete(name = "") 
  gpD = gpD + theme(legend.position='bottom') + scale_colour_discrete(name="") 
  
  if (!is.null(fit)) {
    df$nzcoef = sapply(df$lambda, function(l) length(non.zero.coef(fit, l)))
    
    coef.min = subset(df, lambda.min == T)$nzcoef
    coef.1se = subset(df, lambda.1se == T)$nzcoef
    
    d = coef.1se-coef.min
    
    coef.text = arrange(subset(df,nzcoef %in% c(0,  coef.min, coef.1se, max(nzcoef) )), -lambda.min, -lambda.1se)
    coef.text = arrange(coef.text[!duplicated(coef.text$nzcoef),], -nzcoef)
    
    gp = gp + annotate("text", y=min(df$mean), x=coef.text$log.lambda, label=coef.text$nzcoef, color='darkblue')
    gpD = gpD + annotate("text", y=max(df$mean.b.dev), x=coef.text$log.lambda, label=coef.text$nzcoef, color='darkblue')
  }
  return(list('performance'=gp, 'deviance'=gpD))
}

```


```{r xvalpt, message=F, warning=F, echo=T, fig.height=16, fig.width=16}
nl = 1000

coefs = list(); plots = list(); performance.at.1se = list()
folds = 10; splits = 5
alpha.values = c(0,0.5,0.7,0.8,0.9,1)

file = paste(cache.dir, 'all.pt.alpha.Rdata', sep='/')
if (file.exists(file)) {
  message(paste("loading", file))
  load(file, verbose=T)
} else {
  for (a in alpha.values) {
    fit0 <- glmnet(dysplasia.df, labels, alpha=a, nlambda=nl, family='binomial') # all patients
    #autoplot(fit0, xvar='lambda', main=paste('fit0, all samples, alpha=',a,sep='')) + theme(legend.position='none') 
    
    l = fit0$lambda
    if (a == 0) {
      l = sort(c(fit0$lambda, seq(exp(-5), exp(-10), -1e-6),
            seq(exp(-10), exp(-15), -1e-8),
            seq(exp(-15), exp(-20), -1e-9)), decreasing=T)
    }
    cv.patient = crossvalidate.by.patient(x=dysplasia.df, y=labels, lambda=l, pts=pts, a=a, nfolds=folds, splits=splits, fit=fit0)

    lambda.opt = ifelse( length(cv.patient$lambda.1se) > 0, cv.patient$lambda.1se, cv.patient$lambda.min )
    
    coef.opt = as.data.frame(non.zero.coef(fit0, lambda.opt))
    coefs[[as.character(a)]] = coef.stability(coef.opt, cv.patient$non.zero.cf)
    
    plots[[as.character(a)]] = arrangeGrob(cv.patient$plot+ggtitle('Classification'), cv.patient$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
    
    performance.at.1se[[as.character(a)]] = subset(cv.patient$lambdas, lambda.1se == T)
  }
  save(plots, coefs, performance.at.1se, file=file)
}

do.call(grid.arrange, c(plots, top='All samples, 10fold, 5 splits'))
```


```{r xvalpt2, message=F, warning=F, echo=T}
performance.at.1se = do.call(rbind, performance.at.1se)
performance.at.1se$alpha = rownames(performance.at.1se)

performance.at.1se = cbind(performance.at.1se, do.call(rbind, lapply(coefs, function(x) cbind('n.Coef'=nrow(x), '75%'=length(which(rowSums(x)/50 >= 0.75))))))

ggplot(performance.at.1se, aes(alpha, mean)) + geom_text( aes(label=round(mean, 3)), hjust=-0.5) +
  geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme), color='grey') + geom_point() + ylim(0.5,0.8) +
  geom_text(aes(label=n.Coef), nudge_y=0.03) +
  geom_text(aes(label=paste('(',`75%`,')',sep='')), nudge_y=0.02) +
  labs(x='Elasticnet penalty value', y='Model performance at lambda.1se', 
       title=paste('All samples,', folds, 'folds,', splits, 'patient splits'))

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=length(names(coef.stable)), 
                           dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) 
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1

table(rowSums(cfs))

coef.stable = lapply(coef.stable, function(cf) cf/(folds*splits))

stability = do.call(rbind, lapply(coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(stability) = names(coef.stable)

pander(as.data.frame(sort(names(which((coef.stable[['1']]) >= 0.75)))), justify='left')
```

Values that are closer to full lasso (alpha=1) are pretty similar, however at full lasso we see fewer features than if we regularize it at 0.8. They share all non-zero features as well.
`r pander(sapply(coef.stable, length), caption='Total features at each value of alpha')`

### Feature stability

`r pander(table(rowSums(cfs)), caption="Number of features found in 1, 2, or all 3 models by alpha 0.8, 0.9 and 1")`


`r length(unique(unlist(lapply(coef.stable, function(cf)  names(which(cf > 0.8)) ))))` features are shared at all values of alpha and were non-zero in at least 80% of the folds.

### Features in samples

```{r echo=F, warning=F, message=F}

length(which( coef.stable[['1']] >= 0.5) )

regions = rownames(coefs[['1']])

pg = dysplasia.df[ labels == 1 ,regions]

nonpg = dysplasia.df[ labels == 0 ,regions]

colnames(pg)

meantests = sapply(regions, function(r)   t.test(pg[,r], nonpg[,r])$p.value )

#which(meantests < 0.05)

summary(pg[,'1:119640299-129610322'])
summary(nonpg[,'1:119640299-129610322'])

```


### Feature Information 

```{r, warning=F, message=F}
#topF = names(which((coef.stable[['1']]) >= 0.75))
topF = names(coef.stable[['1']])

features.in.prog = dysplasia.df[names(which(labels == 1)),topF]
features.nonprog = dysplasia.df[names(which(labels == 0)),topF]

feat = which(colnames(dysplasia.df) %in% topF)

# Gain or loss not correlated in prog or non

gl = cbind.data.frame(
  'P.gain' = apply(features.in.prog, 2, function(x) sum( as.integer(x >= 1.1) ) ), 
  'P.loss' = apply(features.in.prog, 2, function(x) sum( as.integer(x <= 0.9) ) ),
  'NP.gain' = apply(features.nonprog, 2, function(x) sum( as.integer(x >= 1.1) ) ),
  'NP.loss' = apply(features.nonprog, 2, function(x) sum( as.integer(x <= 0.9) ) ) ) 


allgl = cbind.data.frame(
  'ALL.P.gain' = apply(dysplasia.df[names(which(labels == 1)),], 2, function(x) sum( as.integer(x >= 1.1) ) ), 
  'ALL.P.loss' = apply(dysplasia.df[names(which(labels == 1)),], 2, function(x) sum( as.integer(x <= 0.9) ) ),
  'ALL.NP.gain' = apply(dysplasia.df[names(which(labels == 0)),], 2, function(x) sum( as.integer(x >= 1.1) ) ), 
  'ALL.NP.loss' = apply(dysplasia.df[names(which(labels == 0)),], 2, function(x) sum( as.integer(x <= 0.9) ) ) )

grid.arrange(
  ggcorr(gl, label=T) + ggtitle('Selected Features'),
  ggcorr(allgl, label=T) + ggtitle('All regions'), 
top='Gains vs Losses') 


# Gains/losses enriched in the top features?
fisher.test(rbind(colSums(gl[,c('P.gain','P.loss')]),colSums(allgl[-feat,c('ALL.P.gain','ALL.P.loss')])  )) 

fisher.test(rbind(colSums(gl[,c('NP.gain','NP.loss')]),colSums(allgl[-feat,c('ALL.NP.gain','ALL.NP.loss')])  )) 

```

#### bp Length



```{r, echo=F, warning=F, message=F}
chromInfo = read.table('hg19_genes.txt', sep='\t', header=T)
chromInfo$chr = factor(chromInfo$chr, levels=c(1:22, 'X','Y'))

topFSegments = do.call(rbind,  strsplit( topF, ':|-' ))
colnames(topFSegments) = c('chr','start','end')
topFSegments = as.data.frame(apply(topFSegments, 2, as.numeric))
topFSegments$chr = factor(topFSegments$chr, levels=chromInfo$chr)
topFSegments = arrange(topFSegments, chr, start)

topFRegions = arrange(merge(chromInfo[,c('chr','length', 'protein_coding')], topFSegments %>% group_by(chr) %>% summarise('totalbp'=sum(end-start), 'n.segments'=length(chr)), all.x=T), -protein_coding)

# Number of segments is strongly correlated with the length of the chromosome
with(topFRegions, cor.test(length, n.segments))

topFRegions$adj.info.content = topFRegions$protein_coding/(topFRegions$length^0.6)
with(topFRegions, cor.test(length, adj.info.content))

topFRegions = transform(topFRegions, ai.cut=cut(adj.info.content, 5, labels=c(5:1), include.lowest=T))           # bin data
ggplot(topFRegions, aes(reorder(chr, -adj.info.content), totalbp/length, fill=ai.cut)) + ylim(0, 0.5) +
  labs(y='Feature bp/total chr length', title='Ratio of covered base pairs', x='chr') +
  geom_bar(stat='identity') +  geom_text(aes(label=round(totalbp/length, 2)), vjust=-0.5) +
  scale_fill_discrete(name='Length adjusted information content') + theme(legend.position = 'bottom')

ct = with(topFRegions, cor.test(adj.info.content, totalbp/length))

bpCovered = with(topFRegions, sum(totalbp, na.rm=T)/sum(as.numeric(length)))
```

Strong correlation between the length adjusted information content (protein coding genes) and the covered bases in a chromosome r=`r round(ct$estimate, 2)` (p-value `r round(ct$p.value, 4)`). No features from chromosome 19 (the most gene dense chromosome) were selected.  

`r round(bpCovered, 2)*100`% of the total bases are within the top selected features. 

#### Genes?

Using Biomart get all genes that are fully within the the segment boundaries.

```{r, warning=F, message=F}
mart = biomaRt::useMart("ENSEMBL_MART_ENSEMBL", host="grch37.ensembl.org")
biomart = useDataset("hsapiens_gene_ensembl", mart)



attr = c('ensembl_gene_id','hgnc_symbol','start_position','end_position', 'chromosome_name', 'ensembl_exon_id','percentage_gene_gc_content')

bm = getBM(mart=biomart, attributes=attr, 
           filters=list('chromosomal_region'=sub('-', ':',topF), "biotype"=c("IG_C_gene","IG_D_gene","IG_J_gene","IG_V_gene","protein_coding","TR_C_gene","TR_D_gene","TR_J_gene","TR_V_gene")))
#head(bm)
bm = bm[-which(bm$hgnc_symbol == ""),]
bm = bm %>% group_by(hgnc_symbol, chromosome_name, start_position, end_position) %>% summarise('exon_count' = length(ensembl_exon_id)) 
exons = bm %>% group_by(chromosome_name) %>% summarise('exons'=sum(exon_count))

for (i in 1:nrow(topFSegments)) {
  s = topFSegments[i,]
  sbm = with(bm, which(chromosome_name == s[['chr']] & start_position >= s[['start']] & end_position <= s[['end']]))
  
  topFSegments[i,'genes'] = length(sbm)
  topFSegments[i,'exons'] = sum(bm[sbm,'exon_count'])

  bm[sbm, 'segment.index'] = i
}

# Not related to the length of the segment
with(topFSegments, cor.test(genes, (end-start)))

head(bm)

topFRegions = merge(topFRegions, (bm %>% group_by(chromosome_name) %>% summarise('gene.cnt'=length(unique(hgnc_symbol)), 'exon.cnt'=sum(exon_count))), by.x='chr', by.y='chromosome_name', all.x=T)

with(topFRegions, cor.test(protein_coding, exon.cnt))
with(topFRegions, cor.test(gene.cnt, exon.cnt)) # expected

# Exons correlate with the adjusted information content measure
with(topFRegions, cor.test(exon.cnt, adj.info.content))
  
ggplot(topFRegions, aes(reorder(chr, -adj.info.content), gene.cnt/protein_coding, fill=ai.cut)) + geom_bar(stat='identity') +
  labs(y='Gene Ratio', x='chr', title='Feature genes vs all protein coding genes') +
  geom_text(aes(label=round(gene.cnt/protein_coding, 2)), vjust=-0.5) +
  scale_fill_discrete(name='Length adjusted information content') + theme(legend.position = 'bottom')

# Merge COSMIC genes
ccgenes = read.table('~/Data/CosmicCensusGenes.tsv', sep='\t', header=T, stringsAsFactors=F)
ccgenes = ccgenes[-grep('X,Y', ccgenes$Genome.Location),]

# This is going to strip out some genes that don't appear in ensembl
attr = c('ensembl_gene_id','hgnc_symbol','start_position','end_position', 'chromosome_name')
ccbm = getBM(mart=biomart, attributes=attr, filters=list('external_gene_name'=ccgenes$Gene.Symbol, 'chromosome_name'=c(1:22,'X','Y')))
ccbm = ccbm[-which(ccbm$hgnc_symbol == ""),]
ccbm$chromosome_name = factor(ccbm$chromosome_name, levels=chromInfo$chr)

topFRegions = merge(topFRegions, (ccbm %>% group_by(chromosome_name) %>% summarise('COSMIC.all'=length(unique(hgnc_symbol)))), by.x='chr',by.y='chromosome_name', all.x=T)

containedCC = subset(bm, hgnc_symbol %in% ccbm$hgnc_symbol)

topFRegions = merge(topFRegions, (containedCC %>% group_by(chromosome_name) %>% summarise('COSMIC.cont'=length(unique(hgnc_symbol)))), by.x='chr',by.y='chromosome_name', all.x=T)

ggplot(topFRegions, aes(reorder(chr, -adj.info.content), COSMIC.cont/COSMIC.all, fill=ai.cut)) + geom_bar(stat='identity') +
  labs(y='COSMIC Gene Ratio', x='chr', title='COSMIC genes: Feature vs all')+
  geom_text(aes(label=round(COSMIC.cont/COSMIC.all, 2)), vjust=-0.5) +
  scale_fill_discrete(name='Length adjusted information content') + theme(legend.position = 'bottom')
```

However, the `r round(bpCovered, 2)*100`% of bases appear to include `r round(length(unique(bm$hgnc_symbol))/sum(chromInfo$protein_coding), 2)*100`% (`r length(unique(bm$hgnc_symbol))`) of known protein-coding genes.  Similarly, `r round(length(unique(containedCC$hgnc_symbol))/length(ccgenes$Gene.Symbol),2)*100`% of the `r length(ccgenes$Gene.Symbol)` COSMIC genes are contained within these regions.

This appears to be a fairly large enrichment.  


### Remove HGD/IMC Samples

Just as a point, the initial model is trained against `r nrow(dysplasia.df)` samples with `r ncol(dysplasia.df)` features.  What happens if we retrain it without HGD?

```{r noHGD, echo=T, message=F, warning=F, fig.height=16, fig.width=16}
`%nin%` <- Negate(`%in%`)

info = do.call(rbind, lapply(patient.data, function(df) df$info))

no.hgd.plots = list(); coefs = list(); performance.at.1se = list()

file = paste(cache.dir, 'nohgd.Rdata', sep='/')
if (file.exists(file)) {
  message(paste("loading file", file))
  load(file, verbose=T)
} else {
  # No HGD/IMC
  samples = intersect(rownames(dysplasia.df), subset(info, Pathology %nin% c('HGD', 'IMC'))$Samplename)
  for (a in alpha.values) {
    # all patients
    fitNoHGD <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, family='binomial', nlambda=nl) 
    cv.nohgd = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitNoHGD$lambda, pts=subset(pts, Samplename %in% samples), a=a, nfolds=folds, splits=splits, fit=fitNoHGD)
    
    no.hgd.plots[[as.character(a)]] = arrangeGrob(cv.nohgd$plot+ggtitle('Classification'), cv.nohgd$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
    
    coef.1se = as.data.frame(non.zero.coef(fitNoHGD, cv.nohgd$lambda.1se))
    coefs[[as.character(a)]] = coef.stability(coef.1se, cv.nohgd$non.zero.cf)
    
    performance.at.1se[[as.character(a)]] = subset(cv.nohgd$lambdas, lambda.1se == T)
  }
  save(no.hgd.plots, coefs, performance.at.1se, file=file)
}

do.call(grid.arrange, c(no.hgd.plots, top="No HGD/IMC samples", ncol=2))
```

```{r noHGD2, echo=T, message=F, warning=F}
performance.at.1se = do.call(rbind, performance.at.1se)
performance.at.1se$alpha = rownames(performance.at.1se)

performance.at.1se = cbind(performance.at.1se, do.call(rbind, lapply(coefs, function(x) cbind('n.Coef'=nrow(x), '75%'=length(which(rowSums(x)/50 >= 0.75))))))

ggplot(performance.at.1se, aes(alpha, mean)) + geom_point() + geom_text( aes(label=round(mean, 2)), hjust=-0.5) +
  geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + ylim(0.5,0.8) + 
  geom_text(aes(label=n.Coef), nudge_y=0.03) +
  geom_text(aes(label=paste('(',`75%`,')',sep='')), nudge_y=0.02) +
  labs(x='Elasticnet penalty value', y='Model performance at lambda.1se', 
       title=paste('No HGD/IMC samples,', folds, 'folds,', splits, 'patient splits'))

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=3, 
                           dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) {
  if (is.null(names(coef.stable$`0.8`))) next
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1
}

coef.stable = lapply(coef.stable, function(cf) cf/(folds*splits))

stability = do.call(rbind, lapply(coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(stability) = names(coef.stable)
```

#### Feature Stability

`r pander(stability, caption='Total features found at each value of alpha in 50% or more of the folds run')`

They also share all of the non-zero coefficients (`r which.max(sapply(coef.stable, length))` identifies the most non-zero coefs `r max(sapply(coef.stable, length))`).

`r pander(table(rowSums(cfs)), justify='left')`


### Remove HGD/IMC & LGD Samples

Now remove all LGD samples from the progressor's and retrain.

```{r noLGD, echo=T, message=F, warning=F, fig.height=16, fig.width=16}
file = paste(cache.dir, 'nolgd.Rdata', sep='/')

nolgd.plots = list(); coefs = list(); performance.at.1se = list()
if (file.exists(file)) {
  load(file, verbose=T)
} else {
  # No LGD
  samples = intersect(rownames(dysplasia.df), c(subset(info, Status == 'NP')$Samplename, subset(info, Pathology %nin% c('HGD', 'IMC', 'LGD') & Status == 'P')$Samplename))
  
  # No HGD/IMC/LGD in all patients
  samples = intersect(rownames(dysplasia.df), subset(info, Pathology %nin% c('HGD', 'IMC', 'LGD'))$Samplename)
  
  for (a in alpha.values) {
    fitNoLGD <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, family='binomial', nlambda=nl) # all patients
    
    cv.nolgd = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitNoLGD$lambda, pts=subset(pts, Samplename %in% samples), a=a, nfolds=folds, splits=splits, fit=fitNoLGD)
    
    nolgd.plots[[as.character(a)]] = arrangeGrob(cv.nolgd$plot+ggtitle('Classification'), cv.nolgd$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
    
    coef.1se = as.data.frame(non.zero.coef(fitNoLGD, cv.nolgd$lambda.1se))
    coefs[[as.character(a)]] = coef.stability(coef.1se, cv.nolgd$non.zero.cf)
    
    performance.at.1se[[as.character(a)]] = subset(cv.nolgd$lambdas, lambda.1se == T)
  }
  save(nolgd.plots, coefs, performance.at.1se, file=file)
}

do.call(grid.arrange, c(nolgd.plots, top="No HGD/IMC/LGD samples", ncol=2))
```

```{r noLGD2, echo=T, message=F, warning=F}
if (class(performance.at.1se) == 'list')
  performance.at.1se = do.call(rbind, performance.at.1se)

performance.at.1se$alpha = rownames(performance.at.1se)

performance.at.1se = cbind(performance.at.1se, do.call(rbind, lapply(coefs, function(x) cbind('n.Coef'=nrow(x), '75%'=length(which(rowSums(x)/50 >= 0.75))))))

ggplot(performance.at.1se, aes(alpha, mean)) + geom_point() + geom_text( aes(label=round(mean, 2)), hjust=-0.5) +
  geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + ylim(0.5,0.8) +
  geom_text(aes(label=n.Coef), nudge_y=0.03) +
  geom_text(aes(label=paste('(',`75%`,')',sep='')), nudge_y=0.02) +
  labs(x='Elasticnet penalty value', y='Model performance at lambda.1se', 
       title=paste('No HGD/IMC/LGD samples,', folds, 'folds,', splits, 'patient splits'))

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=3, 
                           dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) 
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1

coef.stable = lapply(coef.stable, function(cf) cf/(folds*splits))

stability = do.call(rbind, lapply(coef.stable, function(cf) {
  cbind('75%'=length(which(cf >= 0.75)),  '50%'=length(which(cf >= 0.5)),  '<50%'=length(which(cf < 0.5)))
}))
rownames(stability) = names(coef.stable)

```

#### Feature Stability

`r pander(stability, caption='Total features found at each value of alpha in 50% or more of the folds run')`

They also share all of the non-zero coefficients (`r which.max(sapply(coef.stable, length))` identifies the most non-zero coefs `r max(sapply(coef.stable, length))`).

`r pander(table(rowSums(cfs)), justify='left')`


## Compare progressors with many samples vs few

```{r, echo=T, message=F, warning=F, fig.height=16, fig.width=16}
m = median(subset(sum.patient.data, Status == 'P')$total.samples)
nonps = as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'NP')$Patient], function(df) df$info$Samplename)))

file = paste(cache.dir, 'few.Rdata', sep='/')

if (file.exists(file)) {
  load(file, verbose=T)
} else {
  # Few
  samples = c(nonps, as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'P' & total.samples < m)$Patient], function(pt) pt$info$Samplename))))
  
  few.plots = list()
  for (a in alpha.values) {
    fitF <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, nlambda=nl, family='binomial') # all patients
    cv.back.few = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitF$lambda, pts=subset(pts, Samplename %in% samples), splits=5, nfolds=20, a=a, fit=fitF, minR=0.1)
    #few.plots[[as.character(a)]] = cv.back.few$plot + ggtitle(paste('alpha=',a,sep=''))
    
    few.plots[[as.character(a)]] = arrangeGrob(cv.back.few$plot+ggtitle('Classification'), cv.back.few$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
  }
  save(few.plots, file=file)
} 
do.call(grid.arrange, c(few.plots, ncol=2, top=paste(nrow(subset(sum.patient.data, Status == 'P' & total.samples < m))," Progressors with < median # samples (",m,")")))

# Many
file = paste(cache.dir, 'many.Rdata', sep='/')

if (file.exists(file)) {
  load(file, verbose=T)
} else {
  samples = c(nonps, as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'P' & total.samples >= m)$Patient], function(pt) pt$info$Samplename))))
  
  many.plots = list()
  for (a in alpha.values) {
    fitM <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, nlambda=nl, family='binomial') # all patients
    
    cv.back.many = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fitM$lambda, pts=subset(pts, Samplename %in% samples), splits=5, nfolds=20, a=a, fit=fitM)
    
    many.plots[[as.character(a)]] = arrangeGrob(cv.back.many$plot+ggtitle('Classification'), cv.back.many$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
  }
  save(many.plots, file=file)
}

do.call(grid.arrange, c(many.plots, ncol=2, top=paste(nrow(subset(sum.patient.data, Status == 'P' & total.samples >= m))," Progressors with >= median # samples (",m,")")))

```

## Randomize the labels

By patient.  This should break the models...and as would be expected, the models are really poor with perfomance at 50% or less (coin toss).

```{r, echo=T, warning=F, message=F, fig.height=10, fig.width=10}

file = paste(cache.dir, 'rand.Rdata', sep='/')

if (file.exists(file)) {
  load(file, verbose=T)
} else {
  rand.patients = sum.patient.data
  rand.patients$Status = sample( c('NP','P'), nrow(sum.patient.data), replace=T, prob=c(0.5,0.5))

  r.labels = unlist(apply(rand.patients[,c('Patient','Status')],1, function(st) {
    info = patient.data[[ st['Patient'] ]]$info
    lbl = rep(as.integer( st['Status'] == 'P' ), nrow(info))
    names(lbl) = info$Samplename
    return(lbl)
  }))
  names(r.labels) = sub('.*\\.', '',  names(r.labels))
  
  # sort in label order
  r.dysplasia.df = t(mergedDf[,names(r.labels)])

  r.coefs = list(); r.plots = list()
  folds = 10; splits=5
  for (a in c(0,0.5,1)) {
    fitR <- glmnet(r.dysplasia.df, r.labels, alpha=a, nlambda=nl, family='binomial') # all patients
    
    cv.R = crossvalidate.by.patient(x=r.dysplasia.df, y=r.labels, lambda=fitR$lambda, pts=pts, a=a, nfolds=folds, splits=splits, fit=fitR)
    
    lambda.opt = ifelse( length(cv.R$lambda.1se) > 0, cv.R$lambda.1se, cv.R$lambda.min )
    
    coef.opt = as.data.frame(non.zero.coef(fitR, lambda.opt))
    r.coefs[[as.character(a)]] = coef.stability(coef.opt, cv.R$non.zero.cf)
    
    r.plots[[as.character(a)]] = arrangeGrob(cv.R$plot+ggtitle('Classification'), cv.R$deviance.plot+ggtitle('Binomial Deviance'), top=paste('alpha=',a,sep=''), ncol=2)
  }
  save(r.plots, r.coefs, file=file) 
}

do.call(grid.arrange, c(r.plots, top='All samples, Randomized labels (by patient)'))


```


# Leave one out and evaluate predictions

Leave out each patient (both P and NP) and run a new CV fit each time then predict the samples for the patient that was left out.  Fitted model includes HGD samples.

```{r leaveoneout, echo=T, message=F, warning=F}

info = do.call(rbind, lapply(patient.data, function(df) df$info))
pg.samp = lapply(patient.data, function(pt) {
  info = pt$info
  info$SampleSD = NA
  info$SampleMEAN = NA
  if (length(info$Samplename) > 1) {
    info$SampleSD = apply(pt$seg.vals[,info$Samplename], 2, sd)
    info$SampleMEAN =  apply(pt$seg.vals[,info$Samplename], 2, mean)
  }
  info$Prediction = NA
  info$Prediction.Dev.Resid = NA
  info$PID = unlist(lapply(info$Path.ID, function(x) unlist(strsplit(x, 'B'))[1]))
  return(info)
})

file = paste(cache.dir, 'loo.Rdata', sep='/')

if (file.exists(file)) {
  load(file, verbose=T)
} else {
  performance.at.1se = c(); coefs = list(); plots = list(); fits = list()
  # Remove each patient (LOO)
  for (pt in names(pg.samp)) {
    print(pt)
    tmp.patient.data = patient.data[subset(sum.patient.data, Patient != pt)$Patient]
    samples = as.vector(unlist(sapply(tmp.patient.data, function(df) df$info$Samplename )))
    
    train.rows = which(rownames(dysplasia.df) %in% samples)
    training = dysplasia.df[train.rows,]
    test = as.matrix(dysplasia.df[-train.rows,])
    if ( nrow(test) == ncol(dysplasia.df) ) test = t(test)
    
    # Predict function giving me difficulty when I have only a single sample, this ensures the dimensions are the same
    sparsed_test_data <- Matrix(data=0, nrow=ifelse(length(pg.samp[[pt]]$Samplename) > 1, nrow(test), 1),  ncol=ncol(training),
                                dimnames=list(pg.samp[[pt]]$Samplename,colnames(training)), sparse=T)
    for(i in colnames(dysplasia.df)) sparsed_test_data[,i] = test[,i]
    
    # Fit generated on all samples, including HGD
    a = 1
    fitLOO <- glmnet(training, labels[train.rows], alpha=a, family='binomial', nlambda=nl) # all patients
    #l = sort(c(fitLOO$lambda, seq(exp(-5), exp(-10), -0.001),
    #      seq(exp(-10), exp(-15), -1e-7), 
    #      seq(exp(-15), exp(-20), -1e-9)), decreasing=T)
    l = fitLOO$lambda
    cv = crossvalidate.by.patient(x=training, y=labels[train.rows], lambda=l, 
                                  pts=subset(pts, Samplename %in% samples), a=a, nfolds=10, splits=5, fit=fitLOO)
    #plots[[pt]] = cv$plot + ggtitle(pt)
    
    plots[[pt]] = arrangeGrob(cv$plot+ggtitle('Classification'), cv$deviance.plot+ggtitle('Binomial Deviance'), top=pt, ncol=2)
    
    fits[[pt]] = cv  
    
    if ( length(cv$lambda.1se) > 0 ) {
      performance.at.1se = c(performance.at.1se, subset(cv$lambdas, lambda == cv$lambda.1se)$mean)
      
      coef.1se = as.data.frame(non.zero.coef(fitLOO, cv$lambda.1se))
      coefs[[pt]] = coef.stability(coef.1se, cv$non.zero.cf)
      
      logit <- function(p){log(p/(1-p))}
      inverse.logit <- function(or){1/(1 + exp(-or))}
      
      pm = predict(fitLOO, newx=sparsed_test_data, s=cv$lambda.1se, type='response')
      sy = as.matrix(sqrt(binomial.deviance(pm, labels[pg.samp[[pt]]$Samplename])))
      
      #df = cbind.data.frame(pm, sy, 1:length(sy))
      #colnames(df) = c('pred','dev','x')
      #ggplot(df, aes(x, pred)) + geom_point() + geom_errorbar(aes(ymin=pred-dev, ymax=pred+dev))
      
      pg.samp[[pt]]$Prediction = pm[,1]
      pg.samp[[pt]]$Prediction.Dev.Resid = sy[,1] 
      
    } else {
      warning(paste("Patient", pt, "did not have a 1se"))
    }
  }
  save(plots, performance.at.1se, coefs, fits, pg.samp, file=file)
}

ggplot(as.data.frame(performance.at.1se), aes(y=performance.at.1se, x='LOO')) + ylim(0.5,0.9) +
  geom_boxplot( fill='lightblue', color='darkgrey', outlier.fill=NA, outlier.color=NA) + geom_jitter() +
  labs(y='performance at lambda.1se', x='', title='Performance for Leave One (patient) Out, at alpha=1')

```


## How do the predictions look at each time point?

Providing a single sample at a time to predict is unnecessary as each sample is independently predicted from the test matrix. Predictions are exactly the same even when a single sample is provided (checked).  

```{r timepoints, echo=F, message=F, warning=F}
# For HGD/IMC samples, progressors only
hgd = sapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  hgd = subset(pt, Pathology %in% c('HGD', 'IMC'))
  sum(as.integer(hgd$Prediction > 0.5))/nrow(hgd)
})


hgd = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  hgd = subset(pt, Pathology %in% c('HGD', 'IMC'))
  cbind.data.frame('correctly.predicted'=sum(as.integer(hgd$Prediction > 0.5)), 'total.hgd.samples'=nrow(hgd))
}))
rownames(hgd) = subset(sum.patient.data, Status == 'P')$Patient
hgd = subset(hgd, total.hgd.samples > 0) # Some patients had no hgd samples

hgd$pred.ratio = hgd$correctly.predicted/hgd$total.hgd.samples

hgd$Patients = rownames(hgd)
hgd = transform(hgd, Patients=reorder(Patients, -pred.ratio) ) 

ggplot(hgd[,c('total.hgd.samples','pred.ratio', 'Patients')], aes(x=Patients, y=pred.ratio)) + 
  geom_bar(stat='identity', fill='darkblue', alpha=0.6) + geom_label( aes(label=total.hgd.samples) ) +
  coord_flip() + labs(title='Correctly predicted HGD samples', y='Correct prediction ratio')


lgd = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  e = subset(pt, Pathology  == 'LGD')
  cbind.data.frame('predicted'=sum(as.integer(e$Prediction > 0.5)), 'total.lgd'=nrow(e))
}))
lgd = subset(lgd, total.lgd > 0)

lgd$pred.ratio = round(lgd$predicted/lgd$total.lgd, 2)

lgd$Patients = rownames(lgd)
lgd = transform(lgd, Patients=reorder(Patients, -pred.ratio) ) 

ggplot(lgd[,c('total.lgd','pred.ratio', 'Patients')], aes(x=Patients, y=pred.ratio)) + 
  geom_bar(stat='identity', fill='darkgreen', alpha=0.6) + geom_label( aes(label=total.lgd) ) +
  coord_flip() + labs(title='Correctly predicted LGD samples', y='Correct prediction ratio')

# All samples
all.predictions = sapply(pg.samp, function(pt) {
  if (unique(pt$Status) == 'NP') {
    as.integer(pt$Prediction < 0.5)
  } else {
    as.integer(pt$Prediction > 0.5)
  }
})

all.predictions = do.call(rbind, lapply(all.predictions, function(x) {
  cbind.data.frame('correctly.predicted'=sum(x), 'total.samples'=length(x))
}))

all.predictions$Status = ifelse (rownames(all.predictions) %in% subset(sum.patient.data, Status == 'P')$Patient, 'P', 'NP')

#ggplot(melt(all.predictions, id.vars='Status'), aes(y=value, x=Status, fill=variable)) +
#  geom_bar(stat='identity', position='dodge') + labs(x='', y='Sample Counts', main='Total Sample Predictions')

all.predictions$pred.ratio = with(all.predictions, correctly.predicted/total.samples)
all.predictions$Patient = rownames(all.predictions)
all.predictions = transform(all.predictions, Patient=reorder(Patient, pred.ratio) ) 

m = melt(all.predictions, id.vars=c('Patient','Status'), measure.vars=c('pred.ratio'))

grid.arrange(
  ggplot() + geom_bar(data=subset(m, Status == 'P'), aes(Patient, value), fill='darkblue', color='grey', stat='identity') +  
    theme(legend.position='none', axis.text.x = element_text(angle = 45, hjust = 1)) + coord_flip() +
    labs(title='Progressors', x='', y='Ratio predicted samples'),
  ggplot() + geom_bar(data=subset(m, Status == 'NP'), aes(Patient, value), fill='darkgreen', color='grey', stat='identity') + 
    theme(legend.position='none', axis.text.x = element_text(angle = 45, hjust = 1)) + coord_flip() +
    labs(title='Non Progressors', x='', y='Ratio predicted samples'),
  ncol=2)

pg = subset(all.predictions, Status == 'P')
np = subset(all.predictions, Status == 'NP')

```

### Progressors

In progressors, `r round(sum(hgd$correctly.predicted)/sum(hgd$total.hgd.samples), 2)*100`% of the HGD samples were correctly predicted.

When predicting each sample individually `r round(nrow(subset(pg, pred.ratio == 1))/nrow(pg))*100`% patients were correctly predicted in all samples. `r round(nrow(subset(pg, pred.ratio < 1 & pred.ratio > 0))/nrow(pg),2)*100`% of patients were correctly predicted in some samples

`r nrow(subset(pg, pred.ratio == 0))` patients had no correctly predicted samples.

### Non-Progressors

In non-progressors `r round(nrow(subset(np, pred.ratio == 1))/nrow(np), 2)*100`% of patients were predicted to be non-progressors in _all_ of their samples. `r nrow(subset(np, pred.ratio >= 0.7))` patients were predicted correctly in 70% or more of their samples. Only `r nrow(subset(np, pred.ratio < 0.7 & pred.ratio > 0))` patients had fewer than 70% of their samples predicted correctly, and both of these have consistently been predicting _progression_ across multiple endoscopies.


```{r timepoints2, echo=T, message=F, warning=F}
# Only BE, Progressors
ndbe.only = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  e = subset(pt, Pathology %in% c('BE', '', '?'))
  cbind.data.frame('predicted'=sum(as.integer(e$Prediction > 0.5)), 'total.be'=nrow(e))
}))
ndbe.only = subset(ndbe.only, total.be > 0)
ndbe.only$pred.ratio = round(ndbe.only$predicted/ndbe.only$total.be, 2)

ndbe.only$Patients = rownames(ndbe.only)
ndbe.only = transform(ndbe.only, Patients=reorder(Patients, -pred.ratio) ) 

ggplot(ndbe.only[,c('total.be','pred.ratio', 'Patients')], aes(x=Patients, y=pred.ratio)) + 
  geom_bar(stat='identity', fill='darkred', alpha=0.6) + geom_label( aes(label=total.be) ) +
  coord_flip() + labs(title='Correctly predicted NDBE samples', y='Correct prediction ratio')


total = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
    cbind.data.frame('predicted'=sum(as.integer(pt$Prediction > 0.5)), 'total'=nrow(pt))
}))

total$pred.ratio = round(total$predicted/total$total, 2)

total$Patients = rownames(total)
total = transform(total, Patients=reorder(Patients, -pred.ratio) ) 
ggplot(total[,c('total','pred.ratio', 'Patients')], aes(x=Patients, y=pred.ratio)) + 
  geom_bar(stat='identity', fill='purple', alpha=0.6) + geom_label( aes(label=total) ) +
  coord_flip() + labs(title='Correctly predicted Progressor samples', y='Correct prediction ratio')


# For all pre-HGD/IMC/LGD in progressors
pre.hgd = sapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  e = subset(pt, Pathology %nin% c('HGD', 'IMC', 'LGD'))
  e$Prediction
  sum(as.integer(e$Prediction > 0.5))/nrow(e)
})

pred.by.endo<-function(pt, nyears) {
  e = subset(pt, PID %in% unique(pt$PID)[nyears])
  if (  length(which(e$Pathology %in% c('HGD','IMC'))) > 0 ) return(NA)
  p = as.vector(as.integer(e$Prediction > 0.5))
  names(p) = e$Endoscopy.Year
  return(p)
}

predicted.progression<-function(x, pos=1) { 
  if(length(which(is.na(x))) == length(x)) return(NA)
  as.integer(length(which(x == pos)) > 0)  
}

cols = c('%.Predicted','n.Endoscopies','n.Patients', 'FNR','FPR', 'median.pred.samp','median.total.samp', 'max.samples', 'sd.samples')
p.predictions = as.data.frame(matrix(data=0,nrow=6, ncol=length(cols), dimnames=list(c(), cols)))
for (i in 1:nrow(p.predictions)) {
  p = sapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], pred.by.endo, i)
  pp = sapply(p, predicted.progression)
  pp = pp[!is.na(pp)]
  
  fpr = sapply(pg.samp[subset(sum.patient.data, Status == 'NP')$Patient], pred.by.endo, i)
  fpr = sapply(fpr, predicted.progression)
  fpr = fpr[!is.na(fpr)]

  p.predictions[i,] =  rbind(round(sum(pp[!is.na(pp)]/length(pp[!is.na(pp)])), 2), i, length(pp), 
                             round(length(which(pp == 0))/length(pp), 2),
                             round(sum(fpr)/length(fpr), 2), 
                             median(sapply(p[names(which(pp > 0) )], function(x) length(which(x == 1)) )),
                             median(sapply(p[names(which(pp > 0))], length)), 
                             max(sapply(p[names(which(pp > 0))], length)),
                             sd(sapply(p[names(which(pp > 0))], length))
                             )
}

ggplot(p.predictions, aes(y=`%.Predicted`,x=factor(n.Endoscopies))) + 
  geom_bar(stat="identity", fill='dodgerblue3') + 
  geom_text(aes(label=paste(`%.Predicted`*100, '%', sep='')), nudge_y=-0.02, size=8) + 
  geom_text(aes(label=paste(n.Patients, 'patients')), nudge_y=0.01, size=8) + 
  geom_bar(stat='identity', fill='darkred', alpha=0.3, aes(y=FPR)) +
  geom_text(aes(label=FPR, y=FPR)) +
  labs(title="Progression predicted per endoscopy", x='Number of Endoscopies', y='% Patients') +
  theme(text = element_text(size = 14))

```


## Backwards from HGD

Clinicians always ask to see the timepoints peeled backwards to see how early prediction is possible.

```{r, warning=F, message=F, echo=F, fig.width=16, fig.height=8}

back = lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], function(pt) {
  pt = pt %>% group_by(PID, Endoscopy.Year) %>% summarise(
                  'pred'=sum(as.integer(Prediction > 0.5)), 'samples'=length(PID), 
                  #'final.endo'= length(which(grepl('HGD|IMC', Pathology))) > 0,
                  'months.before.final'= (max(pt$Endoscopy.Year) - unique(Endoscopy.Year))*12 )
  arrange(pt, -Endoscopy.Year, PID)
})

endos = do.call(rbind, lapply(1:max(sapply(back,nrow)), function(i) {
  df = do.call(rbind, lapply(back, function(x) {
    if (i > nrow(x)) return(NA)
    cbind(x[i, c('Endoscopy.Year','pred','samples', 'months.before.final')], i)
  }))
  df$Patient = rownames(df)
  return(df)
}))

endos = endos[complete.cases(endos),]
endos$i = endos$i-1

endos$pred.ratio = round(endos$pred/endos$samples, 2)
#endos$i = as.factor(endos$i)

#endos$brks <- cut(endos$pred.ratio, include.lowest=T,
#                   breaks=c(0, 0.1, 0.25, 0.5, 0.75, 0.9, 1), 
#                   labels=c('0', '0.1-.25', '.25-.5', '.5-.75','.75-0.9', '1'))
                     
endos = transform(endos, Patient=reorder(Patient, -months.before.final) ) 
endos$months.before.final = endos$months.before.final*-1

plot.endo<-function(df, minM=NULL, maxM=NULL) {
  if (is.null(minM)) minM = min(df$months.before.final)
  if (is.null(maxM)) maxM = max(df$months.before.final)

  df$i = as.factor(df$i)
  ggplot(df, aes(months.before.final, pred.ratio, group=1,color=Patient)) + facet_grid(Patient ~., scales='fixed') + scale_x_continuous(breaks=seq(minM, maxM, by = 12)) +
    geom_line() + geom_point() + labs(x='Months before diagnosis', y='Sample prediction ratio') + theme(legend.position='none') 
}


me = endos %>% group_by(Patient) %>% summarise('max.endo' = max(i))

grid.arrange(
  plot.endo( subset(endos, Patient %in% subset(me, max.endo >=5)$Patient), min(endos$months.before.final), max(endos$months.before.final) ),
  plot.endo( subset(endos, Patient %in% subset(me, max.endo < 5 & max.endo > 2)$Patient),min(endos$months.before.final), max(endos$months.before.final)  ),
  ncol=2)

```

```{r, echo=F, warning=F, message=F, fig.height=8, fig.width=8, eval=F}
plot.endo( subset(endos, Patient %in% subset(me, max.endo <= 2 & max.endo > 0)$Patient), min(endos$months.before.final), max(endos$months.before.final)  )
```


```{r, echo=F, warning=F, message=F, fig.height=4, fig.width=4, eval=F}
plot.endo( subset(endos, Patient %in% subset(me, max.endo == 0)$Patient),min(endos$months.before.final), max(endos$months.before.final) )
```

### How many samples?  TBD

Clinicians asked how many samples they needed to take.  


```{r, warning=F, message=F, fig.height=4, fig.width=4, eval=F}
# How do I evaluate the predictions?

#Basically I need to to a FNR and FPR calculation that is extremely conservative and use that somehow

all = do.call(rbind,lapply(pg.samp, function(df) {
  df[,c('Status','Prediction','Prediction.Dev.Resid')]
}))

ggplot(all, aes(Prediction.Dev.Resid, Prediction, color=Status)) + geom_point()  + 
  labs(y='Predicted (response)', x='Binomial dev residual?')
```


## Data Quality

### By Batch first

```{r, echo=T, warning=F, message=F, eval=F}
all = do.call(rbind, pg.samp)

prog = do.call(rbind, pg.samp[subset(sum.patient.data, Status == 'P')$Patient])
nonp = do.call(rbind, pg.samp[subset(sum.patient.data, Status == 'NP')$Patient])

## Similar numbers of each in these batches so how did we do?
tfPR = sapply( c('Exome_subcohort','Batch_1','Batch_2','Batch_3','Batch_4'), function(b) {
  pred = subset(prog, Batch.Name == b)$Prediction
  predB = subset(nonp, Batch.Name == b)$Prediction
  
  tpr = length(which(pred > 0.5))/length(pred)
  fpr = length(which(predB > 0.5))/length(predB)
  cbind.data.frame('TPR'=tpr, 'FPR'=fpr)
})

tfPR

## Exome_cohort fpr driven by the two patients that we predicted to be progressors in nearly every sample
length(which(pg.samp$AD0591$Prediction > 0.5))
length(which(pg.samp$AH0329$Prediction > 0.5))


## Except the exome cohort, the TPR rate appears to be right around 48-68%
## FPR is over 20% in Batch_4 though

unique(subset(nonp, Batch.Name == 'Batch_4')$Patient)

pg.samp$AHM0100$Prediction > 0.5
pg.samp$AHM0121$Prediction > 0.5
pg.samp$AD0644$Prediction > 0.5

cb = combn( as.character(unique(all$Batch.Name)), 2)

batch =  sapply(1:ncol(cb), function(i) {
  sapply(c('SampleSD','SampleMEAN', 'Total.Reads'), function(x) {
    tt = t.test(subset(all, Batch.Name == cb[1,i])[[x]], subset(all, Batch.Name == cb[2,i])[[x]])  
    tt$p.value
  })
}) 
```


```{r, echo=T, warning=F, message=F, eval=F}
smp<-function(df) {
  df[,c('Prediction', 'Prediction.Dev.Resid', 'SampleMEAN', 'SampleSD', 'Barretts.Cellularity', 'Total.Reads', 'p53.Status', 'PID')]
}

prog = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'P')$Patient], smp) )
nonp = do.call(rbind, lapply(pg.samp[subset(sum.patient.data, Status == 'NP')$Patient], smp) )

GGally::ggcorr(data=prog, method=c('pairwise','pearson'), label=T) + labs(title='Pearson correlation for several QC measures') 


# Compare poorly predicted to well predicted based on the sample SD measures
ttfunc<-function(col, df=NULL) {
  tt = t.test(subset(df, Prediction < 0.5)[[col]], subset(df, Prediction > 0.5)[[col]])
  cbind.data.frame('p.val'=tt$p.value, 'statistic'=tt$statistic)
}

```



```{r, echo=T, warning=F, message=F, eval=F}
 pander(sapply(colnames(prog)[-(1:2)], ttfunc, df=prog), caption='Progressor t-test stats for QC measures between predictions probabilies  >&< 0.5 ')

 pander(sapply(colnames(nonp)[-(1:2)], ttfunc, df=nonp), caption='Non-progressor t-test stats for QC measures between predictions probabilies  >&< 0.5 ')


hist(prog$SampleSD)


subset(prog, SampleSD <= 0.025)


```

