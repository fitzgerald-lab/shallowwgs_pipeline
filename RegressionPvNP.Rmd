---
title: "RegressionPvNP"
author: "Sarah Killcoyne"
date: "31 March 2017"
output: 
  html_document: 
    fig_caption: yes
    fig_height: 10
    fig_width: 10
    toc: yes
---

```{r setup, include=FALSE}
library(ggplot2)
library(ggfortify)
library(plyr)
library(pander)
#library(Hmisc)
library(reshape2)
library(gridExtra)
library(GenomicRanges)
library(glmnet)


load("Test_patients.Rdata", verbose=T)
validation.patient.data = patient.data

dataset = 'Training'
load(paste(dataset, '_patients.Rdata', sep=''), verbose=T)

source('lib/load_patient_metadata.R')

data = '~/Data/Ellie'

data.files = list.files(paste(data, 'QDNAseq',sep='/'), full.names=T)
plot.dir = paste(data, 'Analysis/multipcf_plots_fitted_perPatient', sep='/')

if (length(list.files(plot.dir)) <= 0)
  stop(paste("No analysis files found in", plot.dir ))

## Patient info file
patient.file = grep('All_patient_info.txt', data.files, value=T)
if (length(patient.file) != 1)
  stop(paste("Missing/too many patient info file(s) in", data))

patient.info = read.patient.info(patient.file, set=dataset)
patient.info$Patient = gsub("/", "_", patient.info$Patient)
head(patient.info)

validation.patient.info = read.patient.info(patient.file, set='Test')
sum.validation.info = summarise.patient.info(validation.patient.info)

patient.info = arrange(patient.info, Status, Patient, Endoscopy.Year, Pathology)

sum.patient.data = summarise.patient.info(patient.info)
sum.patient.data = as.data.frame(subset(sum.patient.data, Patient %in% names(patient.data))) ## For now
pander(sum.patient.data[,c('Patient', 'Status', 'start.year', 'end.year','total.samples','total.endos','highest.path')])


# Missing some of the samples as they aren't all sequenced yet
for (pt in names(patient.data)) {
  patient.data[[pt]]$info = patient.data[[pt]]$info[patient.data[[pt]]$info$Samplename %in% colnames(patient.data[[pt]]$seg.vals)[-(1:5)],]
}
```


```{r apclust-func, echo=F, message=F, warning=F}
#suppressPackageStartupMessages( library(apcluster) )

apclust.data<-function(segdata, samples) {
  x1 = segdata[,intersect(colnames(segdata), samples)]
  x1 = x1[, samples]
  rownames(x1) = (segdata[,c(1:4)] %>%
    rowwise() %>%
    mutate(location=paste(paste(chrom, arm, sep=''), '.', start.pos, '-', end.pos, sep='')))$location
  return(x1)  
}

# apclust<-function(segdata) {
#   x1 = apclust.data(segdata)
#   # q=0 minimize off-diagonal similarity
#   ac = apcluster(negDistMat(r=2), x1, details=T, convits=25, q=0)
#   return(list('apres'=ac, 'data'=x1))
# }

```

The previous analysis employed AP clustering on a per-patient basis to look for evidence that it was possible to see differences between the progressors and non-progressors, and to quantify that in some sort of complexity measure.  It was able to separate them, and continued to do so after removing HGD samples, but quickly failed to model any difference as I removed timepoints from the patient.  

As suggested by Moritz it is possible to merge all of the patient samples and use GLMs to model the differences, and to select regions that may be most reflective of the differences. In this report that's what I've done, and it worked very well.  However, I'm a bit suspicious that it's worked too well.  I'm looking for biases in the data, or mistakes I might have made in setting up the model.

# Encode the large data matrix

To use GLMs across the patients I merge all samples from all patients and overlap the copy number segments. Where segments from a sample get split, the value of the original sample is retained for each of the split samples.


```{r tile, echo=F, message=F, warning=F, include=F}

chr.lengths = get.chr.lengths()
chr.lengths$chrom = sub('chr','',chr.lengths$chrom)
chr.lengths$start = 1

tile.w=1e7

genome = makeGRangesFromDataFrame(chr.lengths[1:22,], seqnames.field = 'chrom', end.field='chr.length')
tiles = tile(genome, width=tile.w)

tile.sample.segments<-function(tileFile, sample.data) {
  if (file.exists(tileFile)) {
    load(tileFile, verbose=T)
  } else {
    grList = lapply(sample.data, function(df) {
      df$info = arrange(df$info, Endoscopy.Year, Pathology)
    
      x1 = apclust.data(df$seg.vals, df$info$Samplename)

      segnames = as.data.frame(do.call(rbind, sapply(rownames(x1), strsplit, '[p|q].|-')))
      colnames(segnames) = c('chr','start','end')
      segnames[c('start','end')] = lapply(segnames[c('start','end')], function(x) as.numeric(as.character(x)))
    
      pt = unique(df$info$Patient)
      return( makeGRangesFromDataFrame(cbind(segnames,pt,x1), keep.extra.columns=T) )
    })
    
    sampleNames = unlist(sapply(sample.data, function(df) {
      df$info = arrange(df$info, Endoscopy.Year, Pathology)
      x1 = apclust.data(df$seg.vals, df$info$Samplename)
      colnames(x1)
    }))
    
    tile.segments<-function(tiles, gr, mergedSegments) {
      ov = findOverlaps(tiles, gr)
      for (qh in unique(queryHits(ov))) {
        #print(qh)
        currentTile = tiles[[qh]]
        curov = findOverlaps(currentTile, gr)
      
        for (i in 1:length(curov)) {
          segment = currentTile[ queryHits(curov)[i]  ]
          #print(segment)
          rows = with(mergedDf, which( chr==as.character(seqnames(segment)) & start == start(segment) & end == end(segment)))
          segmentVals = as.data.frame(elementMetadata(gr[ subjectHits(curov)[i] ]))
          mergedSegments[rows, names(segmentVals)[-1]] = segmentVals[,-1]
        }
      }
      return(mergedSegments)  
    }
    
    mergedDf = do.call(rbind, lapply(tiles, function(tile) { 
      cbind('chr'=as.character(seqnames(tile)), as.data.frame(ranges(tile))[1:2]) 
      }) )
    mergedDf[,sampleNames] = NA
      
    for (gr in grList) {
      #print(unique(gr$pt))
      mergedDf = tile.segments(tiles, gr, mergedDf)
    }
    
    #tmp = mergedDf
    mergedDf[is.na(mergedDf)] = 0
    rownames(mergedDf) = with(mergedDf, paste(chr, ':', start, '-', end, sep=''))
    
    save(mergedDf, file=tileFile)
  }
  return(mergedDf)
}

mergedDf = tile.sample.segments(paste(as.character(tile.w),'_tiledpts.Rdata', sep=''), patient.data)

#validatioMergedDf = tile.sample.segments(paste(as.character(tile.w),'validation_tiledpts.Rdata', sep=''), validation.patient.data)

```

Using a very large segment size for tiling across all patients (`r tile.w`) I get the following binomial models.  None of them fit well, and decreasing the size of the segments results in poor (or no) fits with mostly NA coefficients, or a lot of P(0 or 1).

# y = Progressors (1) vs Non (0)

The labels are split such that all samples from progressor patients are (1) and all samples from non-progressors are (0).

```{r labelsPNP, echo=F, message=F, include=F}
## binomial: dysplasia 1, BE 0
labels = unlist(sapply(patient.data, function(df) {
  df$info = arrange(df$info, Endoscopy.Year, Pathology)
  label = as.integer(df$info$Status == 'P') #as.integer(df$info$Pathology %in% c('HGD', 'IMC'))
  names(label) = df$info$Samplename
  return(label)
}))
names(labels) = sub('.*\\.', '',  names(labels))

pts = do.call(rbind, lapply(patient.data, function(df) {
  df$info = arrange(df$info, Endoscopy.Year, Pathology)
  cbind(df$info[,c('Patient','Samplename')])
}))
rownames(pts) = 1:nrow(pts)

# sort in label order
if (length(setdiff(colnames(mergedDf), names(labels))) > 3)
  warning("Labels vector is missing samples")

dysplasia.df = t(mergedDf[,names(labels)])
```

We have `r table(labels)[1]` samples from non-progressors and `r table(labels)[2]` samples from progressors.

Data matrix currently:
`r pander(dysplasia.df[1:10, 1:5], caption=paste("dimensions:", paste(dim(dysplasia.df), collapse=', ')), justify='left')`

## cv.glmnet Ridge vs Lasso

```{r cvglmnetPNP, echo=F, warning=F, message=F, eval=F}
#With all `r ncol(dysplasia.df)` regions of the genome, first check if/which regression may be appropriate. It appears that pure lasso may provide the best balance for number of features with non-zero coefficients.

plot.multi.cv.glmnet<-function(x,y,alpha=c(0,1), family="binomial", title='cv.glmnet') {
  plots <- list(  )
  cv.models = data.frame(matrix(ncol=3,nrow=0,dimnames=list(c(), c('class','lambda','cvm'))))
  for (a in alpha) {
    cv = cv.glmnet(x, y, alpha=a, family=family, standardize=F)
    cv.models = rbind(cv.models, cbind('class'=paste('cv',a,sep=''), 'lambda'=log(cv$lambda), 'cvm'=cv$cvm))
    plots[[as.character(a)]] = autoplot(cv, main=paste('alpha',a), ylab=cv$name)
  }
  cv.models[c('lambda','cvm')] = lapply(cv.models[c('lambda','cvm')], function(x) as.numeric(as.character(x)))
  
  gg = ggplot(cv.models, aes(x=lambda, y=cvm, color=class)) + geom_point() + geom_line() + 
      labs(x='log(Lambda)', y='Binomial Deviance', title=title)
  
  plots[['all']] = gg
  
  return(plots)
}

gp = plot.multi.cv.glmnet(dysplasia.df, labels, alpha=c(0,0.5,0.8,1), title='cv.glmnet P vs NP')

cv1 = cv.glmnet(dysplasia.df, labels, alpha=1, family='binomial')
gg = autoplot(cv1$glmnet.fit, xvar='lambda', main='alpha=1 glmnet.fit from cv.glmnet') + theme(legend.position='none') 

gp[['coef.1']] = gg

do.call(grid.arrange, c(gp, ncol=2, nrow=3))

```

## Build/Cross-validate by patient

Keeping in mind that the matrix is built on a sample, not patient, basis while the labels (Progressor/Non) are on a per-patient basis I generated the cross validated models for various values of alpha by running 10 or more iterations of 5-fold cross validation that pulled out all the samples for 5 patients at each fold.  1000 lambda values.

```{r cv.funcs, message=F, warning=F, echo=T, fig.height=6, fig.width=6}
pi.hat<-function(x) exp(x)/(1+exp(x))

non.zero.coef<-function(fit, s) {
  cf =  as.matrix(coef(fit, s))
  cf[which(cf != 0),][-1]
}

create.patient.sets<-function(pts, n, splits, minR=0.2) {
  # This function just makes sure the sets don't become too unbalanced with regards to the labels.
  check.sets<-function(df, grpCol, min) {
    sets = table(cbind.data.frame('set'=df[[grpCol]], 'labels'=labels[df$Samplename]))
    while ( (length(which(sets/rowSums(sets) < minR) ) >= 2 | length(which(sets/rowSums(sets) == 0)) > 0) ) {
      #print(sets/rowSums(sets))
      s = sample(rep(seq(5), length = length(unique(df$Patient))))
      df2 = merge(df, cbind('Patient'=unique(df$Patient), 'tmpgrp'=s), by="Patient")
      df[[grpCol]] = df2$tmpgrp
      sets = table(cbind.data.frame('set'=df[[grpCol]], 'labels'=labels[df$Samplename]))
    }
  return(df[[grpCol]])
  }
  
  s = sample(rep(seq(splits), length = length(unique(pts$Patient))))
  patients = merge(pts, cbind('Patient'=unique(pts$Patient), 'group'=s), by="Patient")  
  colnames(patients)[3] = c('fold.1')
  patients$fold.1 = check.sets(patients, 'fold.1', minR)

  for (i in 2:n) {
    s = sample(rep(seq(5), length = length(unique(pts$Patient))))
    patients = merge(patients, cbind('Patient'=unique(patients$Patient), 'group'=s), by="Patient")  
    foldcol = grep('group',colnames(patients))
    colnames(patients)[foldcol] = paste('fold',i,sep='.')
    patients[[paste('fold',i,sep='.')]] = check.sets(patients, paste('fold',i,sep='.'))
  }
  return(patients)
}

crossvalidate.by.patient<-function(x,y,lambda,pts,a=1,nfolds=10, splits=5, fit=NULL, minR=0.2) {
  message(paste("Running", splits, "splits",nfolds,"times on", paste(dim(x), collapse=':'), 'alpha=',a ))
  fit.e = list()
  pts = create.patient.sets(pts, nfolds, splits, minR)
  cv.pred = (matrix(nrow=0, ncol=length(lambda)))
  for (n in 1:nfolds) {  
    message(paste(n, "fold"))
    setCol = grep(paste('^fold.',n,'$',sep=''), colnames(pts))
    
    cv.class = matrix(nrow=splits, ncol=length(lambda))
    for (i in 1:splits) { 
      message(paste(i, "split"))
      test.rows = which(rownames(x) %in% pts[which(pts[,setCol] == i), 'Samplename'])
      test = x[test.rows,]
      training = x[-test.rows,]
      # pre-spec lambda seq
      fit <- glmnet(training, y[-test.rows], lambda=lambda, family='binomial', alpha=a) 
      # autoplot(fit) + theme(legend.position="none")
      # careful: matrix
      pred <- pi.hat(predict(fit, test, type='link')) 
      cv.class[i,] = apply(pred, 2, function(p) {
        # Confusion matrix: quantitiative, pos results + neg results / number of test rows
        (p%*%labels[test.rows] + (1-p) %*% (1-labels[test.rows]))/ length(test.rows)
      })
      
      fit.e[[length(fit.e)+1]] = fit
    }
  cv.pred = rbind(cv.pred, cv.class)    
  }

  df = cbind.data.frame('lambda.at'=1:ncol(cv.pred),
                        'mean'= colMeans(cv.pred), 
                        'sme'= apply(cv.pred, 2, sd)/sqrt(nrow(cv.pred)), 
                        'sd'= apply(cv.pred, 2, sd), 
                        'lambda'= lambda)

  df2 = df
  df2$lambda = log(lambda)
  df2$mean = 1-df2$mean
  
  df2$lambda.min = df2$mean == min(df2$mean)
  df2$lambda.1se = FALSE
  df2[df2$lambda < subset(df2, mean == min(mean))$lambda-sd(df2$lambda),][1,'lambda.1se'] = TRUE
  
  lambda.min = lambda[subset(df2, lambda.min == T)$lambda.at]
  lambda.1se = lambda[subset(df2, lambda.1se == T)$lambda.at]
  nzcf = lapply(fit.e, non.zero.coef, s=lambda.min)
  
  gp = ggplot(df2, aes(y=mean,x=lambda)) + geom_point() + 
    geom_errorbar(aes(ymin=mean-sme, ymax=mean+sme)) + 
    geom_point(data=subset(df2, lambda.min == T), aes(y=mean, x=lambda, colour="lambda.min"), size=2 ) +          
    geom_vline(xintercept = subset(df2, lambda.min == T)$lambda, colour='grey') +
    annotate("text", x=subset(df2, lambda.min == T)$lambda, y=0.35, 
             label=round(df[subset(df2, lambda.min == T)$lambda.at,'mean'],3)) 
  
  if (length(lambda.1se) > 0) {
    gp = gp + geom_point(data=subset(df2, lambda.1se == T), aes(y=mean, x=lambda, colour='lambda.1se'), size=2 ) +
      geom_vline(xintercept = subset(df2, lambda.1se == T)$lambda, colour='grey') +
      annotate("text", x=subset(df2, lambda.1se == T)$lambda, y=0.35, 
               label=round(df[subset(df2, lambda.1se == T)$lambda.at,'mean'],3)) 
  }
   gp = gp + theme( legend.position='bottom') + 
          scale_colour_discrete(name = "") +
          labs(title=paste(splits,' splits, ',nfolds,' folds, alpha=',a, sep=''), y='1-mean (P)', x='log(lambda)') 

  if (!is.null(fit)) {
    df2$nzcoef = sapply(df$lambda, function(l) length(non.zero.coef(fit, l)))
  
    coef.min = subset(df2, lambda.min == T)$nzcoef
    coef.1se = subset(df2, lambda.1se == T)$nzcoef
    d = coef.1se-coef.min
    
    coef.text = arrange(subset(df2,nzcoef %in% c(0,  coef.min, coef.1se, max(nzcoef) )), -lambda.min, -lambda.1se)
    coef.text = arrange(coef.text[!duplicated(coef.text$nzcoef),], -nzcoef)
    
    gp = gp + annotate("text", y=max(df2$mean), x=coef.text$lambda, label=coef.text$nzcoef)
  }
  
  return(list('max.cm'=df[which.max(df$mean)-1,'mean'], 
              'lambda.min'=lambda.min, 'lambda.1se'=lambda.1se, 'lambdas'=df, 'plot'=gp, 'non.zero.cf'=nzcf))
}

coef.stability<-function(opt, nz.list) {
  for (i in 1:length(nz.list)) {
    df = as.data.frame(nz.list[[i]])
    colnames(df) = i
    opt = merge(opt, df, by='row.names', all.x=T)
    rownames(opt) = opt$Row.names
    opt$Row.names = NULL
    opt[,(i+1)] = as.integer(!is.na(opt[,(i+1)]))
  }
  opt = opt[order(sapply(rownames(opt), function(x) as.numeric(unlist(strsplit(x, ':'))[1])  )),]
  return(opt)
}
```


```{r xvalpt, message=F, warning=F, echo=T, fig.height=12, fig.width=12}
plots = list()
coefs = list()
folds = 10; splits=5
alpha.values = c(0,0.5,0.7,0.8,0.9,1)
for (a in alpha.values) {
  fit0 <- glmnet(dysplasia.df, labels, alpha=a, nlambda=1000, family='binomial') # all patients
  autoplot(fit0, xvar='lambda', main=paste('fit0, all samples, alpha=',a,sep='')) + theme(legend.position='none') 
  lambda0 <- fit0$lambda

  cv.patient = crossvalidate.by.patient(x=dysplasia.df, y=labels, lambda=lambda0, pts=pts, a=a, nfolds=folds, splits=splits, fit=fit0)

  coef.1se = as.data.frame(non.zero.coef(fit0, cv.patient$lambda.1se))
  coefs[[as.character(a)]] = coef.stability(coef.1se, cv.patient$non.zero.cf)
  
  plots[[as.character(a)]] = cv.patient$plot + theme(legend.position='bottom') + ggtitle(paste('alpha=',a,sep=''))
}

do.call(grid.arrange, c(plots, top='All samples, 10fold, 5 splits', ncol=2))

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=3, 
                dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) 
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1
  
```


Values that are closer to full lasso (alpha=1) are pretty similar, however at full lasso we see fewer features than if we regularize it at 0.8.  


### Feature stability

`r pander(as.data.frame(coef.stable[['1']])/(folds*splits), justify='left', caption='Percentage of features from the glm fitted with all data. Numbers indicate the total number of folds that feature was identified in.')`

They also share all of the non-zero coefficients (0.8 identifies the most non-zero coefs `r max(sapply(coef.stable, length))`).

`r pander(cfs, justify='left')`

#### Did I understand correctly?

In order to predict, my trained model is then my original model, but using the optimal lambda value?
This isn't quite correct because the model was trained including these patients, just an example. 

```{r}
fit0 <- glmnet(dysplasia.df, labels, alpha=a, nlambda=200, family='binomial') # all patients
cv.patient = crossvalidate.by.patient(x=dysplasia.df, y=labels, lambda=fit0$lambda, pts=pts, a=1, nfolds=20, splits=5, fit=fit0)

# Progressor, minus the HGD sample(s)
cbind.data.frame(
  predict(fit0, dysplasia.df[patient.data$PR1_HIN_042$info$Samplename[-(9:10)],], s=cv.patient$lambda.1se, type='response'),
  predict(fit0, dysplasia.df[patient.data$PR1_HIN_042$info$Samplename[-(9:10)],], s=cv.patient$lambda.1se, type='class'))

# Non-P, all correctly predicted
cbind.data.frame(predict(fit0, dysplasia.df[patient.data$AD0610$info$Samplename,], s=cv.patient$lambda.1se, type='response'),
                 predict(fit0, dysplasia.df[patient.data$AD0610$info$Samplename,], s=cv.patient$lambda.1se, type='class'))

```

### Remove HGD/IMC Samples

Just as a point, the initial model is trained against `r nrow(dysplasia.df)` samples with `r ncol(dysplasia.df)` features.  What happens if we retrain it without HGD?

```{r noHGD, echo=T, message=F, warning=F, fig.height=8, fig.width=8}
`%nin%` <- Negate(`%in%`)

info = do.call(rbind, lapply(patient.data, function(df) df$info))

# No HGD/IMC
samples = intersect(rownames(dysplasia.df), subset(info, Pathology %nin% c('HGD', 'IMC'))$Samplename)

plots = list()
coefs = list()
for (a in alpha.values) {
  fit1 <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, family='binomial', nlambda=1000) # all patients
  autoplot(fit1, xvar='lambda', main='fit1, no HGD/IMC samples') + theme(legend.position='none') 
  lambda1 <- fit1$lambda

  cv.nohgd = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=lambda1, pts=subset(pts, Samplename %in% samples), a=a, nfolds=folds, splits=splits, fit=fit1)
  
  plots[[as.character(a)]] = cv.nohgd$plot + ggtitle(paste('alpha=',a,sep=''))
  coef.1se = as.data.frame(non.zero.coef(fit1, cv.nohgd$lambda.1se))
  coefs[[as.character(a)]] = coef.stability(coef.1se, cv.nohgd$non.zero.cf)
}

do.call(grid.arrange, c(plots, top="No HGD/IMC samples", ncol=2))


## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})

cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=3, 
                dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) 
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1

```

#### Feature Stability

`r pander(as.data.frame(coef.stable[['1']])/(folds*splits), justify='left', caption='Percentage of features from the glm fitted with all data. Numbers indicate the total number of folds that feature was identified in.')`

They also share all of the non-zero coefficients (`r which.max(sapply(coef.stable, length))` identifies the most non-zero coefs `r max(sapply(coef.stable, length))`).

`r pander(cfs, justify='left')`


### Remove HGD/IMC & LGD Samples

Now remove all LGD samples from the progressor's and retrain.

```{r noLGD, echo=T, message=F, warning=F, fig.height=8, fig.width=8}
# No LGD
samples = intersect(rownames(dysplasia.df), c(subset(info, Status == 'NP')$Samplename, subset(info, Pathology %nin% c('HGD', 'IMC', 'LGD') & Status == 'P')$Samplename))

# No HGD/IMC/LGD in all patients
samples = intersect(rownames(dysplasia.df), subset(info, Pathology %nin% c('HGD', 'IMC', 'LGD'))$Samplename)

plots = list()
coefs = list()
for (a in alpha.values) {
  fit2 <- glmnet(dysplasia.df[samples,], labels[samples], alpha=a, family='binomial', nlambda=1000) # all patients
  autoplot(fit2, xvar='lambda', main='fit1, no HGD/IMC samples') + theme(legend.position='none') 
  lambda2 <- fit2$lambda

  cv.nolgd = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=lambda2, pts=subset(pts, Samplename %in% samples), a=a, nfolds=folds, splits=splits, fit=fit2)

  plots[[as.character(a)]] = cv.nolgd$plot + ggtitle(paste('alpha=',a,sep=''))

  coef.1se = as.data.frame(non.zero.coef(fit2, cv.nolgd$lambda.1se))
  coefs[[as.character(a)]] = coef.stability(coef.1se, cv.nolgd$non.zero.cf)
}

do.call(grid.arrange, c(plots, top="No HGD/IMC/LGD samples", ncol=2))
 

## check how often the feature(s) is selected at that lamda in each split. "stability selection"
coef.stable = lapply( coefs[c('0.8','0.9','1')], function(cf) {
  sort(rowSums(cf[,-1]), decreasing=T)
})


cfs = as.data.frame(matrix(data=0,nrow=length(unique(names(table(unlist(lapply(coef.stable, names)))))), ncol=3, 
                dimnames=list( unique(names(table(unlist(lapply(coef.stable, names))))), names(coef.stable) )))

for (i in names(coef.stable)) 
  cfs[intersect(rownames(cfs), names(coef.stable[[i]])), i] = 1


```

#### Feature Stability

`r pander(as.data.frame(coef.stable[['1']])/(folds*splits), justify='left', caption='Percentage of features from the glm fitted with all data. Numbers indicate the total number of folds that feature was identified in.')`

They also share all of the non-zero coefficients (`r which.max(sapply(coef.stable, length))` identifies the most non-zero coefs `r max(sapply(coef.stable, length))`).

`r pander(cfs, justify='left')`


### Leave one patient out

Using the 'all sample' fit, run a LOO (patient)

```{r loop, echo=T, message=F, warning=F, fig.height=6, fig.width=6, eval=F}
fit0 <- glmnet(dysplasia.df, labels, alpha=1, nlambda=200, family='binomial') # all patients

# Randomize patients
pt.samp = sample(sum.patient.data$Patient)
performance.at.1se = c()
coefs = list()
for (pt in pt.samp) {
  samples = as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Patient != pt)$Patient], function(pt) pt$info$Samplename)))

  cv.loo = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fit0$lambda, pts= subset(pts, Samplename %in% samples), a=1, nfolds=20, splits=5, fit=fit0)

  performance.at.1se = c(performance.at.1se, subset(cv.loo$lambdas, lambda == cv.loo$lambda.1se)$mean)
  
  coef.1se = as.data.frame(non.zero.coef(fit0, cv.loo$lambda.1se))
  coefs[[as.character(a)]] = coef.stability(coef.1se, cv.loo$non.zero.cf)
}

ggplot(as.data.frame(performance.at.1se), aes(y=performance.at.1se, x='LOO')) + ylim(0.6,0.8) +
  geom_boxplot( fill='lightblue', color='darkgrey', outlier.fill=NA, outlier.color=NA) + geom_jitter() +
    labs(y='performance at lambda.1se', x='', title='Performance for Leave One (patient) Out, at alpha=1')

```


## Compare progressors with many samples vs few

```{r, echo=T, message=F, warning=F, eval=F}
m = median(subset(sum.patient.data, Status == 'P')$total.samples)


nonps = as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'NP')$Patient], function(df) df$info$Samplename)))

# Few
samples = c(nonps, as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'P' & total.samples < m)$Patient], function(pt) pt$info$Samplename))))

fit0 <- glmnet(dysplasia.df[samples,], labels[samples], alpha=1, nlambda=200, family='binomial') # all patients
autoplot(fit0) + theme(legend.position = 'none')

cv.back.few = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda=fit0$lambda, pts=subset(pts, Samplename %in% samples), splits=5, nfolds=20, a=1, fit=fit0)

# Many
samples = c(nonps, as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Status == 'P' & total.samples >= m)$Patient], function(pt) pt$info$Samplename))))
cv.back.many = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda0, subset(patients, Samplename %in% samples))

grid.arrange(cv.back.few$plot + ggtitle(paste(nrow(subset(sum.patient.data, Status == 'P' & total.samples < m))," Progressors with < median # samples")), 
             cv.back.many$plot + ggtitle(paste(nrow(subset(sum.patient.data, Status == 'P' & total.samples >= m)), "Progressors with >= median # samples")), top=paste("Median number of samples:", m))

```

## Peel back timepoints

So here, do I use the initial fit (all samples), or should I be using the noHGD fit...

```{r timepoints, echo=T, message=F, warning=F, eval=F}

get.next.samples<-function(patient.data, i) {
  as.vector(unlist(sapply(patient.data, function(pd) {
    j = i
    pd$info = arrange(pd$info, Endoscopy.Year, Pathology)
  
    if (j >= nrow(pd$info)) j = nrow(pd$info)
    
    pd$info[1:(nrow(pd$info)-j), 'Samplename']
  }) ))
}

nonps = get.next.samples(patient.data[subset(sum.patient.data, Status == 'NP')$Patient], 0)
fpr = data.frame(matrix(ncol=3,nrow=0))
for (i in c(0,1,seq(2,35,3))) {
  ps = get.next.samples(patient.data[subset(sum.patient.data, Status == 'P')$Patient], i)
  #samples = get.next.samples(patient.data, i)
  samples = c(nonps, ps)
  cv.back = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda0, subset(patients, Samplename %in% samples))
  fpr = rbind(fpr, c(cv.back$max.cm, sum(cv.back$lambdas[ which.max(cv.back$lambdas$mean)-1, c('sme')]), length(ps)))
#  plots[[ as.character(i) ]] = cv.back$plot + ggtitle(paste(length(ps), "progressor samples. mean f1:", cv.back$f1))
}
colnames(fpr) = c('performance','error','n.prog.samples')

grid.arrange(ggplot(fpr, aes(performance,n.prog.samples)) + geom_point(),
             ggplot(fpr, aes(performance, error)) + geom_point(), top='Peel back samples')

pander(fpr, justify='left', caption="Removing timepoints from the model and retraining")
```





Can look then at a single point that may be an example of the performance of the model.

```{r singletimepoint, echo=T, message=F, warning=F, eval=F}
ps = get.next.samples(patient.data[subset(sum.patient.data, Status == 'P')$Patient], 11)
samples = c(nonps, ps)
cv.back = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda0, subset(patients, Samplename %in% samples))

cv.back$plot + ggtitle(paste("CV Performance - ", length(ps), "progressor samples"))

info = subset(patient.info, Samplename %in% samples)

coef.opt.stability = coef.stability(coef.opt, cv.back$non.zero.cf)

pander(as.data.frame(sort(rowSums(coef.opt.stability[,2:6]), decreasing = T)), justify='left', caption="Features initially identified found in x of 5 xval splits for this limited model")

```


## Example

This can't be right...

```{r , echo=T, message=F, warning=F, eval=F}

# prog.samp = sample(subset(sum.patient.data, Status == 'P')$Patient)
# 
# pt = 'AHM0952' # P
# #pt = 'AHM0254' # NP
# testSamples = as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Patient == pt)$Patient], function(pt) pt$info$Samplename)))
# 
# #samples = as.vector(unlist(lapply(patient.data[subset(sum.patient.data, Patient != pt)$Patient], function(pt) pt$info$Samplename)))
# #cv.back = crossvalidate.by.patient(x=dysplasia.df[samples,], y=labels[samples], lambda0, subset(patients, Samplename %in% samples))
# #cv.back$plot
# 
# predict(fit0, dysplasia.df[testSamples,], s=lambda.opt, type='response')
# predict(fit0, dysplasia.df[testSamples,], s=lambda.opt, type='class')
# 
# 
# # Random selection of samples
# randomSamples = sample(as.vector(unlist(lapply(patient.data, function(pt) pt$info$Samplename))), 30)
# predict(fit0, dysplasia.df[randomSamples,], s=lambda.opt, type='response')
# 
# cbind(predict(fit0, dysplasia.df[randomSamples,], s=lambda.opt, type='class'),labels[randomSamples])


```






